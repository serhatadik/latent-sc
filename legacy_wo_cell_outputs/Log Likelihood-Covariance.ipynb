{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de32eaf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def dB_to_lin(pow_dB):\n",
    "    return 10**(np.array(pow_dB)/10)\n",
    "\n",
    "def lin_to_dB(pow_lin):\n",
    "    return 10*np.log10(np.array(pow_lin))\n",
    "\n",
    "rssi_3470_3510 = np.array([-85.53,\n",
    "-95.92,\n",
    "-96.29,\n",
    "-101.19,\n",
    "-93.44,\n",
    "-99.88,\n",
    "-88.62,\n",
    "-99.78,\n",
    "-96.45,\n",
    "-88.8\n",
    "])\n",
    "\n",
    "\n",
    "rssi_3610_3650 = np.array([-84.93,\n",
    "-98.39,\n",
    "-101.84,\n",
    "-101.58,\n",
    "-92.01,\n",
    "-101.67,\n",
    "-86.77,\n",
    "-98.45,\n",
    "-102.38,\n",
    "-89.61\n",
    "])\n",
    "\n",
    "rssi_2160_2170 = np.array([-85.43,\n",
    "-76.88,\n",
    "-76.22,\n",
    "-86.68,\n",
    "-78.85,\n",
    "-80.78,\n",
    "-79.53,\n",
    "-74.71,\n",
    "-80.88,\n",
    "-72.33\n",
    "])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcdf8b98",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import scipy.io as sio\n",
    "import matplotlib.pyplot as plt\n",
    "map_folderdir = \"./\"\n",
    "directory = os.listdir(map_folderdir)\n",
    "flag = 0\n",
    "for fname in directory:\n",
    "    if \"SLCmap\" in fname:\n",
    "        map_file = os.path.join(map_folderdir, fname)\n",
    "        flag = 1\n",
    "\n",
    "if flag == 0:\n",
    "    errorMessage = 'Error: The file does not exist in the folder:\\n ' + map_folderdir\n",
    "    warnings.warn(errorMessage)\n",
    "\n",
    "print('Now reading ' + map_file + \"\\n\")\n",
    "x = sio.loadmat(map_file)\n",
    "map_struct = x['SLC']\n",
    "\n",
    "# Define a new struct named SLC\n",
    "SLC = map_struct[0][0]\n",
    "column_map = dict(zip([name for name in SLC.dtype.names], [i for i in range(len(SLC.dtype.names))]))\n",
    "\n",
    "map_ = SLC[column_map[\"dem\"]] + 0.3048 * SLC[column_map[\"hybrid_bldg\"]]\n",
    "map_ = map_[::10, ::10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b985675",
   "metadata": {},
   "source": [
    "### Implement Pseudo-Inverse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7b2d879",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_points = np.array([\n",
    "    [966, 2992],\n",
    "    [2569, 3767],\n",
    "    [2873, 3447],\n",
    "    [2621, 4286],\n",
    "    [1312, 3830],\n",
    "    [3828, 2667],\n",
    "    [242, 2442],\n",
    "    [1711, 3145],\n",
    "    [2852, 1584],\n",
    "    [1903, 2393]\n",
    "])\n",
    "data_points = data_points.astype(float)/10.0\n",
    "data_points = np.floor(data_points).astype(\"int\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ad176cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def serialize_index(row, col, num_cols=map_.shape[1]):\n",
    "    return row * num_cols + col\n",
    "\n",
    "dp_serial = []\n",
    "for dp in data_points:\n",
    "    serial_dp = serialize_index(dp[1], dp[0])\n",
    "    dp_serial.append(serial_dp)\n",
    "    \n",
    "dp_serial = np.array(dp_serial)\n",
    "dp_serial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ffcfa3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "def deserialize_index(serialized_index, num_cols=map_.shape[1]):\n",
    "    row = serialized_index // num_cols\n",
    "    col = serialized_index % num_cols\n",
    "    return row, col\n",
    "\n",
    "def euclidean_distance(serialized_index1, serialized_index2, num_cols=map_.shape[1]):\n",
    "    row1, col1 = deserialize_index(serialized_index1, num_cols)\n",
    "    row2, col2 = deserialize_index(serialized_index2, num_cols)\n",
    "\n",
    "    distance = ((row2 - row1)**2 + (col2 - col1)**2)**0.5*(0.5*10)\n",
    "    if distance<1:\n",
    "        distance=1\n",
    "    return distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0be2064b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.optimize import minimize\n",
    "\n",
    "def squared_error(T_i, P_i0, n_p, d_ij, P_j):\n",
    "    \"\"\"\n",
    "    Calculate the squared error for a given Tx power T_i.\n",
    "\n",
    "    :param T_i: Unknown Tx power.\n",
    "    :param P_i0: Reference power at a 1-meter distance from the Tx.\n",
    "    :param n_p: Path loss exponent.\n",
    "    :param d_ij: Array of distances between Tx i and Rx j.\n",
    "    :param P_j: Array of measured powers at Rx j.\n",
    "    :return: Squared error.\n",
    "    \"\"\"\n",
    "    # Calculate modeled received power for each sensor\n",
    "    P_hat_j = T_i - P_i0 - 10 * n_p * np.log10(d_ij)\n",
    "\n",
    "    # Calculate squared error\n",
    "    sqr_error = np.sum((P_hat_j - P_j)**2)\n",
    "    return sqr_error\n",
    "\n",
    "def find_minimizer_Ti(P_j, n_p, d_ij, P_i0):\n",
    "    \"\"\"\n",
    "    Find the minimizer T_i that minimizes the squared error.\n",
    "\n",
    "    :param P_j: Array of measured powers at Rx j.\n",
    "    :param n_p: Path loss exponent.\n",
    "    :param d_ij: Array of distances between Tx i and Rx j.\n",
    "    :param P_i0: Reference power at a 1-meter distance from the Tx.\n",
    "    :return: The minimizer T_i.\n",
    "    \"\"\"\n",
    "    # Initial guess for T_i\n",
    "    initial_T_i = 0\n",
    "\n",
    "    # Minimize the squared error with respect to T_i\n",
    "    result = minimize(squared_error, initial_T_i, args=(P_i0, n_p, d_ij, P_j))\n",
    "\n",
    "    # Return the minimizer T_i\n",
    "    return result.x[0]\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73cb2a7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "P_j = rssi_3470_3510\n",
    "n_p = 2  # Example path loss exponent\n",
    "P_i0 = 0  # Example reference power\n",
    "\n",
    "# Assuming the existence of the necessary variables like P_j, n_p, d_ij, P_i0, and data_points\n",
    "\n",
    "def serialize_index(row_col, num_cols=map_.shape[1]):\n",
    "    return row_col[:, 1] * num_cols + row_col[:, 0]\n",
    "\n",
    "dp_serial = serialize_index(np.array(data_points))\n",
    "\n",
    "def deserialize_index(serialized_indices, num_cols=map_.shape[1]):\n",
    "    rows = serialized_indices // num_cols\n",
    "    cols = serialized_indices % num_cols\n",
    "    return np.vstack((rows, cols)).T\n",
    "\n",
    "dp_coords = deserialize_index(dp_serial)\n",
    "\n",
    "def euclidean_distance(point, dp_coords):\n",
    "    distances = np.linalg.norm(dp_coords - point, axis=1) * 0.5*10\n",
    "    distances[distances < 0.5*10] = 0.5*10\n",
    "    return distances\n",
    "\n",
    "def process_batch(batch_indices, dp_coords, map_shape, num_cols):\n",
    "    minimizer_T_batch = []\n",
    "    for i in batch_indices:\n",
    "        point = deserialize_index(np.array([i]), num_cols)[0]\n",
    "        distances = euclidean_distance(point, dp_coords)\n",
    "        minimizer_T_batch.append(find_minimizer_Ti(P_j, n_p, distances, P_i0))\n",
    "    return minimizer_T_batch\n",
    "\n",
    "# Parallel processing\n",
    "num_cores = -1  # Adjust as needed. -1 uses all available cores\n",
    "batch_size = 100  # Adjust based on memory and performance considerations\n",
    "\n",
    "map_indices = np.arange(map_.size)\n",
    "batches = [map_indices[i:i + batch_size] for i in range(0, len(map_indices), batch_size)]\n",
    "\n",
    "minimizer_T_batches = Parallel(n_jobs=num_cores)(\n",
    "    delayed(process_batch)(batch, dp_coords, map_.shape, map_.shape[1]) for batch in tqdm(batches))\n",
    "\n",
    "minimizer_T = [item for sublist in minimizer_T_batches for item in sublist]\n",
    "minimizer_Ts = np.reshape(minimizer_T, map_.shape)\n",
    "\n",
    "np.save(\"minimizer_T_3470_3510_low_res3.npy\",minimizer_Ts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc5ec3d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "P_j = rssi_3610_3650\n",
    "n_p = 2  # Example path loss exponent\n",
    "P_i0 = 0  # Example reference power\n",
    "\n",
    "# Assuming the existence of the necessary variables like P_j, n_p, d_ij, P_i0, and data_points\n",
    "\n",
    "def serialize_index(row_col, num_cols=map_.shape[1]):\n",
    "    return row_col[:, 1] * num_cols + row_col[:, 0]\n",
    "\n",
    "dp_serial = serialize_index(np.array(data_points))\n",
    "\n",
    "def deserialize_index(serialized_indices, num_cols=map_.shape[1]):\n",
    "    rows = serialized_indices // num_cols\n",
    "    cols = serialized_indices % num_cols\n",
    "    return np.vstack((rows, cols)).T\n",
    "\n",
    "dp_coords = deserialize_index(dp_serial)\n",
    "\n",
    "def euclidean_distance(point, dp_coords):\n",
    "    distances = np.linalg.norm(dp_coords - point, axis=1) * 0.5*10\n",
    "    distances[distances < 0.5*10] = 0.5*10\n",
    "    return distances\n",
    "\n",
    "def process_batch(batch_indices, dp_coords, map_shape, num_cols):\n",
    "    minimizer_T_batch = []\n",
    "    for i in batch_indices:\n",
    "        point = deserialize_index(np.array([i]), num_cols)[0]\n",
    "        distances = euclidean_distance(point, dp_coords)\n",
    "        minimizer_T_batch.append(find_minimizer_Ti(P_j, n_p, distances, P_i0))\n",
    "    return minimizer_T_batch\n",
    "\n",
    "# Parallel processing\n",
    "num_cores = -1  # Adjust as needed. -1 uses all available cores\n",
    "batch_size = 100  # Adjust based on memory and performance considerations\n",
    "\n",
    "map_indices = np.arange(map_.size)\n",
    "batches = [map_indices[i:i + batch_size] for i in range(0, len(map_indices), batch_size)]\n",
    "\n",
    "minimizer_T_batches = Parallel(n_jobs=num_cores)(\n",
    "    delayed(process_batch)(batch, dp_coords, map_.shape, map_.shape[1]) for batch in tqdm(batches))\n",
    "\n",
    "minimizer_T = [item for sublist in minimizer_T_batches for item in sublist]\n",
    "minimizer_Ts = np.reshape(minimizer_T, map_.shape)\n",
    "\n",
    "np.save(\"minimizer_T_3610_3650_low_res3.npy\",minimizer_Ts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cafc449",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "P_j = rssi_2504_2544\n",
    "n_p = 3  # Example path loss exponent\n",
    "P_i0 = 0  # Example reference power\n",
    "\n",
    "# Assuming the existence of the necessary variables like P_j, n_p, d_ij, P_i0, and data_points\n",
    "\n",
    "def serialize_index(row_col, num_cols=map_.shape[1]):\n",
    "    return row_col[:, 1] * num_cols + row_col[:, 0]\n",
    "\n",
    "dp_serial = serialize_index(np.array(data_points))\n",
    "\n",
    "def deserialize_index(serialized_indices, num_cols=map_.shape[1]):\n",
    "    rows = serialized_indices // num_cols\n",
    "    cols = serialized_indices % num_cols\n",
    "    return np.vstack((rows, cols)).T\n",
    "\n",
    "dp_coords = deserialize_index(dp_serial)\n",
    "\n",
    "def euclidean_distance(point, dp_coords):\n",
    "    distances = np.linalg.norm(dp_coords - point, axis=1) * 0.5*10\n",
    "    distances[distances < 0.5*10] = 0.5*10\n",
    "    return distances\n",
    "\n",
    "def process_batch(batch_indices, dp_coords, map_shape, num_cols):\n",
    "    minimizer_T_batch = []\n",
    "    for i in batch_indices:\n",
    "        point = deserialize_index(np.array([i]), num_cols)[0]\n",
    "        distances = euclidean_distance(point, dp_coords)\n",
    "        minimizer_T_batch.append(find_minimizer_Ti(P_j, n_p, distances, P_i0))\n",
    "    return minimizer_T_batch\n",
    "\n",
    "# Parallel processing\n",
    "num_cores = -1  # Adjust as needed. -1 uses all available cores\n",
    "batch_size = 100  # Adjust based on memory and performance considerations\n",
    "\n",
    "map_indices = np.arange(map_.size)\n",
    "batches = [map_indices[i:i + batch_size] for i in range(0, len(map_indices), batch_size)]\n",
    "\n",
    "minimizer_T_batches = Parallel(n_jobs=num_cores)(\n",
    "    delayed(process_batch)(batch, dp_coords, map_.shape, map_.shape[1]) for batch in tqdm(batches))\n",
    "\n",
    "minimizer_T = [item for sublist in minimizer_T_batches for item in sublist]\n",
    "minimizer_Ts = np.reshape(minimizer_T, map_.shape)\n",
    "\n",
    "np.save(\"minimizer_T_2504_2544_low_res.npy\",minimizer_Ts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da00872b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "P_j = rssi_5190_5210\n",
    "n_p = 3  # Example path loss exponent\n",
    "P_i0 = 0  # Example reference power\n",
    "\n",
    "# Assuming the existence of the necessary variables like P_j, n_p, d_ij, P_i0, and data_points\n",
    "\n",
    "def serialize_index(row_col, num_cols=map_.shape[1]):\n",
    "    return row_col[:, 1] * num_cols + row_col[:, 0]\n",
    "\n",
    "dp_serial = serialize_index(np.array(data_points))\n",
    "\n",
    "def deserialize_index(serialized_indices, num_cols=map_.shape[1]):\n",
    "    rows = serialized_indices // num_cols\n",
    "    cols = serialized_indices % num_cols\n",
    "    return np.vstack((rows, cols)).T\n",
    "\n",
    "dp_coords = deserialize_index(dp_serial)\n",
    "\n",
    "def euclidean_distance(point, dp_coords):\n",
    "    distances = np.linalg.norm(dp_coords - point, axis=1) * 0.5*10\n",
    "    distances[distances < 0.5*10] = 0.5*10\n",
    "    return distances\n",
    "\n",
    "def process_batch(batch_indices, dp_coords, map_shape, num_cols):\n",
    "    minimizer_T_batch = []\n",
    "    for i in batch_indices:\n",
    "        point = deserialize_index(np.array([i]), num_cols)[0]\n",
    "        distances = euclidean_distance(point, dp_coords)\n",
    "        minimizer_T_batch.append(find_minimizer_Ti(P_j, n_p, distances, P_i0))\n",
    "    return minimizer_T_batch\n",
    "\n",
    "# Parallel processing\n",
    "num_cores = -1  # Adjust as needed. -1 uses all available cores\n",
    "batch_size = 100  # Adjust based on memory and performance considerations\n",
    "\n",
    "map_indices = np.arange(map_.size)\n",
    "batches = [map_indices[i:i + batch_size] for i in range(0, len(map_indices), batch_size)]\n",
    "\n",
    "minimizer_T_batches = Parallel(n_jobs=num_cores)(\n",
    "    delayed(process_batch)(batch, dp_coords, map_.shape, map_.shape[1]) for batch in tqdm(batches))\n",
    "\n",
    "minimizer_T = [item for sublist in minimizer_T_batches for item in sublist]\n",
    "minimizer_Ts = np.reshape(minimizer_T, map_.shape)\n",
    "\n",
    "np.save(\"minimizer_T_5190_5210_low_res.npy\",minimizer_Ts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd0689af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "P_j = rssi_2160_2170\n",
    "n_p = 2  # Example path loss exponent\n",
    "P_i0 = 0  # Example reference power\n",
    "\n",
    "# Assuming the existence of the necessary variables like P_j, n_p, d_ij, P_i0, and data_points\n",
    "\n",
    "def serialize_index(row_col, num_cols=map_.shape[1]):\n",
    "    return row_col[:, 1] * num_cols + row_col[:, 0]\n",
    "\n",
    "dp_serial = serialize_index(np.array(data_points))\n",
    "\n",
    "def deserialize_index(serialized_indices, num_cols=map_.shape[1]):\n",
    "    rows = serialized_indices // num_cols\n",
    "    cols = serialized_indices % num_cols\n",
    "    return np.vstack((rows, cols)).T\n",
    "\n",
    "dp_coords = deserialize_index(dp_serial)\n",
    "\n",
    "def euclidean_distance(point, dp_coords):\n",
    "    distances = np.linalg.norm(dp_coords - point, axis=1) * 0.5*10\n",
    "    distances[distances < 0.5*10] = 0.5*10\n",
    "    return distances\n",
    "\n",
    "def process_batch(batch_indices, dp_coords, map_shape, num_cols):\n",
    "    minimizer_T_batch = []\n",
    "    for i in batch_indices:\n",
    "        point = deserialize_index(np.array([i]), num_cols)[0]\n",
    "        distances = euclidean_distance(point, dp_coords)\n",
    "        minimizer_T_batch.append(find_minimizer_Ti(P_j, n_p, distances, P_i0))\n",
    "    return minimizer_T_batch\n",
    "\n",
    "# Parallel processing\n",
    "num_cores = -1  # Adjust as needed. -1 uses all available cores\n",
    "batch_size = 100  # Adjust based on memory and performance considerations\n",
    "\n",
    "map_indices = np.arange(map_.size)\n",
    "batches = [map_indices[i:i + batch_size] for i in range(0, len(map_indices), batch_size)]\n",
    "\n",
    "minimizer_T_batches = Parallel(n_jobs=num_cores)(\n",
    "    delayed(process_batch)(batch, dp_coords, map_.shape, map_.shape[1]) for batch in tqdm(batches))\n",
    "\n",
    "minimizer_T = [item for sublist in minimizer_T_batches for item in sublist]\n",
    "minimizer_Ts = np.reshape(minimizer_T, map_.shape)\n",
    "\n",
    "np.save(\"minimizer_T_2160_2170_low_res2.npy\",minimizer_Ts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0eccf9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aae9a4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "minimizer_Ts = np.load(\"minimizer_T_2160_2170_low_res2.npy\")\n",
    "x = np.linspace(0, map_.shape[1], map_.shape[1], endpoint=False)\n",
    "y = np.linspace(0, map_.shape[0], map_.shape[0], endpoint=False)\n",
    "X, Y = np.meshgrid(x, y)\n",
    "\n",
    "\n",
    "plt.figure(figsize=(13, 8))\n",
    "plt.contourf(X, Y, minimizer_Ts, 100, cmap='viridis')  # Use the masked array here\n",
    "plt.colorbar(label='dBX')\n",
    "\n",
    "# Plot the data collection points for reference\n",
    "scatter = plt.scatter(data_points[:, 0], data_points[:, 1], c=rssi_2160_2170, s=50, cmap='plasma', label='Data Collection Points')\n",
    "cbar = plt.colorbar(scatter, label='Signal Strength (dBX)', location='left')  # Colorbar for signal strengths on the left\n",
    "plt.legend()\n",
    "\n",
    "plt.xlabel('UTM_E [m]')\n",
    "plt.ylabel('UTM_N [m]')\n",
    "plt.title('2D Predictions of Signal Strength')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "438f6387",
   "metadata": {},
   "outputs": [],
   "source": [
    "minimizer_Ts = np.load(\"minimizer_T_3610_3650_low_res2.npy\")\n",
    "x = np.linspace(0, map_.shape[1], map_.shape[1], endpoint=False)\n",
    "y = np.linspace(0, map_.shape[0], map_.shape[0], endpoint=False)\n",
    "X, Y = np.meshgrid(x, y)\n",
    "\n",
    "\n",
    "plt.figure(figsize=(13, 8))\n",
    "plt.contourf(X, Y, minimizer_Ts, 100, cmap='viridis')  # Use the masked array here\n",
    "plt.colorbar(label='dBX')\n",
    "\n",
    "# Plot the data collection points for reference\n",
    "scatter = plt.scatter(data_points[:, 0], data_points[:, 1], c=rssi_3610_3650, s=50, cmap='plasma', label='Data Collection Points')\n",
    "cbar = plt.colorbar(scatter, label='Signal Strength (dBX)', location='left')  # Colorbar for signal strengths on the left\n",
    "plt.legend()\n",
    "\n",
    "plt.xlabel('UTM_E [m]')\n",
    "plt.ylabel('UTM_N [m]')\n",
    "plt.title('2D Predictions of Signal Strength')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fad7d01",
   "metadata": {},
   "outputs": [],
   "source": [
    "minimizer_Ts = np.load(\"minimizer_T_2504_2544_low_res.npy\")\n",
    "\n",
    "x = np.linspace(0, map_.shape[1], map_.shape[1], endpoint=False)\n",
    "y = np.linspace(0, map_.shape[0], map_.shape[0], endpoint=False)\n",
    "X, Y = np.meshgrid(x, y)\n",
    "\n",
    "\n",
    "plt.figure(figsize=(13, 8))\n",
    "plt.contourf(X, Y, minimizer_Ts, 100, cmap='viridis')  # Use the masked array here\n",
    "plt.colorbar(label='dBX')\n",
    "\n",
    "# Plot the data collection points for reference\n",
    "scatter = plt.scatter(data_points[:, 0], data_points[:, 1], c=rssi_2504_2544, s=50, cmap='plasma', label='Data Collection Points')\n",
    "cbar = plt.colorbar(scatter, label='Signal Strength (dBX)', location='left')  # Colorbar for signal strengths on the left\n",
    "plt.legend()\n",
    "\n",
    "plt.xlabel('UTM_E [m]')\n",
    "plt.ylabel('UTM_N [m]')\n",
    "plt.title('2D Predictions of Signal Strength')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c01f0692",
   "metadata": {},
   "outputs": [],
   "source": [
    "minimizer_Ts = np.load(\"minimizer_T_5190_5210_low_res.npy\")\n",
    "\n",
    "x = np.linspace(0, map_.shape[1], map_.shape[1], endpoint=False)\n",
    "y = np.linspace(0, map_.shape[0], map_.shape[0], endpoint=False)\n",
    "X, Y = np.meshgrid(x, y)\n",
    "\n",
    "\n",
    "plt.figure(figsize=(13, 8))\n",
    "plt.contourf(X, Y, minimizer_Ts, 100, cmap='viridis')  # Use the masked array here\n",
    "plt.colorbar(label='dBX')\n",
    "\n",
    "# Plot the data collection points for reference\n",
    "scatter = plt.scatter(data_points[:, 0], data_points[:, 1], c=rssi_5190_5210, s=50, cmap='plasma', label='Data Collection Points')\n",
    "cbar = plt.colorbar(scatter, label='Signal Strength (dBX)', location='left')  # Colorbar for signal strengths on the left\n",
    "plt.legend()\n",
    "\n",
    "plt.xlabel('UTM_E [m]')\n",
    "plt.ylabel('UTM_N [m]')\n",
    "plt.title('2D Predictions of Signal Strength')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1537c7d1",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "minimizer_Ts = np.load(\"minimizer_T_3470_3510_low_res.npy\")\n",
    "\n",
    "def deserialize_index(serialized_index, num_cols=map_.shape[1]):\n",
    "    row = serialized_index // num_cols\n",
    "    col = serialized_index % num_cols\n",
    "    return row, col\n",
    "\n",
    "def euclidean_distance(serialized_index1, serialized_index2, num_cols=map_.shape[1]):\n",
    "    row1, col1 = deserialize_index(serialized_index1, num_cols)\n",
    "    row2, col2 = deserialize_index(serialized_index2, num_cols)\n",
    "\n",
    "    distance = ((row2 - row1)**2 + (col2 - col1)**2)**0.5*(0.5*10)\n",
    "    if distance<0.5*10:\n",
    "        distance=0.5*10\n",
    "    return distance\n",
    "\n",
    "signal_strengths = rssi_3470_3510\n",
    "\n",
    "sigma = 2.5\n",
    "sigma_x = 5\n",
    "d_c = 50\n",
    "C = np.zeros((len(data_points),len(data_points)))\n",
    "for i in range(len(data_points)):\n",
    "    for j in range(len(data_points)):\n",
    "        C[i, j] = sigma_x**2*np.exp(-euclidean_distance(dp_serial[i], dp_serial[j])/d_c)\n",
    "\n",
    "C_inv = np.linalg.inv(C)\n",
    "min_T = minimizer_Ts.ravel()\n",
    "map_ = np.zeros((map_.shape[0], map_.shape[1]))\n",
    "#mean_sqr_err = np.zeros((len(map_.ravel()),))\n",
    "\n",
    "f_xz_new = np.zeros((len(map_.ravel()),))\n",
    "for i in tqdm(range(len(map_.ravel()))):\n",
    "    d_ij = []\n",
    "    for j in range(len(dp_serial)):\n",
    "        d_ij.append(euclidean_distance(i, dp_serial[j]))\n",
    "    diff = min_T[i]-10*n_p*np.log10(d_ij)-signal_strengths\n",
    "    f_xz_new[i] = np.exp(-1/2*diff.T@C_inv@diff)\n",
    "    #mean_sqr_err[i] = np.mean(diff**2)\n",
    "\n",
    "\n",
    "        \n",
    "#f_xz = np.exp(-mean_sqr_err/(2*sigma**2))\n",
    "f_xz_new /= np.sum(f_xz_new)\n",
    "\n",
    "f_xz_2d = np.reshape(f_xz_new, map_.shape)\n",
    "\n",
    "x = np.linspace(0, map_.shape[1], map_.shape[1], endpoint=False)\n",
    "y = np.linspace(0, map_.shape[0], map_.shape[0], endpoint=False)\n",
    "X, Y = np.meshgrid(x, y)\n",
    "\n",
    "\n",
    "plt.figure(figsize=(13, 8))\n",
    "plt.contourf(X, Y, f_xz_2d, 100, cmap='viridis')  # Use the masked array here\n",
    "plt.colorbar(label='dBX')\n",
    "\n",
    "# Plot the data collection points for reference\n",
    "scatter = plt.scatter(data_points[:, 0], data_points[:, 1], c=signal_strengths, s=50, cmap='plasma', label='Data Collection Points')\n",
    "cbar = plt.colorbar(scatter, label='Signal Strength (dBX)', location='left')  # Colorbar for signal strengths on the left\n",
    "plt.legend()\n",
    "\n",
    "plt.xlabel('UTM_E [m]')\n",
    "plt.ylabel('UTM_N [m]')\n",
    "plt.title('2D Predictions of Signal Strength')\n",
    "plt.show()\n",
    "\n",
    "np.save(\"f_xz_2d_3470_3510_low_res_cov.npy\",f_xz_2d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3748052f",
   "metadata": {},
   "outputs": [],
   "source": [
    "f_xz_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17c41f82",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(0, map_.shape[1], map_.shape[1], endpoint=False)\n",
    "y = np.linspace(0, map_.shape[0], map_.shape[0], endpoint=False)\n",
    "X, Y = np.meshgrid(x, y)\n",
    "\n",
    "f_xz_2d = np.load(\"f_xz_2d_3470_3510_low_res.npy\")\n",
    "plt.figure(figsize=(13, 8))\n",
    "plt.contourf(X, Y, f_xz_2d, 100, cmap='viridis')  # Use the masked array here\n",
    "plt.colorbar(label='Probability')\n",
    "\n",
    "# Plot the data collection points for reference\n",
    "scatter = plt.scatter(data_points[:, 0], data_points[:, 1], c=rssi_3470_3510, s=50, cmap='plasma', label='Data Collection Points')\n",
    "cbar = plt.colorbar(scatter, label='Signal Strength (dBX)', location='left')  # Colorbar for signal strengths on the left\n",
    "plt.legend()\n",
    "\n",
    "plt.xlabel('UTM_E [m]')\n",
    "plt.ylabel('UTM_N [m]')\n",
    "plt.title('2D Probability Distribution of Transmitter Location')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d8def78",
   "metadata": {},
   "source": [
    "### Calculate the Signal Strength Estimates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64b2b8f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "signal_strengths = rssi_3470_3510\n",
    "minimizer_Ts = np.load(\"minimizer_T_3470_3510_low_res.npy\")\n",
    "min_T = minimizer_Ts.ravel()\n",
    "n_p=3\n",
    "probabilities = np.load(\"f_xz_2d_3470_3510_low_res.npy\")\n",
    "probabilities = probabilities.ravel()\n",
    "\n",
    "idx_useful = np.where(probabilities>0.3e-5)[0]\n",
    "print(len(idx_useful))\n",
    "print(np.sum(probabilities[idx_useful]))\n",
    "signal_estimates = np.zeros_like(map_)\n",
    "signal_estimates = signal_estimates.ravel()\n",
    "for i in tqdm(range(len(map_.ravel()))):\n",
    "    d_ij = []\n",
    "    for j in idx_useful:\n",
    "        d_ij.append(euclidean_distance(i, j))\n",
    "    P_hats = min_T[idx_useful] - 10 * n_p * np.log10(d_ij)\n",
    "    signal_estimates[i] = np.sum(probabilities[idx_useful]*dB_to_lin(P_hats))/np.sum(probabilities[idx_useful])\n",
    "    if i%3000000 == 0 and i!=0:\n",
    "        plt.figure(figsize=(13, 8))\n",
    "        plt.contourf(X, Y, np.reshape(lin_to_dB(signal_estimates),map_.shape), 100, cmap='viridis')  # Use the masked array here\n",
    "        plt.colorbar(label='dBX')\n",
    "\n",
    "        # Plot the data collection points for reference\n",
    "        scatter = plt.scatter(data_points[:, 0], data_points[:, 1], c=signal_strengths, s=50, cmap='plasma', label='Data Collection Points')\n",
    "        cbar = plt.colorbar(scatter, label='Signal Strength (dBX)', location='left')  # Colorbar for signal strengths on the left\n",
    "        plt.legend()\n",
    "\n",
    "        plt.xlabel('UTM_E [m]')\n",
    "        plt.ylabel('UTM_N [m]')\n",
    "        plt.title('2D Predictions of Signal Strength')\n",
    "        plt.show()\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "049a3e2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.spatial.distance import cdist\n",
    "from tqdm import tqdm\n",
    "from joblib import Parallel, delayed\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load your data\n",
    "signal_strengths = rssi_3470_3510\n",
    "minimizer_Ts = np.load(\"minimizer_T_3470_3510.npy\").ravel()\n",
    "probabilities = np.load(\"f_xz_2d_3470_3510.npy\").ravel()\n",
    "n_p = 3\n",
    "\n",
    "# Identify useful indices\n",
    "idx_useful = np.where(probabilities > 0.3e-6)[0]\n",
    "\n",
    "\n",
    "# Dimensions of the map\n",
    "\n",
    "# Generate meshgrid for map coordinates\n",
    "Y, X = np.meshgrid(np.arange(map_.shape[1]), np.arange(map_.shape[0]))\n",
    "\n",
    "# Flatten the meshgrid and stack them as coordinates\n",
    "all_coords = np.stack((X.ravel(), Y.ravel()), axis=1)\n",
    "# Convert idx_useful to 2D coordinates\n",
    "useful_2d_indices = np.unravel_index(idx_useful, (map_.shape[0], map_.shape[1]))\n",
    "\n",
    "# Stack these coordinates\n",
    "useful_coords = np.stack(useful_2d_indices, axis=1)\n",
    "\n",
    "# Now, all_coords is ready to be used in your distance calculations\n",
    "\n",
    "# Vectorize distance calculation\n",
    "distances = cdist(all_coords, useful_coords)\n",
    "print()\n",
    "# Function to process a chunk of data\n",
    "def process_chunk(start_idx, end_idx):\n",
    "    partial_signal_estimates = np.zeros(end_idx - start_idx)\n",
    "    for i in range(start_idx, end_idx):\n",
    "        d_ij = distances[i]\n",
    "        P_hats = minimizer_Ts[idx_useful] - 10 * n_p * np.log10(d_ij)\n",
    "        weighted_sum = np.sum(probabilities[idx_useful] * dB_to_lin(P_hats))\n",
    "        partial_signal_estimates[i - start_idx] = weighted_sum / np.sum(probabilities[idx_useful])\n",
    "    return partial_signal_estimates\n",
    "\n",
    "# Parallel computation\n",
    "num_cores = -1  # Adjust based on your CPU\n",
    "chunk_size = len(all_coords) // num_cores\n",
    "ranges = [(i, min(i + chunk_size, len(all_coords))) for i in range(0, len(all_coords), chunk_size)]\n",
    "\n",
    "results = Parallel(n_jobs=num_cores)(delayed(process_chunk)(*r) for r in ranges)\n",
    "signal_estimates = np.concatenate(results)\n",
    "\n",
    "# Reshape signal_estimates if necessary and use it for plotting\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2da9d8ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "minimizer_Ts = np.load(\"minimizer_T_3470_3510_low_res.npy\")\n",
    "min_T = minimizer_Ts.ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7d187d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "min_T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a20fa902",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.spatial.distance import cdist\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "# Assuming dB_to_lin and other necessary functions are defined\n",
    "signal_strengths = rssi_5190_5210\n",
    "minimizer_Ts = np.load(\"minimizer_T_3470_3510_low_res.npy\")\n",
    "min_T = minimizer_Ts.ravel()\n",
    "n_p=3\n",
    "probabilities = np.load(\"f_xz_2d_3470_3510_low_res_cov.npy\")\n",
    "probabilities = probabilities.ravel()\n",
    "idx_useful = np.where(probabilities > 0)[0]\n",
    "\n",
    "useful_2d_indices = np.unravel_index(idx_useful, (map_.shape[0], map_.shape[1]))\n",
    "print(useful_2d_indices)\n",
    "# Stack these coordinates\n",
    "useful_coords = np.stack(useful_2d_indices, axis=1)\n",
    "print(useful_coords[:10, :])\n",
    "\n",
    "all_coords = useful_coords.copy()\n",
    "print(all_coords[:10, :])\n",
    "\n",
    "# Batch processing function\n",
    "def process_batch(start_idx, end_idx):\n",
    "    # Compute distances for a batch\n",
    "    batch_distances = cdist(all_coords[start_idx:end_idx], useful_coords)*5\n",
    "    # Initialize an array to hold signal estimates for this batch\n",
    "    batch_signal_estimates = np.zeros(end_idx - start_idx)\n",
    "\n",
    "    # Process distances and compute signal estimates\n",
    "    for i, d_ij in enumerate(batch_distances):\n",
    "        P_hats = min_T[idx_useful] - 10 * n_p * np.log10(1+d_ij)\n",
    "        weighted_sum = np.sum(probabilities[idx_useful] * dB_to_lin(P_hats))\n",
    "        batch_signal_estimates[i] = weighted_sum / np.sum(probabilities[idx_useful])\n",
    "\n",
    "    return batch_signal_estimates\n",
    "\n",
    "# Parameters for batch processing\n",
    "num_cores = -1  # Adjust based on your CPU\n",
    "num_points = all_coords.shape[0]\n",
    "batch_size = 10  # Adjust based on your memory capacity\n",
    "\n",
    "# Create ranges for batches\n",
    "ranges = [(i, min(i + batch_size, num_points)) for i in range(0, num_points, batch_size)]\n",
    "# Perform parallel batch processing\n",
    "results = Parallel(n_jobs=num_cores)(delayed(process_batch)(*r) for r in ranges)\n",
    "\n",
    "# Combine results from all batches\n",
    "signal_estimates = np.concatenate(results)\n",
    "np.save(\"signal_estimates_3470_3510_low_res_cov.npy\", signal_estimates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8921e58",
   "metadata": {},
   "outputs": [],
   "source": [
    "signal_estimates = np.reshape(signal_estimates, map_.shape)\n",
    "x = np.linspace(0, map_.shape[1], map_.shape[1], endpoint=False)\n",
    "y = np.linspace(0, map_.shape[0], map_.shape[0], endpoint=False)\n",
    "X, Y = np.meshgrid(x, y)\n",
    "\n",
    "\n",
    "plt.figure(figsize=(13, 8))\n",
    "plt.contourf(X, Y, lin_to_dB(signal_estimates), 100, cmap='viridis')  # Use the masked array here\n",
    "plt.colorbar(label='dBX')\n",
    "\n",
    "# Plot the data collection points for reference\n",
    "scatter = plt.scatter(data_points[:, 0], data_points[:, 1], c=signal_strengths, s=50, cmap='plasma', label='Data Collection Points')\n",
    "cbar = plt.colorbar(scatter, label='Signal Strength (dBX)', location='left')  # Colorbar for signal strengths on the left\n",
    "plt.legend()\n",
    "\n",
    "plt.xlabel('UTM_E [m]')\n",
    "plt.ylabel('UTM_N [m]')\n",
    "plt.title('2D Predictions of Signal Strength')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5796515e",
   "metadata": {},
   "outputs": [],
   "source": [
    "signal_estimates = np.reshape(signal_estimates, map_.shape)\n",
    "x = np.linspace(0, map_.shape[1], map_.shape[1], endpoint=False)\n",
    "y = np.linspace(0, map_.shape[0], map_.shape[0], endpoint=False)\n",
    "X, Y = np.meshgrid(x, y)\n",
    "\n",
    "\n",
    "plt.figure(figsize=(13, 8))\n",
    "plt.contourf(X, Y, lin_to_dB(signal_estimates), 100, cmap='viridis')  # Use the masked array here\n",
    "plt.colorbar(label='dBX')\n",
    "\n",
    "# Plot the data collection points for reference\n",
    "scatter = plt.scatter(data_points[:, 0], data_points[:, 1], c=signal_strengths, s=50, cmap='plasma', label='Data Collection Points')\n",
    "cbar = plt.colorbar(scatter, label='Signal Strength (dBX)', location='left')  # Colorbar for signal strengths on the left\n",
    "plt.legend()\n",
    "\n",
    "plt.xlabel('UTM_E [m]')\n",
    "plt.ylabel('UTM_N [m]')\n",
    "plt.title('2D Predictions of Signal Strength')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "483adae7",
   "metadata": {},
   "outputs": [],
   "source": [
    "signal_estimates = np.reshape(signal_estimates, map_.shape)\n",
    "x = np.linspace(0, map_.shape[1], map_.shape[1], endpoint=False)\n",
    "y = np.linspace(0, map_.shape[0], map_.shape[0], endpoint=False)\n",
    "X, Y = np.meshgrid(x, y)\n",
    "\n",
    "\n",
    "plt.figure(figsize=(13, 8))\n",
    "plt.contourf(X, Y, lin_to_dB(signal_estimates), 100, cmap='viridis')  # Use the masked array here\n",
    "plt.colorbar(label='Predicted Signal Strength (dBX)')\n",
    "\n",
    "# Plot the data collection points for reference\n",
    "scatter = plt.scatter(data_points[:, 0], data_points[:, 1], c=signal_strengths, s=50, cmap='plasma', label='Data Collection Points')\n",
    "cbar = plt.colorbar(scatter, label='Observed Signal Strength (dBX)', location='left')  # Colorbar for signal strengths on the left\n",
    "plt.legend()\n",
    "\n",
    "plt.xlabel('UTM_E [m]')\n",
    "plt.ylabel('UTM_N [m]')\n",
    "plt.title('2D Predictions of Signal Strength')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55c878db",
   "metadata": {},
   "outputs": [],
   "source": [
    "signal_estimates = np.reshape(signal_estimates, map_.shape)\n",
    "x = np.linspace(0, map_.shape[1], map_.shape[1], endpoint=False)\n",
    "y = np.linspace(0, map_.shape[0], map_.shape[0], endpoint=False)\n",
    "X, Y = np.meshgrid(x, y)\n",
    "\n",
    "\n",
    "plt.figure(figsize=(13, 8))\n",
    "plt.contourf(X, Y, lin_to_dB(signal_estimates), 100, cmap='viridis')  # Use the masked array here\n",
    "plt.colorbar(label='dBX')\n",
    "\n",
    "# Plot the data collection points for reference\n",
    "scatter = plt.scatter(data_points[:, 0], data_points[:, 1], c=signal_strengths, s=50, cmap='plasma', label='Data Collection Points')\n",
    "cbar = plt.colorbar(scatter, label='Signal Strength (dBX)', location='left')  # Colorbar for signal strengths on the left\n",
    "plt.legend()\n",
    "\n",
    "plt.xlabel('UTM_E [m]')\n",
    "plt.ylabel('UTM_N [m]')\n",
    "plt.title('2D Predictions of Signal Strength')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e8ab215",
   "metadata": {},
   "outputs": [],
   "source": [
    "has_nan_or_inf = np.any(np.isinf(signal_estimates))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7de5fbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "has_nan_or_inf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a039a75c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "scores = []\n",
    "signal_strengths = rssi_3470_3510\n",
    "for i in range(len(data_points)):\n",
    "    train_points = np.delete(data_points, i, axis=0)\n",
    "    train_strengths = np.delete(signal_strengths, i, axis=0)\n",
    "    test_point = data_points[i, :].reshape(1, -1)\n",
    "    test_strength = signal_strengths[i]\n",
    "    predicted_strength = lin_to_dB(signal_estimates[data_points[i][1], data_points[i][0]])\n",
    "    mse = (test_strength-predicted_strength)**2\n",
    "    scores.append(mse)\n",
    "print(np.mean(scores))\n",
    "print(np.std(scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b1dd574",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.std(signal_strengths)**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2e9edfb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8708be27",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b48118e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "257412ff",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.spatial.distance import cdist\n",
    "from joblib import Parallel, delayed\n",
    "from sklearn.metrics import mean_squared_error\n",
    "def deserialize_index(serialized_index, num_cols=map_.shape[1]):\n",
    "    row = serialized_index // num_cols\n",
    "    col = serialized_index % num_cols\n",
    "    return row, col\n",
    "\n",
    "def euclidean_distance(serialized_index1, serialized_index2, num_cols=map_.shape[1]):\n",
    "    row1, col1 = deserialize_index(serialized_index1, num_cols)\n",
    "    row2, col2 = deserialize_index(serialized_index2, num_cols)\n",
    "\n",
    "    distance = ((row2 - row1)**2 + (col2 - col1)**2)**0.5*(0.5*10)\n",
    "    if distance<0.5*10:\n",
    "        distance=0.5*10\n",
    "    return distance\n",
    "\n",
    "import os\n",
    "import scipy.io as sio\n",
    "import matplotlib.pyplot as plt\n",
    "map_folderdir = \"./\"\n",
    "directory = os.listdir(map_folderdir)\n",
    "flag = 0\n",
    "for fname in directory:\n",
    "    if \"SLCmap\" in fname:\n",
    "        map_file = os.path.join(map_folderdir, fname)\n",
    "        flag = 1\n",
    "\n",
    "if flag == 0:\n",
    "    errorMessage = 'Error: The file does not exist in the folder:\\n ' + map_folderdir\n",
    "    warnings.warn(errorMessage)\n",
    "\n",
    "print('Now reading ' + map_file + \"\\n\")\n",
    "x = sio.loadmat(map_file)\n",
    "map_struct = x['SLC']\n",
    "\n",
    "# Define a new struct named SLC\n",
    "SLC = map_struct[0][0]\n",
    "column_map = dict(zip([name for name in SLC.dtype.names], [i for i in range(len(SLC.dtype.names))]))\n",
    "\n",
    "map_ = SLC[column_map[\"dem\"]] + 0.3048 * SLC[column_map[\"hybrid_bldg\"]]\n",
    "map_ = map_[::10, ::10]\n",
    "\n",
    "\n",
    "def optimize_parameters(band, sigma_x, d_c, n_p, map_):\n",
    "    minimizer_Ts = np.load(\"minimizer_T_\"+band+\"_low_res3.npy\")\n",
    "\n",
    "    if band == \"3470_3510\":\n",
    "        signal_strengths = rssi_3470_3510\n",
    "    if band == \"3610_3650\":\n",
    "        signal_strengths = rssi_3610_3650\n",
    "    if band == \"2504_2544\":\n",
    "        signal_strengths = rssi_2504_2544\n",
    "    if band == \"2160_2170\":\n",
    "        signal_strengths = rssi_2160_2170\n",
    "\n",
    "\n",
    "    C = np.zeros((len(data_points),len(data_points)))\n",
    "    for i in range(len(data_points)):\n",
    "        for j in range(len(data_points)):\n",
    "            C[i, j] = sigma_x**2*np.exp(-euclidean_distance(dp_serial[i], dp_serial[j])/d_c)\n",
    "\n",
    "    C_inv = np.linalg.inv(C)\n",
    "    min_T = minimizer_Ts.ravel()\n",
    "    map_ = np.zeros((map_.shape[0], map_.shape[1]))\n",
    "    #mean_sqr_err = np.zeros((len(map_.ravel()),))\n",
    "\n",
    "    f_xz_new = np.zeros((len(map_.ravel()),))\n",
    "    for i in range(len(map_.ravel())):\n",
    "        d_ij = []\n",
    "        for j in range(len(dp_serial)):\n",
    "            d_ij.append(euclidean_distance(i, dp_serial[j]))\n",
    "        diff = min_T[i]-10*n_p*np.log10(d_ij)-signal_strengths\n",
    "        f_xz_new[i] = np.exp(-1/2*diff.T@C_inv@diff)\n",
    "        #mean_sqr_err[i] = np.mean(diff**2)\n",
    "\n",
    "\n",
    "\n",
    "    #f_xz = np.exp(-mean_sqr_err/(2*sigma**2))\n",
    "    f_xz_new /= np.sum(f_xz_new)\n",
    "    if True in np.isnan(f_xz_new):\n",
    "        print(\"Results' length is zero, skipping.\")\n",
    "        print(f\"Band: {band}, Sigma_x: {sigma_x}, d_c: {d_c}, n_p: {n_p}\")\n",
    "        return None\n",
    "    f_xz_2d = np.reshape(f_xz_new, map_.shape)\n",
    "    np.save(\"f_xz_2d_\"+band+\"_low_res_cov3.npy\",f_xz_2d)\n",
    "    \n",
    "    x = np.linspace(0, map_.shape[1], map_.shape[1], endpoint=False)\n",
    "    y = np.linspace(0, map_.shape[0], map_.shape[0], endpoint=False)\n",
    "    X, Y = np.meshgrid(x, y)\n",
    "\n",
    "\n",
    "    plt.figure(figsize=(13, 8))\n",
    "    plt.contourf(X, Y, f_xz_2d, 100, cmap='viridis')  # Use the masked array here\n",
    "    plt.colorbar(label='dBX')\n",
    "\n",
    "    # Plot the data collection points for reference\n",
    "    scatter = plt.scatter(data_points[:, 0], data_points[:, 1], c=signal_strengths, s=50, cmap='plasma', label='Data Collection Points')\n",
    "    cbar = plt.colorbar(scatter, label='Signal Strength (dBX)', location='left')  # Colorbar for signal strengths on the left\n",
    "    plt.legend()\n",
    "\n",
    "    plt.xlabel('UTM_E [m]')\n",
    "    plt.ylabel('UTM_N [m]')\n",
    "    plt.title('2D Predictions of Signal Strength')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "    # Assuming dB_to_lin and other necessary functions are defined\n",
    "    #probabilities = np.load(\"f_xz_2d_3470_3510_low_res_cov.npy\")\n",
    "    probabilities=f_xz_2d\n",
    "    probabilities = probabilities.ravel()\n",
    "    idx_useful = np.where(probabilities > 0)[0]\n",
    "\n",
    "    useful_2d_indices = np.unravel_index(idx_useful, (map_.shape[0], map_.shape[1]))\n",
    "    # Stack these coordinates\n",
    "    useful_coords = np.stack(useful_2d_indices, axis=1)\n",
    "\n",
    "    all_coords = useful_coords.copy()\n",
    "\n",
    "    # Batch processing function\n",
    "    def process_batch(start_idx, end_idx):\n",
    "        # Compute distances for a batch\n",
    "        batch_distances = cdist(all_coords[start_idx:end_idx], useful_coords)*5\n",
    "        # Initialize an array to hold signal estimates for this batch\n",
    "        batch_signal_estimates = np.zeros(end_idx - start_idx)\n",
    "\n",
    "        # Process distances and compute signal estimates\n",
    "        for i, d_ij in enumerate(batch_distances):\n",
    "            P_hats = min_T[idx_useful] - 10 * n_p * np.log10(1+d_ij)\n",
    "            weighted_sum = np.sum(probabilities[idx_useful] * dB_to_lin(P_hats))\n",
    "            batch_signal_estimates[i] = weighted_sum / np.sum(probabilities[idx_useful])\n",
    "\n",
    "        return batch_signal_estimates\n",
    "\n",
    "    # Parameters for batch processing\n",
    "    num_cores = -1  # Adjust based on your CPU\n",
    "    num_points = all_coords.shape[0]\n",
    "    batch_size = 10  # Adjust based on your memory capacity\n",
    "\n",
    "    # Create ranges for batches\n",
    "    ranges = [(i, min(i + batch_size, num_points)) for i in range(0, num_points, batch_size)]\n",
    "    # Perform parallel batch processing\n",
    "    results = Parallel(n_jobs=num_cores)(delayed(process_batch)(*r) for r in ranges)\n",
    "\n",
    "    # Combine results from all batches\n",
    "    signal_estimates = np.concatenate(results)\n",
    "    np.save(\"signal_estimates_\"+band+\"_low_res_cov3.npy\", signal_estimates)\n",
    "    if len(signal_estimates)!=map_.shape[1]*map_.shape[0]:\n",
    "        print(\"Results' length is zero, skipping.\")\n",
    "        print(f\"Band: {band}, Sigma_x: {sigma_x}, d_c: {d_c}, n_p: {n_p}\")\n",
    "        return None\n",
    "    \n",
    "    signal_estimates = np.reshape(signal_estimates, map_.shape)\n",
    "    \n",
    "    plt.figure(figsize=(13, 8))\n",
    "    plt.contourf(X, Y, lin_to_dB(signal_estimates), 100, cmap='viridis')  # Use the masked array here\n",
    "    plt.colorbar(label='dBX')\n",
    "\n",
    "    # Plot the data collection points for reference\n",
    "    scatter = plt.scatter(data_points[:, 0], data_points[:, 1], c=signal_strengths, s=50, cmap='plasma', label='Data Collection Points')\n",
    "    cbar = plt.colorbar(scatter, label='Signal Strength (dBX)', location='left')  # Colorbar for signal strengths on the left\n",
    "    plt.legend()\n",
    "\n",
    "    plt.xlabel('UTM_E [m]')\n",
    "    plt.ylabel('UTM_N [m]')\n",
    "    plt.title('2D Predictions of Signal Strength')\n",
    "    plt.show()\n",
    "    \n",
    "    scores = []\n",
    "    for i in range(len(data_points)):\n",
    "        train_points = np.delete(data_points, i, axis=0)\n",
    "        train_strengths = np.delete(signal_strengths, i, axis=0)\n",
    "        test_point = data_points[i, :].reshape(1, -1)\n",
    "        test_strength = signal_strengths[i]\n",
    "        predicted_strength = lin_to_dB(signal_estimates[data_points[i][1], data_points[i][0]])\n",
    "        mse = (test_strength-predicted_strength)**2\n",
    "        scores.append(mse)\n",
    "    print(f\"Band: {band}, Sigma_x: {sigma_x}, d_c: {d_c}, n_p: {n_p}, score: {np.mean(scores)}\")\n",
    "    return np.mean(scores)\n",
    "\n",
    "\n",
    "for band in [\"3470_3510\", \"3610_3650\"]:\n",
    "    for sigma_x in [4.5]:\n",
    "        for d_c in [400]:\n",
    "            optimize_parameters(band, sigma_x, d_c, 2, map_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1ffa078",
   "metadata": {},
   "outputs": [],
   "source": [
    "f_xz_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0688879",
   "metadata": {},
   "outputs": [],
   "source": [
    "32653/521"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30faddfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = [1,2,3]\n",
    "4 in a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd8e89e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.var(rssi_3470_3510)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bcf88f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.var(rssi_3610_3650)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b16bfe5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.var(rssi_2160_2170)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b777c471",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
