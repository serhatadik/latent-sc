{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "198a3269",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "\n",
    "def min_max_frequencies(monitor_foldername):\n",
    "    def read_gzipped_csv(file_path):\n",
    "        with gzip.open(file_path, 'rt') as file:\n",
    "            return pd.read_csv(file)\n",
    "\n",
    "    def extract_timestamp_from_filename(filename):\n",
    "        match = re.search(r'-(\\d+)\\.csv\\.gz$', filename)\n",
    "        return int(match.group(1)) if match else None\n",
    "\n",
    "    def concatenate_csvs_in_folder(folder_path):\n",
    "        dataframes = []\n",
    "        for filename in os.listdir(folder_path):\n",
    "            if filename.endswith('.gz'):\n",
    "                file_path = os.path.join(folder_path, filename)\n",
    "                df = read_gzipped_csv(file_path)\n",
    "                timestamp = extract_timestamp_from_filename(filename)\n",
    "                df['timestamp'] = pd.to_datetime(timestamp, unit='s')\n",
    "                dataframes.append(df)\n",
    "        return pd.concat(dataframes, ignore_index=True) if dataframes else pd.DataFrame()\n",
    "\n",
    "    folder_path = './rfbaseline/' + monitor_foldername + \"/\"\n",
    "    combined_df = concatenate_csvs_in_folder(folder_path)\n",
    "\n",
    "    if not combined_df.empty:\n",
    "        min_frequency = combined_df['frequency'].min()\n",
    "        max_frequency = combined_df['frequency'].max()\n",
    "        return min_frequency, max_frequency\n",
    "    else:\n",
    "        return None, None\n",
    "\n",
    "# Example usage\n",
    "monitor_foldername = 'Guesthouse'  # Replace with your folder name\n",
    "min_freq, max_freq = min_max_frequencies(monitor_foldername)\n",
    "if min_freq is not None and max_freq is not None:\n",
    "    print(f\"Minimum Frequency: {min_freq} MHz\")\n",
    "    print(f\"Maximum Frequency: {max_freq} MHz\")\n",
    "else:\n",
    "    print(\"No data available\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d00b333c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "\n",
    "def min_max_frequencies(monitor_foldername):\n",
    "    def read_gzipped_csv(file_path):\n",
    "        with gzip.open(file_path, 'rt') as file:\n",
    "            return pd.read_csv(file)\n",
    "\n",
    "    def extract_timestamp_from_filename(filename):\n",
    "        match = re.search(r'-(\\d+)\\.csv\\.gz$', filename)\n",
    "        return int(match.group(1)) if match else None\n",
    "\n",
    "    def concatenate_csvs_in_folder(folder_path):\n",
    "        dataframes = []\n",
    "        for filename in os.listdir(folder_path):\n",
    "            if filename.endswith('.gz'):\n",
    "                file_path = os.path.join(folder_path, filename)\n",
    "                df = read_gzipped_csv(file_path)\n",
    "                timestamp = extract_timestamp_from_filename(filename)\n",
    "                df['timestamp'] = pd.to_datetime(timestamp, unit='s')\n",
    "                dataframes.append(df)\n",
    "        return pd.concat(dataframes, ignore_index=True) if dataframes else pd.DataFrame()\n",
    "\n",
    "    folder_path = './rfbaseline/' + monitor_foldername + \"/\"\n",
    "    combined_df = concatenate_csvs_in_folder(folder_path)\n",
    "\n",
    "    if not combined_df.empty:\n",
    "        min_frequency = combined_df['frequency'].min()\n",
    "        max_frequency = combined_df['frequency'].max()\n",
    "        print(combined_df['frequency'])\n",
    "        return min_frequency, max_frequency\n",
    "    else:\n",
    "        return None, None\n",
    "\n",
    "# Example usage\n",
    "monitor_foldername = 'EBC'  # Replace with your folder name\n",
    "min_freq, max_freq = min_max_frequencies(monitor_foldername)\n",
    "if min_freq is not None and max_freq is not None:\n",
    "    print(f\"Minimum Frequency: {min_freq} MHz\")\n",
    "    print(f\"Maximum Frequency: {max_freq} MHz\")\n",
    "else:\n",
    "    print(\"No data available\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d83a756",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "\n",
    "def median_time_diff_between_folders(monitor_foldername):\n",
    "    def extract_timestamp_from_filename(filename):\n",
    "        # Extract the Unix timestamp using a regular expression\n",
    "        match = re.search(r'-(\\d+)\\.csv\\.gz$', filename)\n",
    "        if match:\n",
    "            return int(match.group(1))\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "    def calculate_median_time_difference(folder_path):\n",
    "        timestamps = []\n",
    "\n",
    "        for filename in os.listdir(folder_path):\n",
    "            if filename.endswith('.gz'):\n",
    "                timestamp = extract_timestamp_from_filename(filename)\n",
    "                if timestamp is not None:\n",
    "                    timestamps.append(timestamp)\n",
    "\n",
    "        if timestamps:\n",
    "            timestamps.sort()\n",
    "            time_differences = np.diff(timestamps)\n",
    "            return np.median(time_differences) if time_differences.size > 0 else None\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "    folder_path = './rfbaseline/' + monitor_foldername + \"/\"\n",
    "    median_diff = calculate_median_time_difference(folder_path)\n",
    "\n",
    "    return median_diff\n",
    "\n",
    "# Example usage of the function\n",
    "median_diff = median_time_diff_between_folders(\"Garage\")/3600\n",
    "if median_diff is not None:\n",
    "    print(f\"Median Time Difference Between Consecutive Files: {median_diff} hours\")\n",
    "else:\n",
    "    print(\"No files to calculate median time difference.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e92b8630",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import gzip\n",
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Define time intervals and seasons\n",
    "intervals = {\n",
    "    'morning': ('04:00:00', '12:00:00'),\n",
    "    'afternoon': ('12:00:00', '20:00:00'),\n",
    "    'night': ('20:00:00', '04:00:00')\n",
    "}\n",
    "seasons = {\n",
    "    'spring': (3, 6),\n",
    "    'summer': (6, 9),\n",
    "    'autumn': (9, 12),\n",
    "    'winter': (0, 3)\n",
    "}\n",
    "\n",
    "# Function to read gzipped CSV files\n",
    "def read_gzipped_csv(file_path):\n",
    "    with gzip.open(file_path, 'rt') as file:\n",
    "        df = pd.read_csv(file)\n",
    "        return df\n",
    "\n",
    "# Function to extract timestamp from filename\n",
    "def extract_timestamp_from_filename(filename):\n",
    "    match = re.search(r'-(\\d+)\\.csv\\.gz$', filename)\n",
    "    return int(match.group(1)) if match else None\n",
    "\n",
    "# Function to concatenate CSVs in a folder\n",
    "def concatenate_csvs_in_folder(folder_path):\n",
    "    dataframes = []\n",
    "    cutoff_date = pd.Timestamp('2023-01-01')\n",
    "\n",
    "    for filename in os.listdir(folder_path):\n",
    "        if filename.endswith('.gz'):\n",
    "            file_path = os.path.join(folder_path, filename)\n",
    "            timestamp = extract_timestamp_from_filename(filename)\n",
    "            df = read_gzipped_csv(file_path)\n",
    "            df['timestamp'] = pd.to_datetime(timestamp, unit='s')\n",
    "            df = df.dropna()\n",
    "            df = df[df['timestamp'] >= cutoff_date]\n",
    "            dataframes.append(df)\n",
    "\n",
    "    return pd.concat(dataframes, ignore_index=True) if dataframes else pd.DataFrame()\n",
    "\n",
    "# Function to calculate linearly varying threshold\n",
    "def linear_threshold(freq, start, end, threshold_start, threshold_end):\n",
    "    return threshold_start + ((threshold_end - threshold_start) * (freq - start) / (end - start))\n",
    "\n",
    "# Function to process data for a single monitor\n",
    "def process_monitor_data(monitor_foldername, threshold_start, threshold_end, band_start, band_end):\n",
    "    folder_path = './rfbaseline/' + monitor_foldername + \"/\"\n",
    "    combined_df = concatenate_csvs_in_folder(folder_path)\n",
    "\n",
    "    results = {'time_of_day': {}, 'season': {}}\n",
    "\n",
    "    # Time-of-Day Analysis\n",
    "    for period, (start_time, end_time) in intervals.items():\n",
    "        if period == 'night':\n",
    "            mask = ((combined_df['timestamp'].dt.time >= pd.to_datetime(start_time).time()) |\n",
    "                    (combined_df['timestamp'].dt.time < pd.to_datetime(end_time).time()))\n",
    "        else:\n",
    "            mask = ((combined_df['timestamp'].dt.time >= pd.to_datetime(start_time).time()) & \n",
    "                    (combined_df['timestamp'].dt.time < pd.to_datetime(end_time).time()))\n",
    "\n",
    "        dfs_period = combined_df.loc[mask]\n",
    "        dfs_period = dfs_period[(dfs_period['frequency'] >= band_start) & (dfs_period['frequency'] <= band_end)]\n",
    "\n",
    "        dfs_period['threshold'] = dfs_period['frequency'].apply(lambda freq: linear_threshold(freq, band_start, band_end, threshold_start, threshold_end))\n",
    "        occupied_df = dfs_period[dfs_period['power'] > dfs_period['threshold']]\n",
    "        \n",
    "        duty_cycle = len(occupied_df) / len(dfs_period) * 100 if len(dfs_period) > 0 else 0\n",
    "        avg_power_occupied = np.mean(occupied_df['power']) if not occupied_df.empty else 0\n",
    "        variance_power_occupied = np.var(occupied_df['power']) if not occupied_df.empty else 0\n",
    "\n",
    "        results['time_of_day'][period] = {\n",
    "            'Duty_Cycle': duty_cycle,\n",
    "            'Avg_Power_Occupied': avg_power_occupied,\n",
    "            'Variance_Power_Occupied': variance_power_occupied\n",
    "        }\n",
    "\n",
    "    # Season Analysis\n",
    "    for season, (start_month, end_month) in seasons.items():\n",
    "        mask = ((combined_df['timestamp'].dt.month >= start_month) &\n",
    "                (combined_df['timestamp'].dt.month < end_month))\n",
    "        df_season = combined_df.loc[mask]\n",
    "        df_season = df_season[(df_season['frequency'] >= band_start) & (df_season['frequency'] <= band_end)]\n",
    "\n",
    "        df_season['threshold'] = df_season['frequency'].apply(lambda freq: linear_threshold(freq, band_start, band_end, threshold_start, threshold_end))\n",
    "        occupied_df = df_season[df_season['power'] > df_season['threshold']]\n",
    "        \n",
    "        duty_cycle = len(occupied_df) / len(df_season) * 100 if len(df_season) > 0 else 0\n",
    "        avg_power_occupied = np.mean(occupied_df['power']) if not occupied_df.empty else 0\n",
    "        variance_power_occupied = np.var(occupied_df['power']) if not occupied_df.empty else 0\n",
    "\n",
    "        results['season'][season] = {\n",
    "            'Duty_Cycle': duty_cycle,\n",
    "            'Avg_Power_Occupied': avg_power_occupied,\n",
    "            'Variance_Power_Occupied': variance_power_occupied\n",
    "        }\n",
    "\n",
    "    return results\n",
    "\n",
    "# Main function to process all monitors and plot results\n",
    "def main():\n",
    "    sns.set_theme()\n",
    "    monitors = ['Bookstore', 'EBC', 'Guesthouse', 'Moran', 'WEB', 'Sagepoint', 'Law73', 'Humanities', 'Madsen', 'Garage']  # Add your monitor folder names here\n",
    "    threshold_start = -105  # Set your threshold start\n",
    "    threshold_end = -105  # Set your threshold end\n",
    "    band_start = 3610  # Set your band start\n",
    "    band_end = 3650  # Set your band end\n",
    "\n",
    "    all_results = {}\n",
    "\n",
    "    # Process data for each monitor\n",
    "    for monitor in monitors:\n",
    "        all_results[monitor] = process_monitor_data(monitor, threshold_start, threshold_end, band_start, band_end)\n",
    "\n",
    "    # Plotting for both time-of-day and season analysis\n",
    "    fig, axs = plt.subplots(6, 1, figsize=(15, 30))  # Create 6 subplots vertically\n",
    "    fig.suptitle('RF Monitor Metrics at Different Times of Day and Seasons')\n",
    "\n",
    "    metrics = ['Duty_Cycle', 'Avg_Power_Occupied', 'Variance_Power_Occupied']\n",
    "    for idx, metric in enumerate(metrics):\n",
    "        for monitor, results in all_results.items():\n",
    "            times_of_day = list(results['time_of_day'].keys())\n",
    "            values = [results['time_of_day'][time][metric] for time in times_of_day]\n",
    "            axs[idx].plot(times_of_day, values, label=monitor)\n",
    "\n",
    "        axs[idx].set_title(metric + ' (Time of Day)')\n",
    "        axs[idx].set_xlabel('Time of Day')\n",
    "        axs[idx].set_ylabel(metric)\n",
    "        axs[idx].legend()\n",
    "\n",
    "    for idx, metric in enumerate(metrics):\n",
    "        for monitor, results in all_results.items():\n",
    "            season_names = list(results['season'].keys())\n",
    "            values = [results['season'][season][metric] for season in season_names]\n",
    "            axs[idx + 3].plot(season_names, values, label=monitor)\n",
    "\n",
    "        axs[idx + 3].set_title(metric + ' (Season)')\n",
    "        axs[idx + 3].set_xlabel('Season')\n",
    "        axs[idx + 3].set_ylabel(metric)\n",
    "        axs[idx + 3].legend()\n",
    "\n",
    "    plt.tight_layout(rect=[0, 0.03, 1, 0.95])  # Adjust layout to make room for the main title\n",
    "    plt.show()\n",
    "    \n",
    "    return all_results\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    all_results = main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6670dc01",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"monitoring_metrics_day_season_36103650.npy\", all_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e9de11c",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_results_dummy = np.load(\"monitoring_metrics_day_season.npy\", allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17ef6ea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "070447d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_results_dummy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c50bd40b",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "\n",
    "custom = {\"axes.edgecolor\": \"black\"}\n",
    "sns.set_style(\"whitegrid\", rc=custom)\n",
    "matplotlib.rcParams.update({'font.size': 18})\n",
    "\n",
    "fig, axs = plt.subplots(2, 3, figsize=(20, 15))  # Create 6 subplots\n",
    "fig.suptitle('RF Monitoring Metrics at Different Times of Day and Seasons, 3610-3650 MHz', fontsize=30)\n",
    "\n",
    "metrics = ['Avg_Power_Occupied', 'Duty_Cycle', 'Variance_Power_Occupied']\n",
    "metric_units = ['dBX', 'Percentage', 'dBX$^2$']\n",
    "metric_labels = ['Average Occupancy Power', 'Duty Cycle', 'Signal Variance']\n",
    "colors = ['tab:blue', 'tab:orange', 'tab:green', 'tab:red', 'tab:purple', 'tab:brown', 'tab:pink', 'tab:gray', 'tab:olive', 'tab:cyan']  # Add more colors if needed\n",
    "avg_color = 'black'  # Color for the average line\n",
    "\n",
    "legend_labels = []\n",
    "legend_lines = []\n",
    "\n",
    "# Plot individual lines and calculate averages for the top row (Time of Day)\n",
    "for idx, metric in enumerate(metrics):\n",
    "    all_values = []  # To store values for calculating average\n",
    "    for monitor_idx, (monitor, results) in enumerate(all_results.items()):\n",
    "        times_of_day = list(results['time_of_day'].keys())\n",
    "        values = [results['time_of_day'][time][metric] for time in times_of_day]\n",
    "        all_values.append(values)\n",
    "        line, = axs[0, idx].plot(times_of_day, values, color=colors[monitor_idx], linewidth=2, linestyle='solid')\n",
    "        if idx == 0:\n",
    "            legend_labels.append(monitor)\n",
    "            legend_lines.append(line)\n",
    "\n",
    "    # Calculate and plot average line\n",
    "    average_values = np.mean(all_values, axis=0)\n",
    "    axs[0, idx].plot(times_of_day, average_values, color=avg_color, linewidth=3.5, linestyle='--', label='Average')\n",
    "\n",
    "    axs[0, idx].set_title(metric_labels[idx] + ' (Time of Day)', fontsize=20)\n",
    "    axs[0, idx].set_xlabel('Time of Day', fontsize=18)\n",
    "    axs[0, idx].set_ylabel(metric_units[idx], fontsize=18)\n",
    "    axs[0, idx].grid(True)\n",
    "    axs[0, idx].tick_params(axis='both', labelsize=16)\n",
    "    axs[0, idx].text(-0.1, 1.1, f'({chr(97 + idx)})', transform=axs[0, idx].transAxes, fontsize=20, fontweight='bold', va='top', ha='right')\n",
    "\n",
    "# Add legend for the top row\n",
    "fig.legend(legend_lines + [plt.Line2D([0], [0], color=avg_color, linestyle='--', linewidth=3)], legend_labels + ['Average'], loc='lower center', ncol=len(legend_labels) + 1, fontsize=16, bbox_to_anchor=(0.5, 0.905))\n",
    "\n",
    "# Clear legend lines and labels for the bottom row\n",
    "legend_lines.clear()\n",
    "legend_labels.clear()\n",
    "\n",
    "# Plot individual lines and calculate averages for the bottom row (Season)\n",
    "for idx, metric in enumerate(metrics):\n",
    "    all_values = []  # To store values for calculating average\n",
    "    for monitor_idx, (monitor, results) in enumerate(all_results.items()):\n",
    "        season_names = list(results['season'].keys())\n",
    "        values = [results['season'][season][metric] for season in season_names]\n",
    "        all_values.append(values)\n",
    "        line, = axs[1, idx].plot(season_names, values, color=colors[monitor_idx], linewidth=2, linestyle='dashed')\n",
    "        if idx == 0:\n",
    "            legend_labels.append(monitor)\n",
    "            legend_lines.append(line)\n",
    "\n",
    "    # Calculate and plot average line\n",
    "    average_values = np.mean(all_values, axis=0)\n",
    "    axs[1, idx].plot(season_names, average_values, color=avg_color, linewidth=3.5, linestyle='-', label='Average')\n",
    "\n",
    "    axs[1, idx].set_title(metric_labels[idx] + ' (Season)', fontsize=20)\n",
    "    axs[1, idx].set_xlabel('Season', fontsize=18)\n",
    "    axs[1, idx].set_ylabel(metric_units[idx], fontsize=18)\n",
    "    axs[1, idx].grid(True)\n",
    "\n",
    "    # Adjust tick parameters\n",
    "    axs[1, idx].tick_params(axis='both', labelsize=16)\n",
    "    axs[1, idx].text(-0.1, 1.1, f'({chr(100 + idx)})', transform=axs[1, idx].transAxes, fontsize=20, fontweight='bold', va='bottom', ha='right')\n",
    "\n",
    "    \n",
    "\n",
    "# Add legend for the bottom row\n",
    "fig.legend(legend_lines+ [plt.Line2D([0], [0], color=avg_color, linestyle='-', linewidth=3)], legend_labels+ ['Average'], loc='lower center', ncol=len(legend_labels)+1, fontsize=16, bbox_to_anchor=(0.5, 0.04))\n",
    "\n",
    "# Adjust layout to make room for the main title, legends and improve spacing\n",
    "plt.tight_layout(rect=[0, 0.08, 1, 0.95])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ff9c5ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "var_3470_3510 = np.array([20.95,\n",
    "18.4,\n",
    "25.86,\n",
    "9.8,\n",
    "22.53,\n",
    "13.13,\n",
    "15.03,\n",
    "16.24,\n",
    "21.51,\n",
    "23\n",
    "])\n",
    "\n",
    "\n",
    "var_3610_3650 = np.array([12.64,\n",
    "16.45,\n",
    "5.27,\n",
    "8.38,\n",
    "28.14,\n",
    "10.46,\n",
    "12.16,\n",
    "18.02,\n",
    "4.31,\n",
    "33.2\n",
    "])\n",
    "\n",
    "\n",
    "var_2160_2170 = np.array([49.69,\n",
    "4.03,\n",
    "3.16,\n",
    "24.47,\n",
    "7.04,\n",
    "2.72,\n",
    "16.28,\n",
    "7.33,\n",
    "23.37,\n",
    "3.74\n",
    "])\n",
    "\n",
    "\n",
    "rssi_3470_3510 = np.array([-85.53,\n",
    "-95.92,\n",
    "-96.29,\n",
    "-101.19,\n",
    "-93.44,\n",
    "-99.88,\n",
    "-88.62,\n",
    "-99.78,\n",
    "-96.45,\n",
    "-88.8\n",
    "])\n",
    "\n",
    "\n",
    "rssi_3610_3650 = np.array([-84.93,\n",
    "-98.39,\n",
    "-101.84,\n",
    "-101.58,\n",
    "-92.01,\n",
    "-101.67,\n",
    "-86.77,\n",
    "-98.45,\n",
    "-102.38,\n",
    "-89.61\n",
    "])\n",
    "\n",
    "rssi_2160_2170 = np.array([-85.43,\n",
    "-76.88,\n",
    "-76.22,\n",
    "-86.68,\n",
    "-78.85,\n",
    "-80.78,\n",
    "-79.53,\n",
    "-74.71,\n",
    "-80.88,\n",
    "-72.33\n",
    "])\n",
    "\n",
    "dc_3470_3510 = np.array([45.25,\n",
    "98.68,\n",
    "96.1,\n",
    "37.4,\n",
    "45.42,\n",
    "85.82,\n",
    "44.84,\n",
    "74.81,\n",
    "67.82,\n",
    "100\n",
    "])\n",
    "\n",
    "\n",
    "dc_3610_3650 = np.array([45.25,\n",
    "92.52,\n",
    "77.09,\n",
    "23.34,\n",
    "45.41,\n",
    "55.23,\n",
    "44.84,\n",
    "85.13,\n",
    "38.63,\n",
    "100\n",
    "])\n",
    "\n",
    "dc_2160_2170 = np.array([99.89,\n",
    "100,\n",
    "100,\n",
    "97.74,\n",
    "45.42,\n",
    "100,\n",
    "100,\n",
    "100,\n",
    "100,\n",
    "100\n",
    "])\n",
    "\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "print(\"Var-RSSI-3470-3510\")\n",
    "print(pearsonr(var_3470_3510,rssi_3470_3510))\n",
    "print(\"Var-DC-3470-3510\")\n",
    "print(pearsonr(var_3470_3510,dc_3470_3510))\n",
    "print(\"DC-RSSI-3470-3510\")\n",
    "print(pearsonr(dc_3470_3510,rssi_3470_3510))\n",
    "\n",
    "print(\"Var-RSSI-3610-3650\")\n",
    "print(pearsonr(var_3610_3650,rssi_3610_3650))\n",
    "print(\"Var-DC-3610-3650\")\n",
    "print(pearsonr(var_3610_3650,dc_3610_3650))\n",
    "print(\"DC-RSSI-3610-3650\")\n",
    "print(pearsonr(dc_3610_3650,rssi_3610_3650))\n",
    "\n",
    "print(\"Var-RSSI-2160-2170\")\n",
    "print(pearsonr(var_2160_2170,rssi_2160_2170))\n",
    "print(\"Var-DC-3610-3650\")\n",
    "print(pearsonr(var_2160_2170,dc_2160_2170))\n",
    "print(\"DC-RSSI-3610-3650\")\n",
    "print(pearsonr(dc_2160_2170,rssi_2160_2170))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50b733b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import Ridge, Lasso  # Importing Ridge\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "# Non-linear regression model using polynomial features with Ridge regularization\n",
    "degree = 3  # Degree of polynomial features\n",
    "alpha = 0.05  # Regularization strength\n",
    "\n",
    "model_3470_3510 = make_pipeline(PolynomialFeatures(degree), Lasso(alpha=alpha))\n",
    "model_3610_3650 = make_pipeline(PolynomialFeatures(degree), Lasso(alpha=alpha))\n",
    "model_2160_2170 = make_pipeline(PolynomialFeatures(degree), Lasso(alpha=alpha))\n",
    "\n",
    "# Reshaping the data for the model\n",
    "X_3470_3510 = rssi_3470_3510.reshape(-1, 1)\n",
    "X_3610_3650 = rssi_3610_3650.reshape(-1, 1)\n",
    "X_2160_2170 = rssi_2160_2170.reshape(-1, 1)\n",
    "\n",
    "# Fitting the models\n",
    "model_3470_3510.fit(X_3470_3510, var_3470_3510)\n",
    "model_3610_3650.fit(X_3610_3650, var_3610_3650)\n",
    "model_2160_2170.fit(X_2160_2170, var_2160_2170)\n",
    "\n",
    "# Generating predictions for plotting\n",
    "X_plot = np.linspace(-95, -70, 100).reshape(-1, 1)\n",
    "y_pred_3470_3510 = model_3470_3510.predict(X_plot)\n",
    "y_pred_3610_3650 = model_3610_3650.predict(X_plot)\n",
    "y_pred_2160_2170 = model_2160_2170.predict(X_plot)\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(18, 6))\n",
    "\n",
    "# Adjusting font sizes\n",
    "plt.rcParams.update({'font.size': 18})  # Adjust this value as needed\n",
    "\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.scatter(rssi_3470_3510, var_3470_3510, color='blue', label='Data 3470-3510 MHz', s=45)\n",
    "plt.plot(X_plot, y_pred_3470_3510, color='deepskyblue', label='Model Prediction', linewidth=3.5)\n",
    "plt.xlabel('RSSI [dBX]', fontsize=20)\n",
    "plt.ylabel('Variance [dBX$^2$]', fontsize=20)\n",
    "plt.title('Regression for 3470-3510 MHz', fontsize=22)\n",
    "plt.legend(fontsize=18, loc='lower right')\n",
    "plt.tick_params(axis='both', which='major', labelsize=18)  # Adjust the size of ticks\n",
    "\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.scatter(rssi_3610_3650, var_3610_3650, color='green', label='Data 3610-3650 MHz', s=45)\n",
    "plt.plot(X_plot, y_pred_3610_3650, color='limegreen', label='Model Prediction', linewidth=3.5)\n",
    "plt.xlabel('RSSI [dBX]', fontsize=20)\n",
    "plt.ylabel('Variance [dBX$^2$]', fontsize=20)\n",
    "plt.title('Regression for 3610-3650 MHz', fontsize=22)\n",
    "plt.legend(fontsize=18, loc='lower right')\n",
    "plt.tick_params(axis='both', which='major', labelsize=18)  # Adjust the size of ticks\n",
    "\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.scatter(rssi_2160_2170, var_2160_2170, color='red', label='Data 3610-3650 MHz', s=45)\n",
    "plt.plot(X_plot, y_pred_2160_2170, color='darkred', label='Model Prediction', linewidth=3.5)\n",
    "plt.xlabel('RSSI [dBX]', fontsize=20)\n",
    "plt.ylabel('Variance [dBX$^2$]', fontsize=20)\n",
    "plt.title('Regression for 2160-2170 MHz', fontsize=22)\n",
    "plt.legend(fontsize=18, loc='upper right')\n",
    "plt.tick_params(axis='both', which='major', labelsize=18)  # Adjust the size of ticks\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41983351",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.pipeline import make_pipeline\n",
    "import seaborn as sns\n",
    "sns.set_theme()\n",
    "custom = {\"axes.edgecolor\": \"black\"}\n",
    "sns.set_style(\"whitegrid\", rc=custom)\n",
    "# Non-linear regression model using polynomial features with Lasso regularization\n",
    "degree = 3  # Degree of polynomial features\n",
    "alpha = 0.05  # Regularization strength\n",
    "\n",
    "# Combine all datasets\n",
    "X_combined = np.concatenate([rssi_3470_3510, rssi_3610_3650, rssi_2160_2170]).reshape(-1, 1)\n",
    "var_combined = np.concatenate([var_3470_3510, var_3610_3650, var_2160_2170])\n",
    "\n",
    "# Create a single model\n",
    "combined_model = make_pipeline(PolynomialFeatures(degree), Lasso(alpha=alpha))\n",
    "\n",
    "# Fit the model to the combined dataset\n",
    "combined_model.fit(X_combined, var_combined)\n",
    "\n",
    "# Generating predictions for plotting\n",
    "X_plot = np.linspace(X_combined.min(), X_combined.max(), 100).reshape(-1, 1)\n",
    "y_pred_combined = combined_model.predict(X_plot)\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(9, 6))\n",
    "\n",
    "# Adjusting font sizes\n",
    "plt.rcParams.update({'font.size': 14})\n",
    "\n",
    "# Plotting data points\n",
    "plt.scatter(rssi_3470_3510, var_3470_3510, color='blue', label='Data 3470-3510 MHz', s=70, marker='x')\n",
    "plt.scatter(rssi_3610_3650, var_3610_3650, color='green', label='Data 3610-3650 MHz', s=70, marker = 's')\n",
    "plt.scatter(rssi_2160_2170, var_2160_2170, color='red', label='Data 2160.5-2169.5 MHz', s=70)\n",
    "\n",
    "# Plotting the model prediction\n",
    "plt.plot(X_plot, y_pred_combined, color='black', label='Combined Model Prediction', linewidth=3)\n",
    "\n",
    "plt.xlabel('RSSI [dBX]', fontsize=16)\n",
    "plt.ylabel('Variance [dBX$^2$]', fontsize=16)\n",
    "plt.title('Combined Regression Analysis', fontsize=18)\n",
    "plt.legend(fontsize=14, loc='upper left')\n",
    "plt.tick_params(axis='both', which='major', labelsize=14)\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c46c10a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.io as sio\n",
    "import os\n",
    "import warnings\n",
    "\n",
    "def calculate_confidence(map_, data_points, d_max, alpha):\n",
    "    \"\"\"\n",
    "    Calculate the confidence map based on the provided data points and the formula.\n",
    "    \"\"\"\n",
    "    confidence_map = np.zeros_like(map_)\n",
    "    for i in range(map_.shape[1]):\n",
    "        for j in range(map_.shape[0]):\n",
    "            d_p = np.min(np.sqrt((data_points[:, 0] - i)**2 + (data_points[:, 1] - j)**2))\n",
    "            confidence = (1 - min(d_p / d_max, 1)) * np.exp(-alpha * d_p)\n",
    "            confidence_map[j, i] = confidence\n",
    "    return confidence_map\n",
    "\n",
    "# Load the map\n",
    "map_folderdir = \"./\"\n",
    "directory = os.listdir(map_folderdir)\n",
    "flag = 0\n",
    "for fname in directory:\n",
    "    if \"SLCmap\" in fname:\n",
    "        map_file = os.path.join(map_folderdir, fname)\n",
    "        flag = 1\n",
    "\n",
    "if flag == 0:\n",
    "    errorMessage = 'Error: The file does not exist in the folder:\\n ' + map_folderdir\n",
    "    warnings.warn(errorMessage)\n",
    "    raise FileNotFoundError(errorMessage)\n",
    "\n",
    "x = sio.loadmat(map_file)\n",
    "map_struct = x['SLC']\n",
    "\n",
    "# Define a new struct named SLC\n",
    "SLC = map_struct[0][0]\n",
    "column_map = dict(zip([name for name in SLC.dtype.names], [i for i in range(len(SLC.dtype.names))]))\n",
    "\n",
    "# Process the map\n",
    "map_ = SLC[column_map[\"dem\"]] + 0.3048 * SLC[column_map[\"hybrid_bldg\"]]\n",
    "map_ = map_[::10, ::10]\n",
    "\n",
    "# Data points\n",
    "data_points = np.array([\n",
    "    [966, 2992],\n",
    "    [2569, 3767],\n",
    "    [2873, 3447],\n",
    "    [2621, 4286],\n",
    "    [1312, 3830],\n",
    "    [3828, 2667],\n",
    "    [242, 2442],\n",
    "    [1711, 3145],\n",
    "    [2852, 1584],\n",
    "    [1903, 2393]\n",
    "])\n",
    "data_points = data_points.astype(float)/10.0\n",
    "data_points = np.floor(data_points).astype(\"int\")\n",
    "\n",
    "# Define parameters\n",
    "d_max = 1000  # distance threshold\n",
    "alpha = 0.01  # adjustable parameter\n",
    "\n",
    "# Calculate confidence\n",
    "confidence_map = calculate_confidence(map_, data_points, d_max, alpha)\n",
    "\n",
    "# You can add additional code here to visualize or save the confidence map\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab20ed4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_x, grid_y = np.meshgrid(\n",
    "    np.linspace(0, map_.shape[1], map_.shape[1], endpoint=False),\n",
    "    np.linspace(0, map_.shape[0], map_.shape[0], endpoint=False)\n",
    ")\n",
    "\n",
    "# Plotting\n",
    "plt.figure()\n",
    "plt.contourf(grid_x, grid_y, confidence_map*100, 100, cmap='viridis')\n",
    "plt.colorbar(label='Confidence [Percentage]')\n",
    "plt.xlabel('UTM_E [m]')\n",
    "plt.ylabel('UTM_N [m]')\n",
    "plt.title('2D Predictions of Confidence Level')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2dbd3b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"confidence_level.npy\", confidence_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e9d452d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import scipy.io as sio\n",
    "import matplotlib.pyplot as plt\n",
    "map_folderdir = \"./\"\n",
    "directory = os.listdir(map_folderdir)\n",
    "flag = 0\n",
    "for fname in directory:\n",
    "    if \"SLCmap\" in fname:\n",
    "        map_file = os.path.join(map_folderdir, fname)\n",
    "        flag = 1\n",
    "\n",
    "if flag == 0:\n",
    "    errorMessage = 'Error: The file does not exist in the folder:\\n ' + map_folderdir\n",
    "    warnings.warn(errorMessage)\n",
    "\n",
    "print('Now reading ' + map_file + \"\\n\")\n",
    "x = sio.loadmat(map_file)\n",
    "map_struct = x['SLC']\n",
    "\n",
    "# Define a new struct named SLC\n",
    "SLC = map_struct[0][0]\n",
    "column_map = dict(zip([name for name in SLC.dtype.names], [i for i in range(len(SLC.dtype.names))]))\n",
    "\n",
    "map_ = SLC[column_map[\"dem\"]] + 0.3048 * SLC[column_map[\"hybrid_bldg\"]]\n",
    "map_ = map_[::10, ::10]\n",
    "\n",
    "data_points = np.array([\n",
    "    [966, 2992],\n",
    "    [2569, 3767],\n",
    "    [2873, 3447],\n",
    "    [2621, 4286],\n",
    "    [1312, 3830],\n",
    "    [3828, 2667],\n",
    "    [242, 2442],\n",
    "    [1711, 3145],\n",
    "    [2852, 1584],\n",
    "    [1903, 2393]\n",
    "])\n",
    "data_points = data_points.astype(float)/10.0\n",
    "data_points = np.floor(data_points).astype(\"int\")\n",
    "\n",
    "def lin_to_dB(pow_lin):\n",
    "    return 10*np.log10(np.array(pow_lin))\n",
    "\n",
    "band = \"2160_2170\"\n",
    "rssi = lin_to_dB(np.load(\"signal_estimates_\"+band+\"_low_res_cov2.npy\"))\n",
    "y_pred_2160_2170 = combined_model.predict(rssi.reshape(-1, 1)).reshape(map_.shape)\n",
    "\n",
    "grid_x, grid_y = np.meshgrid(\n",
    "    np.linspace(0, map_.shape[1], map_.shape[1], endpoint=False),\n",
    "    np.linspace(0, map_.shape[0], map_.shape[0], endpoint=False)\n",
    ")\n",
    "\n",
    "# Plotting\n",
    "plt.figure()\n",
    "plt.contourf(grid_x, grid_y, y_pred_2160_2170, 100, cmap='viridis')\n",
    "plt.colorbar(label='Variance [dBX$^2$]')\n",
    "plt.scatter(data_points[:, 0], data_points[:, 1], c=var_2160_2170, s=50, cmap='plasma', label='Data Collection Points')\n",
    "plt.xlabel('UTM_E [m]')\n",
    "plt.ylabel('UTM_N [m]')\n",
    "plt.title('2D Predictions of Variance')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bf88c73",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import scipy.io as sio\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from scipy.interpolate import griddata\n",
    "from scipy.spatial import cKDTree\n",
    "\n",
    "# IDW interpolation function with distance threshold\n",
    "def idw_interpolation(x, y, z, xi, yi, max_distance):\n",
    "    # Create a KDTree for efficient neighbor search\n",
    "    tree = cKDTree(np.column_stack((x, y)))\n",
    "    \n",
    "    # Initialize output array\n",
    "    zi = np.zeros_like(xi)\n",
    "\n",
    "    # Iterate over all points in the output grid\n",
    "    for i in range(len(xi)):\n",
    "        # Radius-based query to find neighbors within max_distance\n",
    "        indices = tree.query_ball_point((xi[i], yi[i]), r=max_distance)\n",
    "        \n",
    "        # Get the distances and corresponding z values\n",
    "        distances = np.sqrt((x[indices] - xi[i])**2 + (y[indices] - yi[i])**2)*5\n",
    "        valid_z = z[indices]\n",
    "\n",
    "        # Compute weights and interpolate\n",
    "        weights = 1.0 / (distances**2 + 1e-12)\n",
    "        weights_sum = np.sum(weights)\n",
    "        \n",
    "        if weights_sum > 0 and len(valid_z) > 0:\n",
    "            zi[i] = np.sum(weights * valid_z) / weights_sum\n",
    "        else:\n",
    "            zi[i] = np.nan  # or some default value if needed\n",
    "\n",
    "    return zi\n",
    "# Mask for values below 0 in y_pred_3470_3510\n",
    "mask = y_pred_2160_2170 < 3\n",
    "\n",
    "# Grid coordinates\n",
    "grid_x, grid_y = np.meshgrid(\n",
    "    np.linspace(0, map_.shape[1], map_.shape[1], endpoint=False),\n",
    "    np.linspace(0, map_.shape[0], map_.shape[0], endpoint=False)\n",
    ")\n",
    "\n",
    "# Flatten grid coordinates\n",
    "flat_grid_x = grid_x.ravel()\n",
    "flat_grid_y = grid_y.ravel()\n",
    "\n",
    "# Positive values and their coordinates\n",
    "pos_values = y_pred_2160_2170[~mask]\n",
    "pos_x = flat_grid_x[~mask.ravel()]\n",
    "pos_y = flat_grid_y[~mask.ravel()]\n",
    "\n",
    "# Interpolate using IDW\n",
    "interpolated_values = idw_interpolation(pos_x, pos_y, pos_values, flat_grid_x[mask.ravel()], flat_grid_y[mask.ravel()], max_distance=200.0)\n",
    "\n",
    "# Replace negative values in y_pred_3470_3510 with interpolated values\n",
    "y_pred_2160_2170[mask] = interpolated_values\n",
    "\n",
    "# Plotting (remains the same)\n",
    "plt.figure()\n",
    "plt.contourf(grid_x, grid_y, y_pred_2160_2170, 100, cmap='viridis')\n",
    "plt.colorbar(label='Variance [dBX$^2$]')\n",
    "plt.scatter(data_points[:, 0], data_points[:, 1], c=var_2160_2170, s=50, cmap='plasma', label='Data Collection Points')\n",
    "plt.xlabel('UTM_E [m]')\n",
    "plt.ylabel('UTM_N [m]')\n",
    "plt.title('2D Predictions of Variance')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a0b7164",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"var_pred_2160_2170_3.npy\",y_pred_2160_2170)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "abc1b5e6",
   "metadata": {},
   "source": [
    "import gzip\n",
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Define the time intervals\n",
    "intervals = {\n",
    "    'morning': ('04:00:00', '12:00:00'),\n",
    "    'afternoon': ('12:00:00', '20:00:00'),\n",
    "    'night': ('20:00:00', '04:00:00')\n",
    "}\n",
    "\n",
    "# Function to read gzipped CSV files\n",
    "def read_gzipped_csv(file_path):\n",
    "    with gzip.open(file_path, 'rt') as file:\n",
    "        df = pd.read_csv(file)\n",
    "        return df\n",
    "\n",
    "# Function to extract timestamp from filename\n",
    "def extract_timestamp_from_filename(filename):\n",
    "    match = re.search(r'-(\\d+)\\.csv\\.gz$', filename)\n",
    "    return int(match.group(1)) if match else None\n",
    "\n",
    "# Function to concatenate CSVs in a folder\n",
    "def concatenate_csvs_in_folder(folder_path):\n",
    "    dataframes = []\n",
    "    cutoff_date = pd.Timestamp('2023-01-01')\n",
    "\n",
    "    for filename in os.listdir(folder_path):\n",
    "        if filename.endswith('.gz'):\n",
    "            file_path = os.path.join(folder_path, filename)\n",
    "            timestamp = extract_timestamp_from_filename(filename)\n",
    "            df = read_gzipped_csv(file_path)\n",
    "            df['timestamp'] = pd.to_datetime(timestamp, unit='s')\n",
    "            df = df.dropna()\n",
    "            df = df[df['timestamp'] >= cutoff_date]\n",
    "            dataframes.append(df)\n",
    "\n",
    "    return pd.concat(dataframes, ignore_index=True) if dataframes else pd.DataFrame()\n",
    "\n",
    "# Function to calculate linearly varying threshold\n",
    "def linear_threshold(freq, start, end, threshold_start, threshold_end):\n",
    "    return threshold_start + ((threshold_end - threshold_start) * (freq - start) / (end - start))\n",
    "\n",
    "# Function to process data for a single monitor\n",
    "def process_monitor_data(monitor_foldername, threshold_start, threshold_end, band_start, band_end):\n",
    "    folder_path = './rfbaseline/' + monitor_foldername + \"/\"\n",
    "    combined_df = concatenate_csvs_in_folder(folder_path)\n",
    "\n",
    "    results = {}\n",
    "    for period, (start_time, end_time) in intervals.items():\n",
    "        if period == 'night':\n",
    "            mask = ((combined_df['timestamp'].dt.time >= pd.to_datetime(start_time).time()) |\n",
    "                    (combined_df['timestamp'].dt.time < pd.to_datetime(end_time).time()))\n",
    "        else:\n",
    "            mask = ((combined_df['timestamp'].dt.time >= pd.to_datetime(start_time).time()) & \n",
    "                    (combined_df['timestamp'].dt.time < pd.to_datetime(end_time).time()))\n",
    "\n",
    "        dfs_period = combined_df.loc[mask]\n",
    "        dfs_period = dfs_period[(dfs_period['frequency'] >= band_start) & (dfs_period['frequency'] <= band_end)]\n",
    "\n",
    "        dfs_period['threshold'] = dfs_period['frequency'].apply(lambda freq: linear_threshold(freq, band_start, band_end, threshold_start, threshold_end))\n",
    "        occupied_df = dfs_period[dfs_period['power'] > dfs_period['threshold']]\n",
    "        \n",
    "        duty_cycle = len(occupied_df) / len(dfs_period) * 100 if len(dfs_period) > 0 else 0\n",
    "        avg_power_occupied = np.mean(occupied_df['power']) if not occupied_df.empty else 0\n",
    "        variance_power_occupied = np.var(occupied_df['power']) if not occupied_df.empty else 0\n",
    "\n",
    "        results[period] = {\n",
    "            'Duty_Cycle': duty_cycle,\n",
    "            'Avg_Power_Occupied': avg_power_occupied,\n",
    "            'Variance_Power_Occupied': variance_power_occupied\n",
    "        }\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "# Main function to process all monitors and plot results\n",
    "def main():\n",
    "    sns.set_theme()\n",
    "    monitors = ['Bookstore', 'EBC', 'Guesthouse', 'Moran', 'WEB', 'Sagepoint', 'Law73', 'Humanities', 'Madsen', 'Garage']  # Add your monitor folder names here\n",
    "    threshold_start = -105  # Set your threshold start\n",
    "    threshold_end = -105  # Set your threshold end\n",
    "    band_start = 3470  # Set your band start\n",
    "    band_end = 3510  # Set your band end\n",
    "\n",
    "    all_results = {}\n",
    "\n",
    "    # Process data for each monitor\n",
    "    for monitor in monitors:\n",
    "        all_results[monitor] = process_monitor_data(monitor, threshold_start, threshold_end, band_start, band_end)\n",
    "\n",
    "    # Set up the subplot figure\n",
    "    fig, axs = plt.subplots(3, 1, figsize=(12, 18))  # Create 3 subplots vertically\n",
    "    fig.suptitle('RF Monitor Metrics at Different Times of Day')\n",
    "\n",
    "    metrics = ['Duty_Cycle', 'Avg_Power_Occupied', 'Variance_Power_Occupied']\n",
    "    for idx, metric in enumerate(metrics):\n",
    "        for monitor, results in all_results.items():\n",
    "            times_of_day = list(results.keys())\n",
    "            values = [results[time][metric] for time in times_of_day]\n",
    "            axs[idx].plot(times_of_day, values, label=monitor)\n",
    "\n",
    "        axs[idx].set_title(metric)\n",
    "        axs[idx].set_xlabel('Time of Day')\n",
    "        axs[idx].set_ylabel(metric)\n",
    "        axs[idx].legend()\n",
    "\n",
    "    plt.tight_layout(rect=[0, 0.03, 1, 0.95])  # Adjust layout to make room for the main title\n",
    "    plt.show()\n",
    "    \n",
    "    return all_results\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    all_results = main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8eba7af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Define the time intervals\n",
    "intervals = {\n",
    "    'morning': ('04:00:00', '12:00:00'),\n",
    "    'afternoon': ('12:00:00', '20:00:00'),\n",
    "    'night': ('20:00:00', '04:00:00')\n",
    "}\n",
    "\n",
    "# Function to read gzipped CSV files\n",
    "def read_gzipped_csv(file_path):\n",
    "    with gzip.open(file_path, 'rt') as file:\n",
    "        df = pd.read_csv(file)\n",
    "        return df\n",
    "\n",
    "# Function to extract timestamp from filename\n",
    "def extract_timestamp_from_filename(filename):\n",
    "    match = re.search(r'-(\\d+)\\.csv\\.gz$', filename)\n",
    "    return int(match.group(1)) if match else None\n",
    "\n",
    "# Function to concatenate CSVs in a folder\n",
    "def concatenate_csvs_in_folder(folder_path):\n",
    "    dataframes = []\n",
    "    cutoff_date = pd.Timestamp('2023-01-01')\n",
    "\n",
    "    for filename in os.listdir(folder_path):\n",
    "        if filename.endswith('.gz'):\n",
    "            file_path = os.path.join(folder_path, filename)\n",
    "            timestamp = extract_timestamp_from_filename(filename)\n",
    "            df = read_gzipped_csv(file_path)\n",
    "            df['timestamp'] = pd.to_datetime(timestamp, unit='s')\n",
    "            df = df.dropna()\n",
    "            df = df[df['timestamp'] >= cutoff_date]\n",
    "            dataframes.append(df)\n",
    "\n",
    "    return pd.concat(dataframes, ignore_index=True) if dataframes else pd.DataFrame()\n",
    "\n",
    "# Function to calculate linearly varying threshold\n",
    "def linear_threshold(freq, start, end, threshold_start, threshold_end):\n",
    "    return threshold_start + ((threshold_end - threshold_start) * (freq - start) / (end - start))\n",
    "\n",
    "# Function to process data for a single monitor\n",
    "def process_monitor_data(monitor_foldername, threshold_start, threshold_end, band_start, band_end):\n",
    "    folder_path = './rfbaseline/' + monitor_foldername + \"/\"\n",
    "    combined_df = concatenate_csvs_in_folder(folder_path)\n",
    "\n",
    "    results = {}\n",
    "    for period, (start_time, end_time) in intervals.items():\n",
    "        if period == 'night':\n",
    "            mask = ((combined_df['timestamp'].dt.time >= pd.to_datetime(start_time).time()) |\n",
    "                    (combined_df['timestamp'].dt.time < pd.to_datetime(end_time).time()))\n",
    "        else:\n",
    "            mask = ((combined_df['timestamp'].dt.time >= pd.to_datetime(start_time).time()) & \n",
    "                    (combined_df['timestamp'].dt.time < pd.to_datetime(end_time).time()))\n",
    "\n",
    "        dfs_period = combined_df.loc[mask]\n",
    "        dfs_period = dfs_period[(dfs_period['frequency'] >= band_start) & (dfs_period['frequency'] <= band_end)]\n",
    "\n",
    "        dfs_period['threshold'] = dfs_period['frequency'].apply(lambda freq: linear_threshold(freq, band_start, band_end, threshold_start, threshold_end))\n",
    "        occupied_df = dfs_period[dfs_period['power'] > dfs_period['threshold']]\n",
    "        \n",
    "        duty_cycle = len(occupied_df) / len(dfs_period) * 100 if len(dfs_period) > 0 else 0\n",
    "        avg_power_occupied = np.mean(occupied_df['power']) if not occupied_df.empty else 0\n",
    "        variance_power_occupied = np.var(occupied_df['power']) if not occupied_df.empty else 0\n",
    "\n",
    "        results[period] = {\n",
    "            'Duty_Cycle': duty_cycle,\n",
    "            'Avg_Power_Occupied': avg_power_occupied,\n",
    "            'Variance_Power_Occupied': variance_power_occupied\n",
    "        }\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "# Main function to process all monitors and plot results\n",
    "def main():\n",
    "    sns.set_theme()\n",
    "    monitors = ['Bookstore', 'EBC', 'Guesthouse', 'Moran', 'WEB', 'Sagepoint', 'Law73', 'Humanities', 'Madsen', 'Garage']  # Add your monitor folder names here\n",
    "    threshold_start = -105  # Set your threshold start\n",
    "    threshold_end = -105  # Set your threshold end\n",
    "    band_start = 3470  # Set your band start\n",
    "    band_end = 3510  # Set your band end\n",
    "\n",
    "    all_results = {}\n",
    "\n",
    "    # Process data for each monitor\n",
    "    for monitor in monitors:\n",
    "        all_results[monitor] = process_monitor_data(monitor, threshold_start, threshold_end, band_start, band_end)\n",
    "\n",
    "    # Set up the subplot figure\n",
    "    fig, axs = plt.subplots(3, 1, figsize=(12, 18))  # Create 3 subplots vertically\n",
    "    fig.suptitle('RF Monitor Metrics at Different Times of Day')\n",
    "\n",
    "    metrics = ['Duty_Cycle', 'Avg_Power_Occupied', 'Variance_Power_Occupied']\n",
    "    for idx, metric in enumerate(metrics):\n",
    "        for monitor, results in all_results.items():\n",
    "            times_of_day = list(results.keys())\n",
    "            values = [results[time][metric] for time in times_of_day]\n",
    "            axs[idx].plot(times_of_day, values, label=monitor)\n",
    "\n",
    "        axs[idx].set_title(metric)\n",
    "        axs[idx].set_xlabel('Time of Day')\n",
    "        axs[idx].set_ylabel(metric)\n",
    "        axs[idx].legend()\n",
    "\n",
    "    plt.tight_layout(rect=[0, 0.03, 1, 0.95])  # Adjust layout to make room for the main title\n",
    "    plt.show()\n",
    "    \n",
    "    return all_results\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    all_results = main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bcdaba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "for metric in metrics:\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    for monitor, results in all_results.items():\n",
    "        times_of_day = list(results.keys())\n",
    "        values = [results[time][metric] for time in times_of_day]\n",
    "        plt.plot(times_of_day, values, label=monitor)\n",
    "\n",
    "    plt.title(f'{metric} for Each Monitor at Different Times of Day')\n",
    "    plt.xlabel('Time of Day')\n",
    "    plt.ylabel(metric)\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "769da6dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import matplotlib\n",
    "\n",
    "def occupancy(monitor_foldername, threshold_start, threshold_end, band_start, band_end):\n",
    "    sns.set_theme()\n",
    "\n",
    "    def read_gzipped_csv(file_path):\n",
    "        with gzip.open(file_path, 'rt') as file:\n",
    "            df = pd.read_csv(file)\n",
    "            return df\n",
    "\n",
    "\n",
    "    def extract_timestamp_from_filename(filename):\n",
    "        # Extract the Unix timestamp using a regular expression\n",
    "        match = re.search(r'-(\\d+)\\.csv\\.gz$', filename)\n",
    "        if match:\n",
    "            return int(match.group(1))\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "\n",
    "    def concatenate_csvs_in_folder(folder_path):\n",
    "        dataframes = []\n",
    "        cutoff_date = pd.Timestamp('2023-01-01')  # Set the cutoff date to Jan 1, 2023\n",
    "\n",
    "        for filename in os.listdir(folder_path):\n",
    "            if filename.endswith('.gz'):\n",
    "                file_path = os.path.join(folder_path, filename)\n",
    "                timestamp = extract_timestamp_from_filename(filename)\n",
    "                df = read_gzipped_csv(file_path)\n",
    "                df['timestamp'] = pd.to_datetime(timestamp, unit='s')  # Convert timestamp to datetime\n",
    "\n",
    "                # Drop rows with NaN values\n",
    "                df = df.dropna()\n",
    "                # Filter out rows older than June 2022\n",
    "                df = df[df['timestamp'] >= cutoff_date]\n",
    "\n",
    "                dataframes.append(df)\n",
    "\n",
    "        # Concatenate all dataframes into a single dataframe\n",
    "        if dataframes:\n",
    "            return pd.concat(dataframes, ignore_index=True)\n",
    "        else:\n",
    "            return pd.DataFrame()\n",
    "\n",
    "\n",
    "    # Example usage\n",
    "    folder_path = './rfbaseline/'+monitor_foldername+\"/\"\n",
    "    combined_df = concatenate_csvs_in_folder(folder_path)\n",
    "    #print(np.unique(combined_df[\"timestamp\"]))\n",
    "    combined_df = combined_df.drop(columns=['center_freq'])\n",
    "\n",
    "\n",
    "    # Assuming combined_df is your DataFrame\n",
    "\n",
    "\n",
    "    #df = combined_df.groupby('frequency')['power'].apply(avg_pow).reset_index()\n",
    "\n",
    "    # Define the start and end of the CBRS band (in MHz)\n",
    "    #band_start = 3470\n",
    "    #band_end = 3510\n",
    "\n",
    "    # Example usage\n",
    "    folder_path = './rfbaseline/'+monitor_foldername+\"/\"\n",
    "    combined_df = concatenate_csvs_in_folder(folder_path)\n",
    "    combined_df = combined_df.drop(columns=['center_freq'])\n",
    "\n",
    "    # Define the start and end of the CBRS band (in MHz)\n",
    "    chunk_size = 1 #np.round((band_end-band_start)/100)  # in MHz\n",
    "\n",
    "    # Initialize a list to store the aggregate results\n",
    "    aggregate_results = []\n",
    "\n",
    "    def linear_threshold(freq, start, end, threshold_start, threshold_end):\n",
    "        \"\"\"Calculate linearly varying threshold.\"\"\"\n",
    "        return threshold_start + ((threshold_end - threshold_start) * (freq - start) / (end - start))\n",
    "\n",
    "    # Iterate over the frequency band in chunks\n",
    "    for start in np.arange(band_start, band_end, chunk_size):\n",
    "        end = start + chunk_size\n",
    "        chunk_df = combined_df[(combined_df['frequency'] >= start) & (combined_df['frequency'] <= end)]\n",
    "\n",
    "        # Calculate mean and standard deviation in the linear domain\n",
    "        mean_power_db = np.mean(chunk_df['power'])\n",
    "        std_power_db = np.std(chunk_df['power'])\n",
    "\n",
    "        # Calculate the dynamic threshold for the chunk\n",
    "        chunk_threshold = linear_threshold((start+end)/2, band_start, band_end, threshold_start, threshold_end)\n",
    "        print(chunk_threshold)\n",
    "        # Calculate the duty cycle and average power when occupied\n",
    "        occupied_df = chunk_df[chunk_df['power'] > chunk_threshold]\n",
    "        duty_cycle = len(occupied_df) / len(chunk_df) * 100\n",
    "        avg_power_occupied = np.mean(occupied_df['power']) if not occupied_df.empty else np.nan\n",
    "        variance_power_occupied = np.var(occupied_df['power']) if not occupied_df.empty else np.nan\n",
    "\n",
    "        # Store the results for the current chunk\n",
    "        aggregate_results.append({\n",
    "            'Chunk_Start': start,\n",
    "            'Chunk_End': end,\n",
    "            'Mean': mean_power_db,\n",
    "            'Std': std_power_db,\n",
    "            'Threshold': chunk_threshold,\n",
    "            'Duty_Cycle': duty_cycle,\n",
    "            'Avg_Power_Occupied': avg_power_occupied,\n",
    "            'Variance_Power_Occupied': variance_power_occupied\n",
    "        })\n",
    "\n",
    "    # Rest of the code for aggregating results and plotting...\n",
    "\n",
    "    \n",
    "    # Convert the list of dicts to a DataFrame\n",
    "    aggregate_results_df = pd.DataFrame(aggregate_results)\n",
    "\n",
    "    # Aggregate the results\n",
    "    # You can choose how to aggregate, here's a simple mean aggregation\n",
    "    final_mean_threshold = aggregate_results_df['Threshold'].mean()\n",
    "    final_duty_cycle = aggregate_results_df['Duty_Cycle'].mean()\n",
    "    final_avg_power_occupied = aggregate_results_df['Avg_Power_Occupied'].mean()\n",
    "    final_variance_power_occupied = aggregate_results_df['Variance_Power_Occupied'].mean()\n",
    "\n",
    "    print(aggregate_results_df)\n",
    "    print(f\"Final Mean Threshold: {final_mean_threshold}\")\n",
    "    print(f\"Final Duty Cycle: {final_duty_cycle}\")\n",
    "    print(f\"Final Average Power Occupied: {final_avg_power_occupied}\")\n",
    "    print(f\"Final Variance of Power Occupied: {final_variance_power_occupied}\")\n",
    "\n",
    "\n",
    "    # Filter rows where frequency is between band start and ends\n",
    "    \n",
    "    ## 3350-3400\n",
    "    #filtered_df = combined_df[(combined_df['frequency'] >= band_start) & (combined_df['frequency'] <= band_end)& ((combined_df['frequency'] <= 3358.5) | (combined_df['frequency'] >= 3361)) & ((combined_df['frequency'] <= 3383) | (combined_df['frequency'] >= 3385.5))]\n",
    "    \n",
    "    ## 3470-3520\n",
    "    filtered_df = combined_df[(combined_df['frequency'] >= band_start) & (combined_df['frequency'] <= band_end)]\n",
    "\n",
    "\n",
    "    # Calculate the linearly varying threshold for each frequency\n",
    "    filtered_df['threshold'] = filtered_df['frequency'].apply(lambda freq: linear_threshold(freq, band_start, band_end, threshold_start, threshold_end))\n",
    "\n",
    "    plt.figure(figsize=(20, 15))\n",
    "    plt.scatter(filtered_df[\"frequency\"], filtered_df[\"power\"], marker=\"*\", s=0.3)\n",
    "    frequencies = np.linspace(band_start, band_end, 500)  # Generate frequency points\n",
    "    thresholds = [linear_threshold(freq, band_start, band_end, threshold_start, threshold_end) for freq in frequencies]\n",
    "    plt.plot(frequencies, thresholds, 'r--', label='Threshold')  # Plot threshold line\n",
    "\n",
    "    # Setting labels, title, and legend\n",
    "    plt.xlabel(\"Frequency (MHz)\")\n",
    "    plt.ylabel(\"Power (dB)\")\n",
    "    plt.title(\"Monitor @\"+monitor_foldername)\n",
    "    plt.legend()\n",
    "\n",
    "    plt.xlabel(\"Frequency (MHz)\")\n",
    "    plt.ylabel(\"Power (dB)\")\n",
    "    plt.title(\"Monitor @\"+monitor_foldername)\n",
    "    plt.show()\n",
    "\n",
    "    # Count the number of rows above their respective power threshold\n",
    "    occupied_count = filtered_df[filtered_df['power'] > filtered_df['threshold']].shape[0]\n",
    "\n",
    "    # Calculate the total number of rows in the filtered DataFrame\n",
    "    total_count = filtered_df.shape[0]\n",
    "\n",
    "    # Calculate the duty cycle\n",
    "    duty_cycle = (occupied_count / total_count) * 100\n",
    "\n",
    "    print(duty_cycle)\n",
    "    matplotlib.rcParams['font.family'] = 'Times New Roman'\n",
    "    fig, ax = plt.subplots(figsize=(8, 6))\n",
    "    plt.hist(filtered_df[\"power\"], bins=30, color='green')\n",
    "    plt.ylabel(\"Count\", fontsize=20)\n",
    "    plt.xlabel(\"Power [dBX]\", fontsize=20)\n",
    "    plt.title(str(band_start) + \"-\" + str(band_end) + \" MHz Power Histogram @ \"+monitor_foldername, fontsize=24)\n",
    "    #ax.set_xticklabels(fontsize=14, rotation=0)\n",
    "    ax.tick_params(axis='x', labelsize=16)\n",
    "    ax.tick_params(axis='y', labelsize=14)\n",
    "    #plt.axvline(x=threshold, color='red', linestyle='dashed')\n",
    "    #plt.text(threshold + 0.5, plt.ylim()[1] * 0.75, 'Threshold', color='red', fontsize=18)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1b395e3",
   "metadata": {},
   "source": [
    "### Functional Form"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ff27aab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import matplotlib\n",
    "\n",
    "\n",
    "def occupancy(monitor_foldername, threshold, band_start, band_end):\n",
    "    sns.set_theme()\n",
    "\n",
    "    def read_gzipped_csv(file_path):\n",
    "        with gzip.open(file_path, 'rt') as file:\n",
    "            df = pd.read_csv(file)\n",
    "            return df\n",
    "\n",
    "\n",
    "    def extract_timestamp_from_filename(filename):\n",
    "        # Extract the Unix timestamp using a regular expression\n",
    "        match = re.search(r'-(\\d+)\\.csv\\.gz$', filename)\n",
    "        if match:\n",
    "            return int(match.group(1))\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "\n",
    "    def concatenate_csvs_in_folder(folder_path):\n",
    "        dataframes = []\n",
    "        cutoff_date = pd.Timestamp('2023-01-01')  # Set the cutoff date to Jan 1, 2023\n",
    "\n",
    "        for filename in os.listdir(folder_path):\n",
    "            if filename.endswith('.gz'):\n",
    "                file_path = os.path.join(folder_path, filename)\n",
    "                timestamp = extract_timestamp_from_filename(filename)\n",
    "                df = read_gzipped_csv(file_path)\n",
    "                df['timestamp'] = pd.to_datetime(timestamp, unit='s')  # Convert timestamp to datetime\n",
    "\n",
    "                # Drop rows with NaN values\n",
    "                df = df.dropna()\n",
    "                # Filter out rows older than June 2022\n",
    "                df = df[df['timestamp'] >= cutoff_date]\n",
    "\n",
    "                dataframes.append(df)\n",
    "\n",
    "        # Concatenate all dataframes into a single dataframe\n",
    "        if dataframes:\n",
    "            return pd.concat(dataframes, ignore_index=True)\n",
    "        else:\n",
    "            return pd.DataFrame()\n",
    "\n",
    "\n",
    "    # Example usage\n",
    "    folder_path = './rfbaseline/'+monitor_foldername+\"/\"\n",
    "    combined_df = concatenate_csvs_in_folder(folder_path)\n",
    "    #print(np.unique(combined_df[\"timestamp\"]))\n",
    "    combined_df = combined_df.drop(columns=['center_freq'])\n",
    "\n",
    "\n",
    "    # Assuming combined_df is your DataFrame\n",
    "\n",
    "\n",
    "    #df = combined_df.groupby('frequency')['power'].apply(avg_pow).reset_index()\n",
    "\n",
    "    # Define the start and end of the CBRS band (in MHz)\n",
    "    #band_start = 3470\n",
    "    #band_end = 3510\n",
    "    chunk_size = band_end-band_start  # in MHz\n",
    "\n",
    "    # Initialize a list to store the aggregate results\n",
    "    aggregate_results = []\n",
    "\n",
    "    # Iterate over the frequency band in chunks of 25 MHz\n",
    "    for start in np.arange(band_start, band_end, chunk_size):\n",
    "        end = start + chunk_size\n",
    "        # Filter the DataFrame for the current chunk\n",
    "        ## 3350-3400\n",
    "        #chunk_df = combined_df[(combined_df['frequency'] >= start) & (combined_df['frequency'] < end)& ((combined_df['frequency'] <= 3358.5) | (combined_df['frequency'] >= 3361)) & ((combined_df['frequency'] <= 3383) | (combined_df['frequency'] >= 3385))]\n",
    "        \n",
    "        ## 3470-3520\n",
    "        chunk_df = combined_df[(combined_df['frequency'] >= start) & (combined_df['frequency'] <= end)]\n",
    "\n",
    "        # Calculate mean and standard deviation in the linear domain\n",
    "        mean_power_db = np.mean(chunk_df['power'])\n",
    "        std_power_db = np.std(chunk_df['power'])\n",
    "\n",
    "        # Set the threshold to mean + 1 standard deviation\n",
    "        #threshold = -145\n",
    "\n",
    "        # Calculate the duty cycle and average power when occupied\n",
    "        occupied_df = chunk_df[chunk_df['power'] > threshold]\n",
    "        duty_cycle = len(occupied_df) / len(chunk_df) * 100\n",
    "        avg_power_occupied = np.mean(occupied_df['power']) if not occupied_df.empty else np.nan\n",
    "        variance_power_occupied = np.var(occupied_df['power']) if not occupied_df.empty else np.nan\n",
    "\n",
    "        # Store the results for the current chunk\n",
    "        aggregate_results.append({\n",
    "            'Chunk_Start': start,\n",
    "            'Chunk_End': end,\n",
    "            'Mean': mean_power_db,\n",
    "            'Std': std_power_db,\n",
    "            'Threshold': threshold,\n",
    "            'Duty_Cycle': duty_cycle,\n",
    "            'Avg_Power_Occupied': avg_power_occupied,\n",
    "            'Variance_Power_Occupied': variance_power_occupied  # Added variance calculation\n",
    "        })\n",
    "\n",
    "    # Convert the list of dicts to a DataFrame\n",
    "    aggregate_results_df = pd.DataFrame(aggregate_results)\n",
    "\n",
    "    # Aggregate the results\n",
    "    # You can choose how to aggregate, here's a simple mean aggregation\n",
    "    final_mean_threshold = aggregate_results_df['Threshold'].mean()\n",
    "    final_duty_cycle = aggregate_results_df['Duty_Cycle'].mean()\n",
    "    final_avg_power_occupied = aggregate_results_df['Avg_Power_Occupied'].mean()\n",
    "    final_variance_power_occupied = aggregate_results_df['Variance_Power_Occupied'].mean()\n",
    "\n",
    "    print(aggregate_results_df)\n",
    "    print(f\"Final Mean Threshold: {final_mean_threshold}\")\n",
    "    print(f\"Final Duty Cycle: {final_duty_cycle}\")\n",
    "    print(f\"Final Average Power Occupied: {final_avg_power_occupied}\")\n",
    "    print(f\"Final Variance of Power Occupied: {final_variance_power_occupied}\")\n",
    "\n",
    "\n",
    "    # Filter rows where frequency is between band start and ends\n",
    "    \n",
    "    ## 3350-3400\n",
    "    #filtered_df = combined_df[(combined_df['frequency'] >= band_start) & (combined_df['frequency'] <= band_end)& ((combined_df['frequency'] <= 3358.5) | (combined_df['frequency'] >= 3361)) & ((combined_df['frequency'] <= 3383) | (combined_df['frequency'] >= 3385.5))]\n",
    "    \n",
    "    ## 3470-3520\n",
    "    filtered_df = combined_df[(combined_df['frequency'] >= band_start) & (combined_df['frequency'] <= band_end)]\n",
    "\n",
    "    plt.figure(figsize=(20, 15))\n",
    "    plt.scatter(filtered_df[\"frequency\"], filtered_df[\"power\"], marker=\"*\", s=0.3)\n",
    "    plt.xlabel(\"Frequency (MHz)\")\n",
    "    plt.ylabel(\"Power (dB)\")\n",
    "    plt.title(\"Monitor @\"+monitor_foldername)\n",
    "    plt.show()\n",
    "    # Define a power threshold, for example, -90 dB\n",
    "    power_threshold = threshold\n",
    "\n",
    "    # Count the number of rows above the power threshold\n",
    "    occupied_count = filtered_df[filtered_df['power'] > power_threshold].shape[0]\n",
    "\n",
    "    # Calculate the total number of rows in the filtered DataFrame\n",
    "    total_count = filtered_df.shape[0]\n",
    "\n",
    "    # Calculate the duty cycle\n",
    "    duty_cycle = (occupied_count / total_count) * 100\n",
    "\n",
    "    print(duty_cycle)\n",
    "    matplotlib.rcParams['font.family'] = 'Times New Roman'\n",
    "    fig, ax = plt.subplots(figsize=(8, 6))\n",
    "    plt.hist(filtered_df[\"power\"], bins=30, color='green')\n",
    "    plt.ylabel(\"Count\", fontsize=20)\n",
    "    plt.xlabel(\"Power [dBX]\", fontsize=20)\n",
    "    plt.title(str(band_start) + \"-\" + str(band_end) + \" MHz Power Histogram @ \"+monitor_foldername, fontsize=24)\n",
    "    #ax.set_xticklabels(fontsize=14, rotation=0)\n",
    "    ax.tick_params(axis='x', labelsize=16)\n",
    "    ax.tick_params(axis='y', labelsize=14)\n",
    "    plt.axvline(x=threshold, color='red', linestyle='dashed')\n",
    "    plt.text(threshold + 0.5, plt.ylim()[1] * 0.75, 'Threshold', color='red', fontsize=18)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0767d4c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "occupancy(\"Bookstore\", -105, 3470, 3510)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f1e931f",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "occupancy(\"EBC\", -105, 3470, 3510)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e65eb6e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "occupancy(\"Guesthouse\", -105, 3470, 3510)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f3939d0",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "occupancy(\"Moran\", -105, 3470, 3510)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e919de7",
   "metadata": {},
   "outputs": [],
   "source": [
    "occupancy(\"WEB\", -105, 3470, 3510)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22300dd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "occupancy(\"Sagepoint\", -105, 3470, 3510)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5564e4f0",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "occupancy(\"Law73\", -105, 3470, 3510)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeac7cfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "occupancy(\"Humanities\", -105, 3470, 3510)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f454f341",
   "metadata": {},
   "outputs": [],
   "source": [
    "occupancy(\"Madsen\", -105, 3470, 3510)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07195039",
   "metadata": {},
   "outputs": [],
   "source": [
    "occupancy(\"Garage\", -105, 3470, 3510)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb6bb688",
   "metadata": {},
   "outputs": [],
   "source": [
    "occupancy(\"Bookstore\", -105, 3610, 3650)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc1f4b9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "occupancy(\"EBC\", -105, 3610, 3650)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fc84217",
   "metadata": {},
   "outputs": [],
   "source": [
    "occupancy(\"Guesthouse\", -105, 3610, 3650)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f257622",
   "metadata": {},
   "outputs": [],
   "source": [
    "occupancy(\"Moran\", -105, 3610, 3650)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91e5ed8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "occupancy(\"WEB\", -105, 3610, 3650)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ebfaaa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "occupancy(\"Sagepoint\", -105, 3610, 3650)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05424a8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "occupancy(\"Law73\", -105, 3610, 3650)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e25eee30",
   "metadata": {},
   "outputs": [],
   "source": [
    "occupancy(\"Humanities\", -105, 3610, 3650)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3050e63",
   "metadata": {},
   "outputs": [],
   "source": [
    "occupancy(\"Madsen\", -105, 3610, 3650)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eddc33d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "occupancy(\"Garage\", -105, 3610, 3650)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff8d0a4e",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "occupancy(\"Bookstore\", -100, -100, 2160.5, 2169.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eefb00b",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "occupancy(\"EBC\",  -100, -100, 2160.5, 2169.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e703df7",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "occupancy(\"Guesthouse\",  -100, -100, 2160.5, 2169.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccfd693e",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "occupancy(\"Moran\",  -100, -100, 2160.5, 2169.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41e5f8a7",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "occupancy(\"WEB\",  -100, -100, 2160.5, 2169.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15c55d8c",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "occupancy(\"Sagepoint\",  -100, -100, 2160.5, 2169.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12fb002b",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "occupancy(\"Law73\", -100, -100, 2160.5, 2169.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4641e43c",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "occupancy(\"Humanities\",  -100, -100, 2160.5, 2169.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "388c5ae0",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "occupancy(\"Madsen\",  -100, -100, 2160.5, 2169.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da7b1b26",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "occupancy(\"Garage\", -100, -100, 2160.5, 2169.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b14717c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccc86635",
   "metadata": {},
   "outputs": [],
   "source": [
    "occupancy(\"Bookstore\", -100, -90, 2504, 2544)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "458f3e30",
   "metadata": {},
   "outputs": [],
   "source": [
    "occupancy(\"EBC\", -100, -90, 2504, 2544)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "045abc11",
   "metadata": {},
   "outputs": [],
   "source": [
    "occupancy(\"Guesthouse\", -100, -90, 2504, 2544)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74f57bd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "occupancy(\"Moran\", -100, -90, 2504, 2544)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d4e8012",
   "metadata": {},
   "outputs": [],
   "source": [
    "occupancy(\"WEB\", -100, -90, 2504, 2544)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2b1d0ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "occupancy(\"Sagepoint\", -100, -90, 2504, 2544)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23340cc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "occupancy(\"Law73\", -100, -90, 2504, 2544)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02c3ba5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "occupancy(\"Humanities\", -100, -90, 2504, 2544)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05753533",
   "metadata": {},
   "outputs": [],
   "source": [
    "occupancy(\"Madsen\", -100, -90, 2504, 2544)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a4a455e",
   "metadata": {},
   "outputs": [],
   "source": [
    "occupancy(\"Garage\", -100, -90, 2504, 2544)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94520907",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "occupancy(\"Bookstore\", -100, -100, 1955, 1970)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a90c8b99",
   "metadata": {},
   "outputs": [],
   "source": [
    "occupancy(\"EBC\", -100, -100, 1955, 1970)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed43a7ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "occupancy(\"Guesthouse\", -100, -100, 1955, 1970)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2259b3b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "occupancy(\"Moran\", -100, -100, 1955, 1970)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76db4f6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "occupancy(\"WEB\", -100, -100, 1955, 1970)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "746d9f0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "occupancy(\"Sagepoint\", -100, -100, 1955, 1970)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb81fd94",
   "metadata": {},
   "outputs": [],
   "source": [
    "occupancy(\"Law73\", -100, -100, 1955, 1970)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f9ba241",
   "metadata": {},
   "outputs": [],
   "source": [
    "occupancy(\"Humanities\", -100, -100, 1955, 1970)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8864e274",
   "metadata": {},
   "outputs": [],
   "source": [
    "occupancy(\"Madsen\", -100, -100, 1955, 1970)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecf267f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "occupancy(\"Garage\", -100, -100, 1955, 1970)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e21d7fb0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44ba5a92",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc2957ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "occupancy_dict = {\"Bookstore\": [45.00, -89.90], \"EBC\": [89.79, -94.60], \"Guesthouse\":[76.90, -99.32], \"Moran\":[21.23, -101.14], \"WEB\": [42.88, -91.95], \"Sagepoint\": [56.00, -98.58], \"Law73\":[44.92, -86.17], \"Humanities\": [71.66, -97.28], \"Madsen\":[44.29, -100.86], \"Garage\": [99.00, -87.82]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89ddd319",
   "metadata": {},
   "outputs": [],
   "source": [
    "occupancy(\"Madsen\", -106, 3470, 3510)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c49bbfa3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "occupancy(\"Garage\", -106, 3470, 3510)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e9970f9",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import gzip\n",
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "def occupancy2(monitor_foldername):\n",
    "    sns.set_theme()\n",
    "\n",
    "    def read_gzipped_csv(file_path):\n",
    "        with gzip.open(file_path, 'rt') as file:\n",
    "            df = pd.read_csv(file)\n",
    "            return df\n",
    "\n",
    "\n",
    "    def extract_timestamp_from_filename(filename):\n",
    "        # Extract the Unix timestamp using a regular expression\n",
    "        match = re.search(r'-(\\d+)\\.csv\\.gz$', filename)\n",
    "        if match:\n",
    "            return int(match.group(1))\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "\n",
    "\n",
    "    def concatenate_csvs_in_folder(folder_path):\n",
    "        dataframes_dict = {}\n",
    "\n",
    "        for filename in os.listdir(folder_path):\n",
    "            if filename.endswith('.gz'):\n",
    "                # Extract the base name before \"_\"\n",
    "                base_name = filename.split('_')[0]\n",
    "                file_path = os.path.join(folder_path, filename)\n",
    "                timestamp = extract_timestamp_from_filename(filename)\n",
    "                df = read_gzipped_csv(file_path)\n",
    "                df['timestamp'] = timestamp\n",
    "\n",
    "                # Append the dataframe to the list in the dictionary for the base name\n",
    "                if base_name not in dataframes_dict:\n",
    "                    dataframes_dict[base_name] = []\n",
    "                dataframes_dict[base_name].append(df)\n",
    "\n",
    "        # Concatenate dataframes in the dictionary and store them in a new dictionary\n",
    "        concatenated_dfs = {}\n",
    "        for base_name, dfs in dataframes_dict.items():\n",
    "            if dfs:\n",
    "                concatenated_dfs[base_name] = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "        return concatenated_dfs\n",
    "\n",
    "\n",
    "    # Example usage\n",
    "    folder_path = './rfbaseline/'+monitor_foldername+\"/\"\n",
    "    combined_df = concatenate_csvs_in_folder(folder_path)\n",
    "    print(combined_df)\n",
    "    \n",
    "    for key in combined_df.keys():\n",
    "        occupancy3(combined_df[key], key)\n",
    "    \n",
    "\n",
    "occupancy2(\"Emulab\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3f18b76",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import gzip\n",
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "def occupancy2(monitor_foldername):\n",
    "    sns.set_theme()\n",
    "\n",
    "    def read_gzipped_csv(file_path, band_start, band_end):\n",
    "        with gzip.open(file_path, 'rt') as file:\n",
    "            df = pd.read_csv(file)\n",
    "            # Filter out frequencies outside the band range\n",
    "            return df[(df['frequency'] >= band_start) & (df['frequency'] <= band_end)]\n",
    "\n",
    "    def extract_timestamp_from_filename(filename):\n",
    "        match = re.search(r'-(\\d+)\\.csv\\.gz$', filename)\n",
    "        if match:\n",
    "            return int(match.group(1))\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "    def concatenate_csvs_in_folder(folder_path, band_start, band_end):\n",
    "        dataframes_dict = {}\n",
    "\n",
    "        for filename in os.listdir(folder_path):\n",
    "            if filename.endswith('.gz'):\n",
    "                base_name = filename.split('_')[0]\n",
    "                file_path = os.path.join(folder_path, filename)\n",
    "                timestamp = extract_timestamp_from_filename(filename)\n",
    "                df = read_gzipped_csv(file_path, band_start, band_end)\n",
    "                df['timestamp'] = timestamp\n",
    "\n",
    "                if base_name not in dataframes_dict:\n",
    "                    dataframes_dict[base_name] = []\n",
    "                dataframes_dict[base_name].append(df)\n",
    "\n",
    "        concatenated_dfs = {}\n",
    "        for base_name, dfs in dataframes_dict.items():\n",
    "            if dfs:\n",
    "                concatenated_dfs[base_name] = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "        return concatenated_dfs\n",
    "\n",
    "    # Define the start and end of the band (in MHz)\n",
    "    band_start = 3550\n",
    "    band_end = 3600\n",
    "\n",
    "    folder_path = './rfbaseline/' + monitor_foldername + \"/\"\n",
    "    combined_df = concatenate_csvs_in_folder(folder_path, band_start, band_end)\n",
    "    for key in combined_df.keys():\n",
    "        print(len(combined_df[key]))\n",
    "        occupancy3(combined_df[key], key)\n",
    "\n",
    "def occupancy3(df_, node_name):\n",
    "    def avg_pow(powers):\n",
    "        pow_mean = np.mean(powers)\n",
    "        return pow_mean\n",
    "\n",
    "    df_ = df_.drop(columns=['center_freq'])\n",
    "\n",
    "    df = df_.groupby('frequency')['power'].apply(avg_pow).reset_index()\n",
    "\n",
    "    # Define the start and end of the CBRS band (in MHz)\n",
    "    band_start = 3550\n",
    "    band_end = 3600\n",
    "    chunk_size = 50  # in MHz\n",
    "\n",
    "    aggregate_results = []\n",
    "\n",
    "    for start in range(band_start, band_end, chunk_size):\n",
    "        end = start + chunk_size\n",
    "        chunk_df = df[(df['frequency'] >= start) & (df['frequency'] < end)]\n",
    "\n",
    "        mean_power_db = np.mean(chunk_df['power'])\n",
    "        std_power_db = np.std(chunk_df['power'])\n",
    "\n",
    "        threshold = -138\n",
    "\n",
    "        occupied_df = chunk_df[chunk_df['power'] > threshold]\n",
    "        duty_cycle = len(occupied_df) / len(chunk_df) * 100\n",
    "        avg_power_occupied = avg_pow(occupied_df['power']) if not occupied_df.empty else np.nan\n",
    "\n",
    "        aggregate_results.append({\n",
    "            'Chunk_Start': start,\n",
    "            'Chunk_End': end,\n",
    "            'Mean': mean_power_db,\n",
    "            'Std': std_power_db,\n",
    "            'Threshold': threshold,\n",
    "            'Duty_Cycle': duty_cycle,\n",
    "            'Avg_Power_Occupied': avg_power_occupied\n",
    "        })\n",
    "\n",
    "    aggregate_results_df = pd.DataFrame(aggregate_results)\n",
    "\n",
    "    final_mean_threshold = aggregate_results_df['Threshold'].mean()\n",
    "    final_duty_cycle = aggregate_results_df['Duty_Cycle'].mean()\n",
    "    final_avg_power_occupied = aggregate_results_df['Avg_Power_Occupied'].mean()\n",
    "\n",
    "    print(aggregate_results_df)\n",
    "    print(f\"Final Mean Threshold: {final_mean_threshold}\")\n",
    "    print(f\"Final Duty Cycle: {final_duty_cycle}\")\n",
    "    print(f\"Final Average Power Occupied: {final_avg_power_occupied}\")\n",
    "\n",
    "    filtered_df = df[(df['frequency'] >= band_start) & (df['frequency'] <= band_end)]\n",
    "\n",
    "    plt.figure(figsize=(20, 15))\n",
    "    plt.scatter(filtered_df[\"frequency\"], filtered_df[\"power\"], marker=\"*\", s=0.3)\n",
    "    plt.xlabel(\"Frequency (MHz)\")\n",
    "    plt.ylabel(\"Power (dB)\")\n",
    "    plt.title(\"Monitor @\" + node_name)\n",
    "    plt.show()\n",
    "    # Define a power threshold, for example, -90 dB\n",
    "    power_threshold = threshold\n",
    "\n",
    "    # Count the number of rows above the power threshold\n",
    "    occupied_count = filtered_df[filtered_df['power'] > power_threshold].shape[0]\n",
    "\n",
    "    # Calculate the total number of rows in the filtered DataFrame\n",
    "    total_count = filtered_df.shape[0]\n",
    "\n",
    "    # Calculate the duty cycle\n",
    "    duty_cycle = (occupied_count / total_count) * 100\n",
    "\n",
    "    print(duty_cycle)\n",
    "\n",
    "    plt.figure()\n",
    "    plt.hist(filtered_df[\"power\"], bins=20)\n",
    "    plt.show()\n",
    "    \n",
    "occupancy2(\"Emulab\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "911461f4",
   "metadata": {},
   "source": [
    "### Scrape the Emulab Website"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db3faf61",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "url = 'https://ops.emulab.net/rfbaseline/Emulab/'\n",
    "\n",
    "response = requests.get(url)\n",
    "\n",
    "if response.status_code == 200:\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    \n",
    "    links = soup.find_all('a', href=True)\n",
    "    \n",
    "    file_urls = [link['href'] for link in links if link['href'].endswith('.gz')]\n",
    "    \n",
    "    for file_url in file_urls:\n",
    "        if (file_url.startswith(\"./cbrssdr1-ustar\") and int(file_url.split(\"-\")[-1].split(\".\")[0])>1611516022) :\n",
    "            download_response = requests.get(url+file_url[2:])\n",
    "            if download_response.status_code == 200:\n",
    "                filename = file_url.split('/')[-1]\n",
    "                filename = filename.split(\":\")[0]+\"_\"+filename.split(\":\")[1]\n",
    "                filepath = f'./emulab_data/{filename}'\n",
    "                with open(filepath, 'wb') as file:\n",
    "                    file.write(download_response.content)\n",
    "            else:\n",
    "                print(f'Failed to download {file_url}')\n",
    "        else:\n",
    "            continue\n",
    "else:\n",
    "    print('Failed to retrieve the webpage')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45f7af28",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
