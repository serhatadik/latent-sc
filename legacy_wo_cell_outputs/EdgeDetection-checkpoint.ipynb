{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "038fe69d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def dB_to_lin(pow_dB):\n",
    "    return 10**(np.array(pow_dB)/10)\n",
    "\n",
    "def lin_to_dB(pow_lin):\n",
    "    return 10*np.log10(np.array(pow_lin))\n",
    "\n",
    "rssi_3470_3510 = np.array([-85.53,\n",
    "-96.04,\n",
    "-96.62,\n",
    "-102.69,\n",
    "-95.77,\n",
    "-100.64,\n",
    "-94.44,\n",
    "-101.19,\n",
    "-97.66,\n",
    "-88.8\n",
    "])\n",
    "\n",
    "\n",
    "rssi_3610_3650 = np.array([-84.93,\n",
    "-98.94,\n",
    "-102.76,\n",
    "-103.86,\n",
    "-94.13,\n",
    "-103.55,\n",
    "-86.8,\n",
    "-99.54,\n",
    "-103.67,\n",
    "-89.61\n",
    "])\n",
    "\n",
    "\n",
    "rssi_2504_2544 = np.array([-92.7,\n",
    "-77.71,\n",
    "-77.51,\n",
    "-90.83,\n",
    "-90.13,\n",
    "-78.96,\n",
    "-84.21,\n",
    "-81.1,\n",
    "-85.72,\n",
    "-77.35\n",
    "])\n",
    "\n",
    "\n",
    "rssi_5190_5210 = np.array([-106.64,\n",
    "-105.2,\n",
    "-106.97,\n",
    "-101.91,\n",
    "-105.03,\n",
    "-103.33,\n",
    "-101.26,\n",
    "-105.88,\n",
    "-107.03,\n",
    "-105.28\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcdf8b98",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import scipy.io as sio\n",
    "import matplotlib.pyplot as plt\n",
    "map_folderdir = \"./\"\n",
    "directory = os.listdir(map_folderdir)\n",
    "flag = 0\n",
    "for fname in directory:\n",
    "    if \"SLCmap\" in fname:\n",
    "        map_file = os.path.join(map_folderdir, fname)\n",
    "        flag = 1\n",
    "\n",
    "if flag == 0:\n",
    "    errorMessage = 'Error: The file does not exist in the folder:\\n ' + map_folderdir\n",
    "    warnings.warn(errorMessage)\n",
    "\n",
    "print('Now reading ' + map_file + \"\\n\")\n",
    "x = sio.loadmat(map_file)\n",
    "map_struct = x['SLC']\n",
    "\n",
    "# Define a new struct named SLC\n",
    "SLC = map_struct[0][0]\n",
    "column_map = dict(zip([name for name in SLC.dtype.names], [i for i in range(len(SLC.dtype.names))]))\n",
    "\n",
    "map_ = SLC[column_map[\"dem\"]] + 0.3048 * SLC[column_map[\"hybrid_bldg\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd6acb66",
   "metadata": {},
   "source": [
    "### Implement Pseudo-Inverse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d32cd23b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_points = np.array([\n",
    "    [966, 2992],\n",
    "    [2569, 3767],\n",
    "    [2873, 3447],\n",
    "    [2621, 4286],\n",
    "    [1312, 3830],\n",
    "    [3828, 2667],\n",
    "    [242, 2442],\n",
    "    [1711, 3145],\n",
    "    [2852, 1584],\n",
    "    [1903, 2393]\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca22dc67",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.gaussian_process.kernels import DotProduct, RationalQuadratic, WhiteKernel, ConstantKernel, RBF, Product, Sum\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from itertools import combinations\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Coordinates of the data collection points\n",
    "data_points = np.array([\n",
    "    [966, 2992],\n",
    "    [2569, 3767],\n",
    "    [2873, 3447],\n",
    "    [2621, 4286],\n",
    "    [1312, 3830],\n",
    "    [3828, 2667],\n",
    "    [242, 2442],\n",
    "    [1711, 3145],\n",
    "    [2852, 1584],\n",
    "    [1903, 2393]\n",
    "])\n",
    "\n",
    "# Sample signal strengths at these data points\n",
    "signal_strengths = rssi_3470_3510\n",
    "\n",
    "# Example parameter ranges for each kernel\n",
    "parameter_ranges = {\n",
    "    RBF: {'length_scale': [10, 100.0, 1000.0, 5000]},\n",
    "    ConstantKernel: {'constant_value': [1.0, 10.0, 100, 1000]},\n",
    "    WhiteKernel: {'noise_level': [1e-1, 1e0,1e1, 1e2]},\n",
    "    DotProduct: {'sigma_0': [1e-1, 10, 100, 1000, 5000]},\n",
    "    RationalQuadratic: {'length_scale': [1.0, 10.0, 100], 'alpha': [0.1, 1.0, 10]}\n",
    "}\n",
    "# Function to generate kernels with different parameter settings\n",
    "def generate_kernels(kernel_class, param_ranges):\n",
    "    if kernel_class in param_ranges:\n",
    "        params = param_ranges[kernel_class]\n",
    "        for param_combination in product(*(params[param] for param in params)):\n",
    "            kwargs = dict(zip(params, param_combination))\n",
    "            yield kernel_class(**kwargs)\n",
    "    else:\n",
    "        yield kernel_class()\n",
    "\n",
    "# Generate all kernels with their parameter variations\n",
    "all_kernels_with_params = []\n",
    "for kernel_class in [RBF, ConstantKernel, WhiteKernel, DotProduct, RationalQuadratic]:\n",
    "    all_kernels_with_params.extend(generate_kernels(kernel_class, parameter_ranges))\n",
    "\n",
    "def create_multiplicative_kernel(kernel_tuple):\n",
    "    # Start with the first kernel in the tuple\n",
    "    combined_kernel = kernel_tuple[0]\n",
    "    # Iterate over the remaining kernels and combine them\n",
    "    for kernel in kernel_tuple[1:]:\n",
    "        combined_kernel = Product(combined_kernel, kernel)\n",
    "    return combined_kernel\n",
    "\n",
    "# Function to create an additive kernel from a combination\n",
    "def create_additive_kernel(kernel_tuple):\n",
    "    # Start with the first kernel in the tuple\n",
    "    combined_kernel = kernel_tuple[0]\n",
    "    # Iterate over the remaining kernels and combine them\n",
    "    for kernel in kernel_tuple[1:]:\n",
    "        combined_kernel = Sum(combined_kernel, kernel)\n",
    "    return combined_kernel\n",
    "\n",
    "\n",
    "# Generate all possible combinations of single kernels, multiplicative and additive combinations of two kernels\n",
    "kernel_combinations_with_params = all_kernels_with_params.copy()\n",
    "\n",
    "for r in range(2, 4):  # r in range(2, 4) to generate combinations of two and three kernels\n",
    "    for combo in combinations(all_kernels_with_params, r):\n",
    "        multiplicative_kernel = create_multiplicative_kernel(combo)\n",
    "        additive_kernel = create_additive_kernel(combo)\n",
    "        kernel_combinations_with_params.extend([multiplicative_kernel, additive_kernel])\n",
    "\n",
    "loocv_scores = []\n",
    "\n",
    "for kernel in tqdm(kernel_combinations_with_params):\n",
    "    scores = []\n",
    "    for i in range(len(data_points)):\n",
    "        train_points = np.delete(data_points, i, axis=0)\n",
    "        train_strengths = np.delete(signal_strengths, i, axis=0)\n",
    "        test_point = data_points[i, :].reshape(1, -1)\n",
    "        test_strength = signal_strengths[i]\n",
    "\n",
    "        gp = GaussianProcessRegressor(kernel=kernel, n_restarts_optimizer=15, alpha=1e-10)\n",
    "        try:\n",
    "            gp.fit(train_points, train_strengths)\n",
    "            predicted_strength, _ = gp.predict(test_point, return_std=True)\n",
    "            mse = mean_squared_error([test_strength], predicted_strength)\n",
    "            scores.append(mse)\n",
    "        except np.linalg.LinAlgError:\n",
    "            scores = []\n",
    "            break\n",
    "\n",
    "    if scores:\n",
    "        avg_score = np.mean(scores)\n",
    "        loocv_scores.append((kernel, avg_score))\n",
    "\n",
    "# Find the kernel with the best average score\n",
    "sorted_loocv_scores = sorted(loocv_scores, key=lambda x: x[1])\n",
    "\n",
    "# Define the number of top kernels to print\n",
    "k = 10  \n",
    "\n",
    "# Print the top k kernels and their scores\n",
    "print(f\"Top {k} Kernels and Their LOOCV Scores:\")\n",
    "for kernel, score in sorted_loocv_scores[:k]:\n",
    "    print(kernel)\n",
    "    print(\"LOOCV Score:\", score)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41d9801f",
   "metadata": {},
   "outputs": [],
   "source": [
    "loocv_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a06e42cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "signal_strengths = rssi_3470_3510\n",
    "summ = 0\n",
    "for i in range(len(data_points)):\n",
    "    train_points = np.delete(data_points, i, axis=0)\n",
    "    train_strengths = np.delete(signal_strengths, i, axis=0)\n",
    "    test_point = data_points[i, :].reshape(1, -1)\n",
    "    test_strength = signal_strengths[i]\n",
    "    summ += (np.mean(train_strengths) - test_strength)**2\n",
    "    \n",
    "summ/len(data_points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63f9caa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "signal_strengths = rssi_3610_3650\n",
    "summ = 0\n",
    "for i in range(len(data_points)):\n",
    "    train_points = np.delete(data_points, i, axis=0)\n",
    "    train_strengths = np.delete(signal_strengths, i, axis=0)\n",
    "    test_point = data_points[i, :].reshape(1, -1)\n",
    "    test_strength = signal_strengths[i]\n",
    "    summ += (np.mean(train_strengths) - test_strength)**2\n",
    "    \n",
    "summ/len(data_points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8240acd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "signal_strengths = rssi_2504_2544\n",
    "summ = 0\n",
    "for i in range(len(data_points)):\n",
    "    train_points = np.delete(data_points, i, axis=0)\n",
    "    train_strengths = np.delete(signal_strengths, i, axis=0)\n",
    "    test_point = data_points[i, :].reshape(1, -1)\n",
    "    test_strength = signal_strengths[i]\n",
    "    summ += (np.mean(train_strengths) - test_strength)**2\n",
    "    \n",
    "summ/len(data_points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd711a3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "signal_strengths = rssi_5190_5210\n",
    "summ = 0\n",
    "for i in range(len(data_points)):\n",
    "    train_points = np.delete(data_points, i, axis=0)\n",
    "    train_strengths = np.delete(signal_strengths, i, axis=0)\n",
    "    test_point = data_points[i, :].reshape(1, -1)\n",
    "    test_strength = signal_strengths[i]\n",
    "    summ += (np.mean(train_strengths) - test_strength)**2\n",
    "    \n",
    "summ/len(data_points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1edef7f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.gaussian_process.kernels import DotProduct, RationalQuadratic, WhiteKernel, ConstantKernel, RBF, Product, Sum\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from itertools import combinations, product\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "# Coordinates of the data collection points\n",
    "data_points = np.array([\n",
    "    [966, 2992],\n",
    "    [2569, 3767],\n",
    "    [2873, 3447],\n",
    "    [2621, 4286],\n",
    "    [1312, 3830],\n",
    "    [3828, 2667],\n",
    "    [242, 2442],\n",
    "    [1711, 3145],\n",
    "    [2852, 1584],\n",
    "    [1903, 2393]\n",
    "])\n",
    "\n",
    "# Sample signal strengths at these data points\n",
    "signal_strengths = rssi_3610_3650\n",
    "\n",
    "# Example parameter ranges for each kernel\n",
    "parameter_ranges = {\n",
    "    RBF: {'length_scale': [10, 100.0, 1000.0, 5000]},\n",
    "    ConstantKernel: {'constant_value': [1.0, 10.0, 100, 1000]},\n",
    "    WhiteKernel: {'noise_level': [1e-1, 1e0,1e1, 1e2]},\n",
    "    DotProduct: {'sigma_0': [1e-1, 10, 100, 1000, 5000]},\n",
    "    RationalQuadratic: {'length_scale': [1.0, 10.0, 100], 'alpha': [0.1, 1.0, 10]}\n",
    "}\n",
    "# Function to generate kernels with different parameter settings\n",
    "def generate_kernels(kernel_class, param_ranges):\n",
    "    if kernel_class in param_ranges:\n",
    "        params = param_ranges[kernel_class]\n",
    "        for param_combination in product(*(params[param] for param in params)):\n",
    "            kwargs = dict(zip(params, param_combination))\n",
    "            yield kernel_class(**kwargs)\n",
    "    else:\n",
    "        yield kernel_class()\n",
    "\n",
    "# Generate all kernels with their parameter variations\n",
    "all_kernels_with_params = []\n",
    "for kernel_class in [RBF, ConstantKernel, WhiteKernel, DotProduct, RationalQuadratic]:\n",
    "    all_kernels_with_params.extend(generate_kernels(kernel_class, parameter_ranges))\n",
    "\n",
    "def create_multiplicative_kernel(kernel_tuple):\n",
    "    # Start with the first kernel in the tuple\n",
    "    combined_kernel = kernel_tuple[0]\n",
    "    # Iterate over the remaining kernels and combine them\n",
    "    for kernel in kernel_tuple[1:]:\n",
    "        combined_kernel = Product(combined_kernel, kernel)\n",
    "    return combined_kernel\n",
    "\n",
    "# Function to create an additive kernel from a combination\n",
    "def create_additive_kernel(kernel_tuple):\n",
    "    # Start with the first kernel in the tuple\n",
    "    combined_kernel = kernel_tuple[0]\n",
    "    # Iterate over the remaining kernels and combine them\n",
    "    for kernel in kernel_tuple[1:]:\n",
    "        combined_kernel = Sum(combined_kernel, kernel)\n",
    "    return combined_kernel\n",
    "\n",
    "\n",
    "# Generate all possible combinations of single kernels, multiplicative and additive combinations of two kernels\n",
    "kernel_combinations_with_params = all_kernels_with_params.copy()\n",
    "\n",
    "for r in range(2, 4):  # r in range(2, 4) to generate combinations of two and three kernels\n",
    "    for combo in combinations(all_kernels_with_params, r):\n",
    "        multiplicative_kernel = create_multiplicative_kernel(combo)\n",
    "        additive_kernel = create_additive_kernel(combo)\n",
    "        kernel_combinations_with_params.extend([multiplicative_kernel, additive_kernel])\n",
    "\n",
    "\n",
    "it=0\n",
    "loocv_scores = []\n",
    "\n",
    "for kernel in tqdm(kernel_combinations_with_params):\n",
    "    it+=1\n",
    "    scores = []\n",
    "    for i in range(len(data_points)):\n",
    "        train_points = np.delete(data_points, i, axis=0)\n",
    "        train_strengths = np.delete(signal_strengths, i, axis=0)\n",
    "        test_point = data_points[i, :].reshape(1, -1)\n",
    "        test_strength = signal_strengths[i]\n",
    "\n",
    "        gp = GaussianProcessRegressor(kernel=kernel, n_restarts_optimizer=15, alpha=1e-10)\n",
    "        try:\n",
    "            gp.fit(train_points, train_strengths)\n",
    "            predicted_strength, _ = gp.predict(test_point, return_std=True)\n",
    "            mse = mean_squared_error([test_strength], predicted_strength)\n",
    "            scores.append(mse)\n",
    "        except np.linalg.LinAlgError:\n",
    "            scores = []\n",
    "            break\n",
    "\n",
    "    if scores:\n",
    "        avg_score = np.mean(scores)\n",
    "        loocv_scores.append((kernel, avg_score))\n",
    "        if it%800 == 0:\n",
    "            k = 10  \n",
    "            sorted_loocv_scores = sorted(loocv_scores, key=lambda x: x[1])\n",
    "            for kernel, score in sorted_loocv_scores[:k]:\n",
    "                print(kernel)\n",
    "                print(\"LOOCV Score:\", score)\n",
    "                print()\n",
    "\n",
    "# Find the kernel with the best average score\n",
    "sorted_loocv_scores = sorted(loocv_scores, key=lambda x: x[1])\n",
    "\n",
    "# Define the number of top kernels to print\n",
    "k = 10  \n",
    "\n",
    "# Print the top k kernels and their scores\n",
    "print(f\"Top {k} Kernels and Their LOOCV Scores:\")\n",
    "for kernel, score in sorted_loocv_scores[:k]:\n",
    "    print(kernel)\n",
    "    print(\"LOOCV Score:\", score)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18c9d3a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "loocv_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cfaa0b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.gaussian_process.kernels import DotProduct, RationalQuadratic, WhiteKernel, ConstantKernel, RBF, Product, Sum\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from itertools import combinations\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Coordinates of the data collection points\n",
    "data_points = np.array([\n",
    "    [966, 2992],\n",
    "    [2569, 3767],\n",
    "    [2873, 3447],\n",
    "    [2621, 4286],\n",
    "    [1312, 3830],\n",
    "    [3828, 2667],\n",
    "    [242, 2442],\n",
    "    [1711, 3145],\n",
    "    [2852, 1584],\n",
    "    [1903, 2393]\n",
    "])\n",
    "\n",
    "# Sample signal strengths at these data points\n",
    "signal_strengths = rssi_2504_2544\n",
    "\n",
    "# Example parameter ranges for each kernel\n",
    "parameter_ranges = {\n",
    "    RBF: {'length_scale': [10, 100.0, 1000.0, 5000]},\n",
    "    ConstantKernel: {'constant_value': [1.0, 10.0, 100, 1000]},\n",
    "    WhiteKernel: {'noise_level': [1e-1, 1e0,1e1, 1e2]},\n",
    "    DotProduct: {'sigma_0': [1e-1, 10, 100, 1000, 5000]},\n",
    "    RationalQuadratic: {'length_scale': [1.0, 10.0, 100], 'alpha': [0.1, 1.0, 10]}\n",
    "}\n",
    "# Function to generate kernels with different parameter settings\n",
    "def generate_kernels(kernel_class, param_ranges):\n",
    "    if kernel_class in param_ranges:\n",
    "        params = param_ranges[kernel_class]\n",
    "        for param_combination in product(*(params[param] for param in params)):\n",
    "            kwargs = dict(zip(params, param_combination))\n",
    "            yield kernel_class(**kwargs)\n",
    "    else:\n",
    "        yield kernel_class()\n",
    "\n",
    "# Generate all kernels with their parameter variations\n",
    "all_kernels_with_params = []\n",
    "for kernel_class in [RBF, ConstantKernel, WhiteKernel, DotProduct, RationalQuadratic]:\n",
    "    all_kernels_with_params.extend(generate_kernels(kernel_class, parameter_ranges))\n",
    "\n",
    "def create_multiplicative_kernel(kernel_tuple):\n",
    "    # Start with the first kernel in the tuple\n",
    "    combined_kernel = kernel_tuple[0]\n",
    "    # Iterate over the remaining kernels and combine them\n",
    "    for kernel in kernel_tuple[1:]:\n",
    "        combined_kernel = Product(combined_kernel, kernel)\n",
    "    return combined_kernel\n",
    "\n",
    "# Function to create an additive kernel from a combination\n",
    "def create_additive_kernel(kernel_tuple):\n",
    "    # Start with the first kernel in the tuple\n",
    "    combined_kernel = kernel_tuple[0]\n",
    "    # Iterate over the remaining kernels and combine them\n",
    "    for kernel in kernel_tuple[1:]:\n",
    "        combined_kernel = Sum(combined_kernel, kernel)\n",
    "    return combined_kernel\n",
    "\n",
    "\n",
    "# Generate all possible combinations of single kernels, multiplicative and additive combinations of two kernels\n",
    "kernel_combinations_with_params = all_kernels_with_params.copy()\n",
    "\n",
    "for r in range(2, 4):  # r in range(2, 4) to generate combinations of two and three kernels\n",
    "    for combo in combinations(all_kernels_with_params, r):\n",
    "        multiplicative_kernel = create_multiplicative_kernel(combo)\n",
    "        additive_kernel = create_additive_kernel(combo)\n",
    "        kernel_combinations_with_params.extend([multiplicative_kernel, additive_kernel])\n",
    "\n",
    "loocv_scores = []\n",
    "\n",
    "for kernel in tqdm(kernel_combinations_with_params):\n",
    "    scores = []\n",
    "    for i in range(len(data_points)):\n",
    "        train_points = np.delete(data_points, i, axis=0)\n",
    "        train_strengths = np.delete(signal_strengths, i, axis=0)\n",
    "        test_point = data_points[i, :].reshape(1, -1)\n",
    "        test_strength = signal_strengths[i]\n",
    "\n",
    "        gp = GaussianProcessRegressor(kernel=kernel, n_restarts_optimizer=15, alpha=1e-10)\n",
    "        try:\n",
    "            gp.fit(train_points, train_strengths)\n",
    "            predicted_strength, _ = gp.predict(test_point, return_std=True)\n",
    "            mse = mean_squared_error([test_strength], predicted_strength)\n",
    "            scores.append(mse)\n",
    "        except np.linalg.LinAlgError:\n",
    "            scores = []\n",
    "            break\n",
    "\n",
    "    if scores:\n",
    "        avg_score = np.mean(scores)\n",
    "        loocv_scores.append((kernel, avg_score))\n",
    "\n",
    "# Find the kernel with the best average score\n",
    "sorted_loocv_scores = sorted(loocv_scores, key=lambda x: x[1])\n",
    "\n",
    "# Define the number of top kernels to print\n",
    "k = 10  \n",
    "\n",
    "# Print the top k kernels and their scores\n",
    "print(f\"Top {k} Kernels and Their LOOCV Scores:\")\n",
    "for kernel, score in sorted_loocv_scores[:k]:\n",
    "    print(kernel)\n",
    "    print(\"LOOCV Score:\", score)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab68e146",
   "metadata": {},
   "outputs": [],
   "source": [
    "loocv_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b671154",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.gaussian_process.kernels import DotProduct, RationalQuadratic, WhiteKernel, ConstantKernel, RBF, Product, Sum\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from itertools import combinations\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Coordinates of the data collection points\n",
    "data_points = np.array([\n",
    "    [966, 2992],\n",
    "    [2569, 3767],\n",
    "    [2873, 3447],\n",
    "    [2621, 4286],\n",
    "    [1312, 3830],\n",
    "    [3828, 2667],\n",
    "    [242, 2442],\n",
    "    [1711, 3145],\n",
    "    [2852, 1584],\n",
    "    [1903, 2393]\n",
    "])\n",
    "\n",
    "# Sample signal strengths at these data points\n",
    "signal_strengths = rssi_5190_5210\n",
    "\n",
    "# Example parameter ranges for each kernel\n",
    "parameter_ranges = {\n",
    "    RBF: {'length_scale': [10, 100.0, 1000.0, 5000]},\n",
    "    ConstantKernel: {'constant_value': [1.0, 10.0, 100, 1000]},\n",
    "    WhiteKernel: {'noise_level': [1e-1, 1e0,1e1, 1e2]},\n",
    "    DotProduct: {'sigma_0': [1e-1, 10, 100, 1000, 5000]},\n",
    "    RationalQuadratic: {'length_scale': [1.0, 10.0, 100], 'alpha': [0.1, 1.0, 10]}\n",
    "}\n",
    "\n",
    "# Function to generate kernels with different parameter settings\n",
    "def generate_kernels(kernel_class, param_ranges):\n",
    "    if kernel_class in param_ranges:\n",
    "        params = param_ranges[kernel_class]\n",
    "        for param_combination in product(*(params[param] for param in params)):\n",
    "            kwargs = dict(zip(params, param_combination))\n",
    "            yield kernel_class(**kwargs)\n",
    "    else:\n",
    "        yield kernel_class()\n",
    "\n",
    "# Generate all kernels with their parameter variations\n",
    "all_kernels_with_params = []\n",
    "for kernel_class in [RBF, ConstantKernel, WhiteKernel, DotProduct, RationalQuadratic]:\n",
    "    all_kernels_with_params.extend(generate_kernels(kernel_class, parameter_ranges))\n",
    "\n",
    "def create_multiplicative_kernel(kernel_tuple):\n",
    "    # Start with the first kernel in the tuple\n",
    "    combined_kernel = kernel_tuple[0]\n",
    "    # Iterate over the remaining kernels and combine them\n",
    "    for kernel in kernel_tuple[1:]:\n",
    "        combined_kernel = Product(combined_kernel, kernel)\n",
    "    return combined_kernel\n",
    "\n",
    "# Function to create an additive kernel from a combination\n",
    "def create_additive_kernel(kernel_tuple):\n",
    "    # Start with the first kernel in the tuple\n",
    "    combined_kernel = kernel_tuple[0]\n",
    "    # Iterate over the remaining kernels and combine them\n",
    "    for kernel in kernel_tuple[1:]:\n",
    "        combined_kernel = Sum(combined_kernel, kernel)\n",
    "    return combined_kernel\n",
    "\n",
    "\n",
    "# Generate all possible combinations of single kernels, multiplicative and additive combinations of two kernels\n",
    "kernel_combinations_with_params = all_kernels_with_params.copy()\n",
    "\n",
    "for r in range(2, 4):  # r in range(2, 4) to generate combinations of two and three kernels\n",
    "    for combo in combinations(all_kernels_with_params, r):\n",
    "        multiplicative_kernel = create_multiplicative_kernel(combo)\n",
    "        additive_kernel = create_additive_kernel(combo)\n",
    "        kernel_combinations_with_params.extend([multiplicative_kernel, additive_kernel])\n",
    "\n",
    "loocv_scores = []\n",
    "\n",
    "for kernel in tqdm(kernel_combinations_with_params):\n",
    "    scores = []\n",
    "    for i in range(len(data_points)):\n",
    "        train_points = np.delete(data_points, i, axis=0)\n",
    "        train_strengths = np.delete(signal_strengths, i, axis=0)\n",
    "        test_point = data_points[i, :].reshape(1, -1)\n",
    "        test_strength = signal_strengths[i]\n",
    "\n",
    "        gp = GaussianProcessRegressor(kernel=kernel, n_restarts_optimizer=15, alpha=1e-10)\n",
    "        try:\n",
    "            gp.fit(train_points, train_strengths)\n",
    "            predicted_strength, _ = gp.predict(test_point, return_std=True)\n",
    "            mse = mean_squared_error([test_strength], predicted_strength)\n",
    "            scores.append(mse)\n",
    "        except np.linalg.LinAlgError:\n",
    "            scores = []\n",
    "            break\n",
    "\n",
    "    if scores:\n",
    "        avg_score = np.mean(scores)\n",
    "        loocv_scores.append((kernel, avg_score))\n",
    "\n",
    "# Find the kernel with the best average score\n",
    "sorted_loocv_scores = sorted(loocv_scores, key=lambda x: x[1])\n",
    "\n",
    "# Define the number of top kernels to print\n",
    "k = 10  \n",
    "\n",
    "# Print the top k kernels and their scores\n",
    "print(f\"Top {k} Kernels and Their LOOCV Scores:\")\n",
    "for kernel, score in sorted_loocv_scores[:k]:\n",
    "    print(kernel)\n",
    "    print(\"LOOCV Score:\", score)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "817c432b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b470aa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "loocv_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66a35197",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b97ce853",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.gaussian_process.kernels import DotProduct, RationalQuadratic, WhiteKernel, ConstantKernel, RBF\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from tqdm import tqdm\n",
    "# Coordinates of the data collection points (red squares on the map)\n",
    "data_points = np.array([\n",
    "    [966, 2992],\n",
    "    [2569, 3767],\n",
    "    [2873, 3447],\n",
    "    [2621, 4286],\n",
    "    [1312, 3830],\n",
    "    [3828, 2667],\n",
    "    [242, 2442],\n",
    "    [1711, 3145],\n",
    "    [2852, 1584],\n",
    "    [1903, 2393]\n",
    "])\n",
    "#data_points = data_points[:, ::-1]\n",
    "\n",
    "\n",
    "# Sample signal strengths at these data points (randomly generated for this example)\n",
    "signal_strengths = np.array([-86.35, -96.03, -99.60, -99.47, -92.57, -98.94, -85.31, -100.40, -99.86, -88.08])\n",
    "\n",
    "\n",
    "# Create a mesh grid to represent the 2D space for PDF calculation.\n",
    "x = np.linspace(0, map_.shape[1], map_.shape[1] , endpoint=False)\n",
    "y = np.linspace(0, map_.shape[0], map_.shape[0], endpoint=False)\n",
    "X, Y = np.meshgrid(x, y)\n",
    "xy = np.vstack([X.ravel(), Y.ravel()]).T\n",
    "\n",
    "\n",
    "\n",
    "from itertools import combinations, product\n",
    "from sklearn.gaussian_process.kernels import DotProduct, RationalQuadratic, WhiteKernel, ConstantKernel, RBF, Product\n",
    "\n",
    "# Example parameter ranges for each kernel\n",
    "parameter_ranges = {\n",
    "    RBF: {'length_scale': [100.0, 1000.0]},\n",
    "    ConstantKernel: {'constant_value': [1.0, 10.0]},\n",
    "    WhiteKernel: {'noise_level': [1e-1, 1e1]},\n",
    "    DotProduct: {'sigma_0': [1e-1, 100, 1000]},\n",
    "    RationalQuadratic: {'length_scale': [1.0, 10.0], 'alpha': [0.1, 1.0]}\n",
    "}\n",
    "# Function to create a multiplicative combination of kernels\n",
    "def create_multiplicative_kernel(combo):\n",
    "    # Start with the first kernel in the combo\n",
    "    multiplicative_kernel = combo[0]\n",
    "    # Multiply with each subsequent kernel in the combo\n",
    "    for kernel in combo[1:]:\n",
    "        multiplicative_kernel *= kernel\n",
    "    return multiplicative_kernel\n",
    "\n",
    "# Generate all possible combinations of kernels, including multiplication\n",
    "kernel_combinations_with_params = []\n",
    "\n",
    "for r in range(1, len(all_kernels) + 1):\n",
    "    for combo in combinations(all_kernels, r):\n",
    "        # Additive combination\n",
    "        additive_kernel = sum(combo)\n",
    "        kernel_combinations_with_params.append(additive_kernel)\n",
    "        \n",
    "        # Multiplicative combination (if more than one kernel in the combo)\n",
    "        if len(combo) > 1:\n",
    "            multiplicative_kernel = create_multiplicative_kernel(combo)\n",
    "            kernel_combinations_with_params.append(multiplicative_kernel)\n",
    "\n",
    "loocv_scores = []\n",
    "it = 0\n",
    "for kernel in tqdm(kernel_combinations_with_params):\n",
    "    it+=1\n",
    "    scores = []\n",
    "    for i in range(len(data_points)):\n",
    "        # Leave one out: Use all data points except the ith point for training\n",
    "        train_points = np.delete(data_points, i, axis=0)\n",
    "        train_strengths = np.delete(signal_strengths, i, axis=0)\n",
    "        test_point = data_points[i, :].reshape(1, -1)\n",
    "        test_strength = signal_strengths[i]\n",
    "\n",
    "        # Create and fit the Gaussian Process model\n",
    "        gp = GaussianProcessRegressor(kernel=kernel, n_restarts_optimizer=15, alpha=1e-10)\n",
    "        try:\n",
    "            gp.fit(train_points, train_strengths)\n",
    "            # Predict the signal strength for the test point\n",
    "            predicted_strength, _ = gp.predict(test_point, return_std=True)\n",
    "            # Calculate the score (mean squared error) for this iteration\n",
    "            mse = mean_squared_error([test_strength], predicted_strength)\n",
    "            scores.append(mse)\n",
    "        except np.linalg.LinAlgError:\n",
    "            # Skip this kernel combination due to numerical error\n",
    "            continue\n",
    "\n",
    "    # Calculate the average score for this kernel over all iterations\n",
    "    if scores:\n",
    "        #print(kernel)\n",
    "        avg_score = np.mean(scores)\n",
    "        loocv_scores.append((kernel, avg_score))\n",
    "        #print(avg_score)\n",
    "        if it%1000 ==0:\n",
    "            best_kernel, best_loocv_score = min(loocv_scores, key=lambda x: x[1])\n",
    "            print(best_kernel)\n",
    "            print(best_loocv_score)\n",
    "\n",
    "\n",
    "# Find the kernel with the best average score\n",
    "best_kernel, best_loocv_score = min(loocv_scores, key=lambda x: x[1])\n",
    "\n",
    "best_kernel, best_loocv_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7545c168",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the kernel with the best average score\n",
    "best_kernel, best_loocv_score = min(loocv_scores, key=lambda x: x[1])\n",
    "\n",
    "best_kernel, best_loocv_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eccfb2c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26590bd3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0964852",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.gaussian_process.kernels import DotProduct, RationalQuadratic, WhiteKernel, ConstantKernel, RBF\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Coordinates of the data collection points (red squares on the map)\n",
    "data_points = np.array([\n",
    "    [966, 2992],\n",
    "    [2569, 3767],\n",
    "    [2873, 3447],\n",
    "    [2621, 4286],\n",
    "    [1312, 3830],\n",
    "    [3828, 2667],\n",
    "    [242, 2442],\n",
    "    [1711, 3145],\n",
    "    [2852, 1584],\n",
    "    [1903, 2393]\n",
    "])\n",
    "#data_points = data_points[:, ::-1]\n",
    "\n",
    "\n",
    "# Sample signal strengths at these data points (randomly generated for this example)\n",
    "signal_strengths = np.array([-86.35, -96.03, -99.60, -99.47, -92.57, -98.94, -85.31, -100.40, -99.86, -88.08])\n",
    "\n",
    "\n",
    "# Create a mesh grid to represent the 2D space for PDF calculation.\n",
    "x = np.linspace(0, map_.shape[1], map_.shape[1] , endpoint=False)\n",
    "y = np.linspace(0, map_.shape[0], map_.shape[0], endpoint=False)\n",
    "X, Y = np.meshgrid(x, y)\n",
    "xy = np.vstack([X.ravel(), Y.ravel()]).T\n",
    "\n",
    "\n",
    "\n",
    "from itertools import product\n",
    "\n",
    "# Example parameter ranges for each kernel\n",
    "parameter_ranges = {\n",
    "    RBF: {'length_scale': [1.0, 10.0]},\n",
    "    ConstantKernel: {'constant_value': [1.0, 10.0]},\n",
    "    WhiteKernel: {'noise_level': [1e-1, 1e0]},\n",
    "    DotProduct: {'sigma_0': [1e-2, 1e1]},\n",
    "    RationalQuadratic: {'length_scale': [1.0, 10.0], 'alpha': [0.1, 1.0]}\n",
    "}\n",
    "\n",
    "# Function to create kernels with varying parameters\n",
    "def create_kernels_with_params():\n",
    "    all_kernels = []\n",
    "    for kernel_cls, params in parameter_ranges.items():\n",
    "        keys, values = zip(*params.items())\n",
    "        for v in product(*values):\n",
    "            param_dict = dict(zip(keys, v))\n",
    "            kernel = kernel_cls(**param_dict)\n",
    "            all_kernels.append(kernel)\n",
    "    return all_kernels\n",
    "\n",
    "# Generate all kernel combinations with varying parameters\n",
    "all_kernels = create_kernels_with_params()\n",
    "kernel_combinations_with_params = []\n",
    "for r in range(1, len(all_kernels) + 1):\n",
    "    for combo in combinations(all_kernels, r):\n",
    "        kernel_combinations_with_params.append(sum(combo))\n",
    "\n",
    "\n",
    "loocv_scores = []\n",
    "\n",
    "for kernel in kernel_combinations_with_params:\n",
    "    scores = []\n",
    "    for i in range(len(data_points)):\n",
    "        # Leave one out: Use all data points except the ith point for training\n",
    "        train_points = np.delete(data_points, i, axis=0)\n",
    "        train_strengths = np.delete(signal_strengths, i, axis=0)\n",
    "        test_point = data_points[i, :].reshape(1, -1)\n",
    "        test_strength = signal_strengths[i]\n",
    "\n",
    "        # Create and fit the Gaussian Process model\n",
    "        gp = GaussianProcessRegressor(kernel=kernel, n_restarts_optimizer=15, alpha=1e-10)\n",
    "        try:\n",
    "            gp.fit(train_points, train_strengths)\n",
    "            # Predict the signal strength for the test point\n",
    "            predicted_strength, _ = gp.predict(test_point, return_std=True)\n",
    "            # Calculate the score (mean squared error) for this iteration\n",
    "            mse = mean_squared_error([test_strength], predicted_strength)\n",
    "            scores.append(mse)\n",
    "        except np.linalg.LinAlgError:\n",
    "            # Skip this kernel combination due to numerical error\n",
    "            continue\n",
    "\n",
    "    # Calculate the average score for this kernel over all iterations\n",
    "    if scores:\n",
    "        print(kernel)\n",
    "        avg_score = np.mean(scores)\n",
    "        loocv_scores.append((kernel, avg_score))\n",
    "        print(avg_score)\n",
    "\n",
    "# Find the kernel with the best average score\n",
    "best_kernel, best_loocv_score = min(loocv_scores, key=lambda x: x[1])\n",
    "\n",
    "best_kernel, best_loocv_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a0e9403",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.std(signal_strengths)**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4d60f26",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_kernel, best_loocv_score = min(loocv_scores, key=lambda x: x[1])\n",
    "\n",
    "best_kernel, best_loocv_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aecd872",
   "metadata": {},
   "outputs": [],
   "source": [
    "Z_mean = np.array([1.5, np.nan, 2.1, 3.0, np.nan, 4.2])\n",
    "print(np.min(Z_mean[np.isnan(Z_mean)==0]))\n",
    "# Replacing NaN values with 0\n",
    "Z_mean = np.nan_to_num(Z_mean)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "655dbe35",
   "metadata": {},
   "outputs": [],
   "source": [
    "Z_mean"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7761a27",
   "metadata": {},
   "source": [
    "### Try Best Kernel Combo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "110da097",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.gaussian_process.kernels import RBF, ConstantKernel as C, DotProduct, WhiteKernel, RationalQuadratic\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Coordinates of the data collection points (red squares on the map)\n",
    "data_points = np.array([\n",
    "    [966, 2992],\n",
    "    [2569, 3767],\n",
    "    [2873, 3447],\n",
    "    [2621, 4286],\n",
    "    [1312, 3830],\n",
    "    [3828, 2667],\n",
    "    [242, 2442],\n",
    "    [1711, 3145],\n",
    "    [2852, 1584],\n",
    "    [1903, 2393]\n",
    "])\n",
    "#data_points = data_points[:, ::-1]\n",
    "\n",
    "# Sample signal strengths at these data points (randomly generated for this example)\n",
    "signal_strengths = dB_to_lin(rssi_3470_3510) #np.array([-86.35, -96.03, -99.60, -99.47, -92.57, -98.94, -85.31, -100.40, -99.86, -88.08])\n",
    "\n",
    "# Define a Gaussian Process model with a Radial Basis Function (RBF) kernel.\n",
    "#kernel = C(1, (1e-3, 1e4)) * RBF(length_scale=1000.0, length_scale_bounds=(1e2, 1e4))\n",
    "#kernel = C(1.0, (1e-3, 5e3)) * RBF(length_scale=2000.0, length_scale_bounds=(1e2, 5e4)) + WhiteKernel(noise_level=1, noise_level_bounds=(1e-10, 5e+1))\n",
    "#kernel = C(constant_value=1**2) * C(constant_value=3.16**2) * DotProduct(sigma_0=0.1) * DotProduct(sigma_0=1000)\n",
    "#kernel =  C(constant_value=3.16**2)* DotProduct(sigma_0=100)\n",
    "kernel = C(constant_value=1e-9, constant_value_bounds = (1e-11, 1e-8)) + RBF(length_scale=10.0, length_scale_bounds=(1e0, 1e3))\n",
    "print(kernel)\n",
    "gp = GaussianProcessRegressor(kernel=kernel, n_restarts_optimizer=20, alpha=1e-10)\n",
    "\n",
    "#kernel = DotProduct(sigma_0=1.0, sigma_0_bounds=(1e-05, 100000.0))\n",
    "#gp = GaussianProcessRegressor(kernel=kernel, n_restarts_optimizer=50, alpha = 1e-7)\n",
    "\n",
    "# Fit the model to the data points.\n",
    "gp.fit(data_points, signal_strengths)\n",
    "\n",
    "# Create a mesh grid to represent the 2D space for PDF calculation.\n",
    "x = np.linspace(0, map_.shape[1], map_.shape[1] , endpoint=False)\n",
    "y = np.linspace(0, map_.shape[0], map_.shape[0], endpoint=False)\n",
    "X, Y = np.meshgrid(x, y)\n",
    "xy = np.vstack([X.ravel(), Y.ravel()]).T\n",
    "\n",
    "# Predict the signal strength at each point in the mesh grid.\n",
    "Z_mean, Z_std = gp.predict(xy, return_std=True)\n",
    "Z_mean = lin_to_dB(Z_mean)\n",
    "Z_mean = np.nan_to_num(Z_mean, nan=np.min(Z_mean[np.isnan(Z_mean)==0]))\n",
    "signal_strengths = lin_to_dB(signal_strengths)\n",
    "Z_mean = Z_mean.reshape(X.shape)\n",
    "Z_std = Z_std.reshape(X.shape)\n",
    "\n",
    "\n",
    "\n",
    "plt.figure(figsize=(13, 8))\n",
    "plt.contourf(X, Y, Z_mean, 100, cmap='viridis')\n",
    "plt.colorbar(label='dBX')\n",
    "#data_points = data_points[:, ::-1]\n",
    "# Plot the data collection points for reference\n",
    "scatter = plt.scatter(data_points[:, 0], data_points[:, 1], c=signal_strengths, s=50, cmap='plasma', label='Data Collection Points')\n",
    "cbar = plt.colorbar(scatter, label='Signal Strength (dBX)', location='left')  # Colorbar for signal strengths on the left\n",
    "plt.legend()\n",
    "\n",
    "plt.xlabel('UTM_E [m]')\n",
    "plt.ylabel('UTM_N [m]')\n",
    "plt.title('2D Predictions of Signal Strength')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Normalize the signal strength predictions to form a probability distribution\n",
    "max_strength = np.max(Z_mean)\n",
    "probabilities = np.exp(Z_mean - max_strength)  # Exponential to make it a positive distribution\n",
    "probabilities /= np.sum(probabilities)  # Normalize to sum up to 1\n",
    "\n",
    "plt.figure(figsize=(13, 8))\n",
    "plt.contourf(X, Y, probabilities, 100, cmap='viridis')\n",
    "plt.colorbar(label='Probability')\n",
    "\n",
    "# Plot the data collection points with a different colormap\n",
    "scatter = plt.scatter(data_points[:, 0], data_points[:, 1], c=signal_strengths, s=50, cmap='plasma', label='Data Collection Points')\n",
    "cbar = plt.colorbar(scatter, label='Signal Strength (dBX)', location='left')  # Colorbar for signal strengths on the left\n",
    "plt.legend()\n",
    "\n",
    "plt.xlabel('UTM_E [m]')\n",
    "plt.ylabel('UTM_N [m]')\n",
    "plt.title('2D Probability Distribution of Transmitter Location')\n",
    "plt.show()\n",
    "\n",
    "print(np.sum(probabilities[:100, :100]))\n",
    "\n",
    "\n",
    "# Calculate the 2D cumulative distribution function (CDF)\n",
    "cumulative_prob = np.cumsum(probabilities, axis=0)\n",
    "cumulative_prob = np.cumsum(cumulative_prob, axis=1)\n",
    "\n",
    "# Plot the 2D CDF\n",
    "plt.figure(figsize=(13, 8))\n",
    "plt.contourf(X, Y, cumulative_prob, 100, cmap='viridis')\n",
    "plt.colorbar(label='Cumulative Probability')\n",
    "\n",
    "# Plot the data collection points for reference\n",
    "scatter = plt.scatter(data_points[:, 0], data_points[:, 1], c=signal_strengths, s=50, cmap='jet', label='Data Collection Points')\n",
    "cbar = plt.colorbar(scatter, label='Signal Strength (dBX)', location='left')  # Colorbar for signal strengths on the left\n",
    "plt.legend()\n",
    "\n",
    "plt.xlabel('UTM_E [m]')\n",
    "plt.ylabel('UTM_N [m]')\n",
    "plt.title('2D Cumulative Distribution Function of Transmitter Location')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "Zmean_dp = []\n",
    "for dp in data_points:\n",
    "    Zmean_dp.append(Z_mean[dp[0], dp[1]])\n",
    "\n",
    "err = signal_strengths-np.array(Zmean_dp)\n",
    "print(err)\n",
    "print(np.mean(err))\n",
    "print(np.std(err))\n",
    "\n",
    "scores = []\n",
    "loocv_scores = []\n",
    "for i in range(len(data_points)):\n",
    "    # Leave one out: Use all data points except the ith point for training\n",
    "    train_points = np.delete(data_points, i, axis=0)\n",
    "    train_strengths = np.delete(signal_strengths, i, axis=0)\n",
    "    test_point = data_points[i, :].reshape(1, -1)\n",
    "    test_strength = signal_strengths[i]\n",
    "\n",
    "    # Create and fit the Gaussian Process model\n",
    "    gp = GaussianProcessRegressor(kernel=kernel, n_restarts_optimizer=15, alpha=1e-10)\n",
    "    try:\n",
    "        gp.fit(train_points, train_strengths)\n",
    "        # Predict the signal strength for the test point\n",
    "        predicted_strength, _ = gp.predict(test_point, return_std=True)\n",
    "        # Calculate the score (mean squared error) for this iteration\n",
    "        mse = mean_squared_error([test_strength], predicted_strength)\n",
    "        scores.append(mse)\n",
    "    except np.linalg.LinAlgError:\n",
    "        print(\"err\")\n",
    "        # Skip this kernel combination due to numerical error\n",
    "        continue\n",
    "\n",
    "avg_score = np.mean(scores)\n",
    "loocv_scores.append((kernel, avg_score))\n",
    "print(loocv_scores)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0ae6640",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "542feb47",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06af6a34",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.gaussian_process.kernels import RBF, ConstantKernel as C\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "# Assuming the red squares are the locations of data collection, we create dummy data\n",
    "# For demonstration, we'll just generate some random data points.\n",
    "# In a real scenario, you would replace these with your actual measurements.\n",
    "\n",
    "# Coordinates of the data collection points (red squares on the map)\n",
    "data_points = np.array([\n",
    "    [966, 2992],\n",
    "    [2569, 3767],\n",
    "    [2873, 3447],\n",
    "    [2621, 4286],\n",
    "    [1312, 3830],\n",
    "    [3828, 2667],\n",
    "    [242, 2442],\n",
    "    [1711, 3145],\n",
    "    [2852, 1584],\n",
    "    [1903, 2393]\n",
    "])\n",
    "\n",
    "\n",
    "# Sample signal strengths at these data points (randomly generated for this example)\n",
    "signal_strengths = np.array([-86.35, -96.03, -99.60, -99.47, -92.57, -98.94, -85.31, -100.40, -99.86, -88.08])\n",
    "\n",
    "# Define a Gaussian Process model with a Radial Basis Function (RBF) kernel.\n",
    "kernel = C(1.0, (1e-3, 1e4)) * RBF(length_scale=1000.0, length_scale_bounds=(1e-2, 1e5))\n",
    "gp = GaussianProcessRegressor(kernel=kernel, n_restarts_optimizer=10)\n",
    "\n",
    "# Fit the model to the data points.\n",
    "gp.fit(data_points, signal_strengths)\n",
    "\n",
    "# Create a mesh grid to represent the 2D space for PDF calculation.\n",
    "x = np.linspace(0, 5000, 3000)\n",
    "y = np.linspace(0, 5000, 3000)\n",
    "X, Y = np.meshgrid(x, y)\n",
    "xy = np.vstack([X.ravel(), Y.ravel()]).T\n",
    "\n",
    "# Predict the signal strength at each point in the mesh grid.\n",
    "# This gives us the mean signal strength prediction.\n",
    "Z_mean, Z_std = gp.predict(xy, return_std=True)\n",
    "\n",
    "# The mean prediction is used as a proxy for likelihood of transmitter location.\n",
    "# The higher the predicted signal strength, the more likely the transmitter is located there.\n",
    "Z_mean = Z_mean.reshape(X.shape)\n",
    "\n",
    "dx = x[1] - x[0]\n",
    "dy = y[1] - y[0]\n",
    "area_per_point = dx * dy\n",
    "\n",
    "# Since signal strengths are negative, we will offset them by the lowest value to make them positive\n",
    "min_strength = np.min(Z_mean)\n",
    "offset_Z_mean = Z_mean - min_strength\n",
    "\n",
    "# Calculate the volume under the surface with the positive values\n",
    "volume_under_surface = np.sum(offset_Z_mean * area_per_point)\n",
    "\n",
    "# Normalize the shifted predictions to form a true PDF\n",
    "Z_pdf = offset_Z_mean / volume_under_surface\n",
    "\n",
    "# Plot the 2D PDF using the mean prediction.\n",
    "fig = plt.figure(figsize=(10, 8))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "# Use the signal strength as the height for the 3D plot to visualize the PDF.\n",
    "ax.plot_surface(X, Y, Z_pdf, cmap='viridis', edgecolor='none')\n",
    "\n",
    "ax.set_xlabel('UTM_E [m]')\n",
    "ax.set_ylabel('UTM_N [m]')\n",
    "ax.set_zlabel('Signal Strength (PDF)')\n",
    "ax.set_title('2D PDF of Transmitter Location')\n",
    "\n",
    "# Show the plot.\n",
    "plt.show()\n",
    "\n",
    "# Since we already have Z_pdf as a proper 2D PDF, we can directly plot it using contourf or imshow for a 2D visualization.\n",
    "\n",
    "# Plotting the 2D PDF using contourf\n",
    "plt.figure(figsize=(10, 8))\n",
    "contour = plt.contourf(X, Y, Z_pdf, 100, cmap='viridis')\n",
    "plt.colorbar(contour)\n",
    "\n",
    "# Plot the data collection points for reference\n",
    "plt.scatter(data_points[:, 0], data_points[:, 1], c='red', s=50, label='Data Collection Points')\n",
    "plt.legend()\n",
    "\n",
    "plt.xlabel('UTM_E [m]')\n",
    "plt.ylabel('UTM_N [m]')\n",
    "plt.title('2D PDF of Transmitter Location')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44af3ddb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.gaussian_process.kernels import RBF, ConstantKernel as C\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "# Assuming the red squares are the locations of data collection, we create dummy data\n",
    "# For demonstration, we'll just generate some random data points.\n",
    "# In a real scenario, you would replace these with your actual measurements.\n",
    "\n",
    "# Coordinates of the data collection points (red squares on the map)\n",
    "data_points = np.array([\n",
    "    [966, 2992],\n",
    "    [2569, 3767],\n",
    "    [2873, 3447],\n",
    "    [2621, 4286],\n",
    "    [1312, 3830],\n",
    "    [3828, 2667],\n",
    "    [242, 2442],\n",
    "    [1711, 3145],\n",
    "    [2852, 1584],\n",
    "    [1903, 2393]\n",
    "])\n",
    "\n",
    "# Sample signal strengths at these data points (randomly generated for this example)\n",
    "signal_strengths = np.array([-86.35, -96.03, -99.60, -99.47, -92.57, -98.94, -85.31, -100.40, -99.86, -88.08])\n",
    "\n",
    "# Define a Gaussian Process model with a Radial Basis Function (RBF) kernel.\n",
    "kernel = C(10.0, (1e-3, 1e4)) * RBF(length_scale=1000.0, length_scale_bounds=(1e-2, 2e3))\n",
    "gp = GaussianProcessRegressor(kernel=kernel, n_restarts_optimizer=15)\n",
    "\n",
    "# Fit the model to the data points.\n",
    "gp.fit(data_points, signal_strengths)\n",
    "\n",
    "# Create a mesh grid to represent the 2D space for PDF calculation.\n",
    "x = np.linspace(0, map_.shape[1], map_.shape[1], endpoint=False)\n",
    "y = np.linspace(0, map_.shape[0], map_.shape[0], endpoint=False)\n",
    "X, Y = np.meshgrid(x, y)\n",
    "xy = np.vstack([X.ravel(), Y.ravel()]).T\n",
    "\n",
    "# Predict the signal strength at each point in the mesh grid.\n",
    "# This gives us the mean signal strength prediction.\n",
    "Z_mean, Z_std = gp.predict(xy, return_std=True)\n",
    "\n",
    "# The mean prediction is used as a proxy for likelihood of transmitter location.\n",
    "# The higher the predicted signal strength, the more likely the transmitter is located there.\n",
    "Z_mean = Z_mean.reshape(map_.shape)\n",
    "max_strength = np.max(Z_mean)\n",
    "probabilities = np.exp(Z_mean - max_strength)  # Exponential to make it a positive distribution\n",
    "probabilities /= np.sum(probabilities)  # Normalize to sum up to 1\n",
    "\n",
    "\n",
    "# Plot the 2D PDF using the mean prediction.\n",
    "fig = plt.figure(figsize=(10, 8))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "# Use the signal strength as the height for the 3D plot to visualize the PDF.\n",
    "ax.plot_surface(X, Y, Z_mean, cmap='viridis', edgecolor='none')\n",
    "\n",
    "ax.set_xlabel('UTM_E [m]')\n",
    "ax.set_ylabel('UTM_N [m]')\n",
    "ax.set_zlabel('Signal Strength (PDF)')\n",
    "ax.set_title('2D PDF of Transmitter Location')\n",
    "\n",
    "# Show the plot.\n",
    "plt.show()\n",
    "\n",
    "# Since we already have Z_pdf as a proper 2D PDF, we can directly plot it using contourf or imshow for a 2D visualization.\n",
    "\n",
    "# Plotting the 2D PDF using contourf\n",
    "plt.figure(figsize=(13, 8))\n",
    "contour = plt.contourf(X, Y, probabilities, 100, cmap='viridis')\n",
    "plt.colorbar(contour)\n",
    "\n",
    "# Plot the data collection points for reference\n",
    "scatter = plt.scatter(data_points[:, 0], data_points[:, 1], c=signal_strengths, s=50, cmap='jet', label='Data Collection Points')\n",
    "cbar = plt.colorbar(scatter, label='Signal Strength (dBX)', location='left')  # Colorbar for signal strengths on the left\n",
    "plt.legend()\n",
    "\n",
    "plt.xlabel('UTM_E [m]')\n",
    "plt.ylabel('UTM_N [m]')\n",
    "plt.title('2D PDF of Transmitter Location')\n",
    "plt.show()\n",
    "\n",
    "Zmean_dp = []\n",
    "for dp in data_points:\n",
    "    Zmean_dp.append(Z_mean[dp[1], dp[0]])\n",
    "print(Zmean_dp)\n",
    "\n",
    "err = signal_strengths-np.array(Zmean_dp)\n",
    "print(err)\n",
    "print(np.mean(err))\n",
    "print(np.std(err))\n",
    "\n",
    "print(Z_mean.shape)\n",
    "print(probabilities.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92604a6e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1cc06ab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12316180",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "75dfabd0",
   "metadata": {},
   "source": [
    "### RBF Kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "428e020a",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.gaussian_process.kernels import RBF, ConstantKernel as C, DotProduct, WhiteKernel\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "# Coordinates of the data collection points (red squares on the map)\n",
    "data_points = np.array([\n",
    "    [966, 2992],\n",
    "    [2569, 3767],\n",
    "    [2873, 3447],\n",
    "    [2621, 4286],\n",
    "    [1312, 3830],\n",
    "    [3828, 2667],\n",
    "    [242, 2442],\n",
    "    [1711, 3145],\n",
    "    [2852, 1584],\n",
    "    [1903, 2393]\n",
    "])\n",
    "data_points = data_points[:, ::-1]\n",
    "\n",
    "# Sample signal strengths at these data points (randomly generated for this example)\n",
    "signal_strengths = np.array([-86.35, -96.03, -99.60, -99.47, -92.57, -98.94, -85.31, -100.40, -99.86, -88.08])\n",
    "\n",
    "# Define a Gaussian Process model with a Radial Basis Function (RBF) kernel.\n",
    "#kernel = C(1, (1e-3, 1e4)) * RBF(length_scale=1000.0, length_scale_bounds=(1e2, 1e4))\n",
    "kernel = C(1.0, (1e-3, 5e3)) * RBF(length_scale=2000.0, length_scale_bounds=(1e2, 5e4)) + WhiteKernel(noise_level=1, noise_level_bounds=(1e-10, 5e+1))\n",
    "\n",
    "gp = GaussianProcessRegressor(kernel=kernel, n_restarts_optimizer=15, alpha=1e-10)\n",
    "\n",
    "#kernel = DotProduct(sigma_0=1.0, sigma_0_bounds=(1e-05, 100000.0))\n",
    "#gp = GaussianProcessRegressor(kernel=kernel, n_restarts_optimizer=50, alpha = 1e-7)\n",
    "\n",
    "# Fit the model to the data points.\n",
    "gp.fit(data_points, signal_strengths)\n",
    "\n",
    "# Create a mesh grid to represent the 2D space for PDF calculation.\n",
    "x = np.linspace(0, map_.shape[0], map_.shape[0] , endpoint=False)\n",
    "y = np.linspace(0, map_.shape[1], map_.shape[1], endpoint=False)\n",
    "X, Y = np.meshgrid(x, y)\n",
    "xy = np.vstack([X.ravel(), Y.ravel()]).T\n",
    "\n",
    "# Predict the signal strength at each point in the mesh grid.\n",
    "Z_mean, Z_std = gp.predict(xy, return_std=True)\n",
    "Z_mean = Z_mean.reshape(X.shape)\n",
    "Z_std = Z_std.reshape(X.shape)\n",
    "\n",
    "\n",
    "\n",
    "plt.figure(figsize=(13, 8))\n",
    "plt.contourf(X, Y, Z_mean, 100, cmap='viridis')\n",
    "plt.colorbar(label='dBX')\n",
    "data_points = data_points[:, ::-1]\n",
    "# Plot the data collection points for reference\n",
    "scatter = plt.scatter(data_points[:, 0], data_points[:, 1], c=signal_strengths, s=50, cmap='plasma', label='Data Collection Points')\n",
    "cbar = plt.colorbar(scatter, label='Signal Strength (dBX)', location='left')  # Colorbar for signal strengths on the left\n",
    "plt.legend()\n",
    "\n",
    "plt.xlabel('UTM_E [m]')\n",
    "plt.ylabel('UTM_N [m]')\n",
    "plt.title('2D Predictions of Signal Strength')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Normalize the signal strength predictions to form a probability distribution\n",
    "max_strength = np.max(Z_mean)\n",
    "probabilities = np.exp(Z_mean - max_strength)  # Exponential to make it a positive distribution\n",
    "probabilities /= np.sum(probabilities)  # Normalize to sum up to 1\n",
    "\n",
    "plt.figure(figsize=(13, 8))\n",
    "plt.contourf(X, Y, probabilities, 100, cmap='viridis')\n",
    "plt.colorbar(label='Probability')\n",
    "\n",
    "# Plot the data collection points with a different colormap\n",
    "scatter = plt.scatter(data_points[:, 0], data_points[:, 1], c=signal_strengths, s=50, cmap='plasma', label='Data Collection Points')\n",
    "cbar = plt.colorbar(scatter, label='Signal Strength (dBX)', location='left')  # Colorbar for signal strengths on the left\n",
    "plt.legend()\n",
    "\n",
    "plt.xlabel('UTM_E [m]')\n",
    "plt.ylabel('UTM_N [m]')\n",
    "plt.title('2D Probability Distribution of Transmitter Location')\n",
    "plt.show()\n",
    "\n",
    "print(np.sum(probabilities[:100, :100]))\n",
    "\n",
    "\n",
    "# Calculate the 2D cumulative distribution function (CDF)\n",
    "cumulative_prob = np.cumsum(probabilities, axis=0)\n",
    "cumulative_prob = np.cumsum(cumulative_prob, axis=1)\n",
    "\n",
    "# Plot the 2D CDF\n",
    "plt.figure(figsize=(13, 8))\n",
    "plt.contourf(X, Y, cumulative_prob, 100, cmap='viridis')\n",
    "plt.colorbar(label='Cumulative Probability')\n",
    "\n",
    "# Plot the data collection points for reference\n",
    "scatter = plt.scatter(data_points[:, 0], data_points[:, 1], c=signal_strengths, s=50, cmap='jet', label='Data Collection Points')\n",
    "cbar = plt.colorbar(scatter, label='Signal Strength (dBX)', location='left')  # Colorbar for signal strengths on the left\n",
    "plt.legend()\n",
    "\n",
    "plt.xlabel('UTM_E [m]')\n",
    "plt.ylabel('UTM_N [m]')\n",
    "plt.title('2D Cumulative Distribution Function of Transmitter Location')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "Zmean_dp = []\n",
    "for dp in data_points:\n",
    "    Zmean_dp.append(Z_mean[dp[0], dp[1]])\n",
    "\n",
    "err = signal_strengths-np.array(Zmean_dp)\n",
    "print(err)\n",
    "print(np.mean(err))\n",
    "print(np.std(err))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eb262c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savez(\"GPR_output_rbf.npz\", X, Y, Z_mean, probabilities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75416fe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum(probabilities[:1200, :1200])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b659ec38",
   "metadata": {},
   "outputs": [],
   "source": [
    "map_.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e04aaec9",
   "metadata": {},
   "source": [
    "### DotProduct + WhiteKernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a8c475e",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.gaussian_process.kernels import DotProduct\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "# Coordinates of the data collection points (red squares on the map)\n",
    "data_points = np.array([\n",
    "    [966, 2992],\n",
    "    [2569, 3767],\n",
    "    [2873, 3447],\n",
    "    [2621, 4286],\n",
    "    [1312, 3830],\n",
    "    [3828, 2667],\n",
    "    [242, 2442],\n",
    "    [1711, 3145],\n",
    "    [2852, 1584],\n",
    "    [1903, 2393]\n",
    "])\n",
    "\n",
    "\n",
    "# Sample signal strengths at these data points (randomly generated for this example)\n",
    "signal_strengths = np.array([-86.35, -96.03, -99.60, -99.47, -92.57, -98.94, -85.31, -100.40, -99.86, -88.08])\n",
    "\n",
    "# Define a Gaussian Process model with a Radial Basis Function (RBF) kernel.\n",
    "#kernel = C(10.0, (1e-3, 1e4)) * RBF(length_scale=1000.0, length_scale_bounds=(1e-2, 2e3))\n",
    "kernel = DotProduct(sigma_0=1.0, sigma_0_bounds=(1e-05, 100000.0)) + WhiteKernel(noise_level=1, noise_level_bounds=(1e-10, 5e+1))\n",
    "gp = GaussianProcessRegressor(kernel=kernel, n_restarts_optimizer=10, alpha = 1e-7)\n",
    "\n",
    "# Fit the model to the data points.\n",
    "gp.fit(data_points, signal_strengths)\n",
    "\n",
    "# Create a mesh grid to represent the 2D space for PDF calculation.\n",
    "x = np.linspace(0, map_.shape[0], map_.shape[0] , endpoint=False)\n",
    "y = np.linspace(0, map_.shape[1], map_.shape[1], endpoint=False)\n",
    "X, Y = np.meshgrid(x, y)\n",
    "xy = np.vstack([X.ravel(), Y.ravel()]).T\n",
    "\n",
    "print(xy[0:-2])\n",
    "# Predict the signal strength at each point in the mesh grid.\n",
    "Z_mean, Z_std = gp.predict(xy, return_std=True)\n",
    "Z_mean = Z_mean.reshape(X.shape)\n",
    "Z_std = Z_std.reshape(X.shape)\n",
    "\n",
    "\n",
    "\n",
    "plt.figure(figsize=(13, 8))\n",
    "plt.contourf(X, Y, Z_mean, 100, cmap='viridis')\n",
    "plt.colorbar(label='dBX')\n",
    "data_points = data_points[:, ::-1]\n",
    "\n",
    "# Plot the data collection points for reference\n",
    "scatter = plt.scatter(data_points[:, 0], data_points[:, 1], c=signal_strengths, s=50, cmap='plasma', label='Data Collection Points')\n",
    "cbar = plt.colorbar(scatter, label='Signal Strength (dBX)', location='left')  # Colorbar for signal strengths on the left\n",
    "plt.legend()\n",
    "\n",
    "plt.xlabel('UTM_E [m]')\n",
    "plt.ylabel('UTM_N [m]')\n",
    "plt.title('2D Predictions of Signal Strength')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Normalize the signal strength predictions to form a probability distribution\n",
    "max_strength = np.max(Z_mean)\n",
    "probabilities = np.exp(Z_mean - max_strength)  # Exponential to make it a positive distribution\n",
    "probabilities /= np.sum(probabilities)  # Normalize to sum up to 1\n",
    "\n",
    "plt.figure(figsize=(13, 8))\n",
    "plt.contourf(X, Y, probabilities, 100, cmap='viridis')\n",
    "plt.colorbar(label='Probability')\n",
    "\n",
    "# Plot the data collection points with a different colormap\n",
    "scatter = plt.scatter(data_points[:, 0], data_points[:, 1], c=signal_strengths, s=50, cmap='plasma', label='Data Collection Points')\n",
    "cbar = plt.colorbar(scatter, label='Signal Strength (dBX)', location='left')  # Colorbar for signal strengths on the left\n",
    "plt.legend()\n",
    "\n",
    "plt.xlabel('UTM_E [m]')\n",
    "plt.ylabel('UTM_N [m]')\n",
    "plt.title('2D Probability Distribution of Transmitter Location')\n",
    "plt.show()\n",
    "\n",
    "print(np.sum(probabilities[:100, :100]))\n",
    "\n",
    "\n",
    "# Calculate the 2D cumulative distribution function (CDF)\n",
    "cumulative_prob = np.cumsum(probabilities, axis=0)\n",
    "cumulative_prob = np.cumsum(cumulative_prob, axis=1)\n",
    "\n",
    "# Plot the 2D CDF\n",
    "plt.figure(figsize=(13, 8))\n",
    "plt.contourf(X, Y, cumulative_prob, 100, cmap='viridis')\n",
    "plt.colorbar(label='Cumulative Probability')\n",
    "\n",
    "# Plot the data collection points for reference\n",
    "scatter = plt.scatter(data_points[:, 0], data_points[:, 1], c=signal_strengths, s=50, cmap='jet', label='Data Collection Points')\n",
    "cbar = plt.colorbar(scatter, label='Signal Strength (dBX)', location='left')  # Colorbar for signal strengths on the left\n",
    "plt.legend()\n",
    "\n",
    "plt.xlabel('UTM_E [m]')\n",
    "plt.ylabel('UTM_N [m]')\n",
    "plt.title('2D Cumulative Distribution Function of Transmitter Location')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "Zmean_dp = []\n",
    "for dp in data_points:\n",
    "    Zmean_dp.append(Z_mean[dp[0], dp[1]])\n",
    "\n",
    "err = signal_strengths-np.array(Zmean_dp)\n",
    "print(err)\n",
    "print(np.mean(err))\n",
    "print(np.std(err))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4368d5eb",
   "metadata": {},
   "source": [
    "### RQ Kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85f62bde",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.gaussian_process.kernels import DotProduct, RationalQuadratic, WhiteKernel\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "# Coordinates of the data collection points (red squares on the map)\n",
    "data_points = np.array([\n",
    "    [966, 2992],\n",
    "    [2569, 3767],\n",
    "    [2873, 3447],\n",
    "    [2621, 4286],\n",
    "    [1312, 3830],\n",
    "    [3828, 2667],\n",
    "    [242, 2442],\n",
    "    [1711, 3145],\n",
    "    [2852, 1584],\n",
    "    [1903, 2393]\n",
    "])\n",
    "#data_points = data_points[:, ::-1]\n",
    "\n",
    "\n",
    "# Sample signal strengths at these data points (randomly generated for this example)\n",
    "signal_strengths = np.array([-86.35, -96.03, -99.60, -99.47, -92.57, -98.94, -85.31, -100.40, -99.86, -88.08])\n",
    "\n",
    "# Define a Gaussian Process model with a Radial Basis Function (RBF) kernel.\n",
    "kernel = RationalQuadratic(length_scale=1.0, alpha=3.0, length_scale_bounds=(100, 5000.0), alpha_bounds=(2, 100.0))\n",
    "#kernel = C(10.0, (1e-3, 1e4)) * RBF(length_scale=1000.0, length_scale_bounds=(1e-2, 2e3))\n",
    "gp = GaussianProcessRegressor(kernel=kernel, n_restarts_optimizer=15, alpha = 1e-10)\n",
    "\n",
    "# Fit the model to the data points.\n",
    "gp.fit(data_points, signal_strengths)\n",
    "\n",
    "# Create a mesh grid to represent the 2D space for PDF calculation.\n",
    "x = np.linspace(0, map_.shape[1], map_.shape[1] , endpoint=False)\n",
    "y = np.linspace(0, map_.shape[0], map_.shape[0], endpoint=False)\n",
    "X, Y = np.meshgrid(x, y)\n",
    "xy = np.vstack([X.ravel(), Y.ravel()]).T\n",
    "\n",
    "# Predict the signal strength at each point in the mesh grid.\n",
    "Z_mean, Z_std = gp.predict(xy, return_std=True)\n",
    "Z_mean = Z_mean.reshape(X.shape)\n",
    "Z_std = Z_std.reshape(X.shape)\n",
    "\n",
    "\n",
    "\n",
    "plt.figure(figsize=(13, 8))\n",
    "plt.contourf(X, Y, Z_mean, 100, cmap='viridis')\n",
    "plt.colorbar(label='dBX')\n",
    "#data_points = data_points[:, ::-1]\n",
    "\n",
    "# Plot the data collection points for reference\n",
    "scatter = plt.scatter(data_points[:, 0], data_points[:, 1], c=signal_strengths, s=50, cmap='plasma', label='Data Collection Points')\n",
    "cbar = plt.colorbar(scatter, label='Signal Strength (dBX)', location='left')  # Colorbar for signal strengths on the left\n",
    "plt.legend()\n",
    "\n",
    "plt.xlabel('UTM_E [m]')\n",
    "plt.ylabel('UTM_N [m]')\n",
    "plt.title('2D Predictions of Signal Strength')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Normalize the signal strength predictions to form a probability distribution\n",
    "max_strength = np.max(Z_mean)\n",
    "probabilities = np.exp(Z_mean - max_strength)  # Exponential to make it a positive distribution\n",
    "probabilities /= np.sum(probabilities)  # Normalize to sum up to 1\n",
    "\n",
    "plt.figure(figsize=(13, 8))\n",
    "plt.contourf(X, Y, probabilities, 100, cmap='viridis')\n",
    "plt.colorbar(label='Probability')\n",
    "\n",
    "# Plot the data collection points with a different colormap\n",
    "scatter = plt.scatter(data_points[:, 0], data_points[:, 1], c=signal_strengths, s=50, cmap='plasma', label='Data Collection Points')\n",
    "cbar = plt.colorbar(scatter, label='Signal Strength (dBX)', location='left')  # Colorbar for signal strengths on the left\n",
    "plt.legend()\n",
    "\n",
    "plt.xlabel('UTM_E [m]')\n",
    "plt.ylabel('UTM_N [m]')\n",
    "plt.title('2D Probability Distribution of Transmitter Location')\n",
    "plt.show()\n",
    "\n",
    "print(np.sum(probabilities[:100, :100]))\n",
    "\n",
    "\n",
    "# Calculate the 2D cumulative distribution function (CDF)\n",
    "cumulative_prob = np.cumsum(probabilities, axis=0)\n",
    "cumulative_prob = np.cumsum(cumulative_prob, axis=1)\n",
    "\n",
    "# Plot the 2D CDF\n",
    "plt.figure(figsize=(13, 8))\n",
    "plt.contourf(X, Y, cumulative_prob, 100, cmap='viridis')\n",
    "plt.colorbar(label='Cumulative Probability')\n",
    "\n",
    "# Plot the data collection points for reference\n",
    "scatter = plt.scatter(data_points[:, 0], data_points[:, 1], c=signal_strengths, s=50, cmap='jet', label='Data Collection Points')\n",
    "cbar = plt.colorbar(scatter, label='Signal Strength (dBX)', location='left')  # Colorbar for signal strengths on the left\n",
    "plt.legend()\n",
    "\n",
    "plt.xlabel('UTM_E [m]')\n",
    "plt.ylabel('UTM_N [m]')\n",
    "plt.title('2D Cumulative Distribution Function of Transmitter Location')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "Zmean_dp = []\n",
    "for dp in data_points:\n",
    "    Zmean_dp.append(Z_mean[dp[0], dp[1]])\n",
    "\n",
    "err = signal_strengths-np.array(Zmean_dp)\n",
    "print(err)\n",
    "print(np.mean(err))\n",
    "print(np.std(err))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa8fdf66",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec4b1e71",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "67865388",
   "metadata": {},
   "source": [
    "### DotProduct Kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "346a2abf",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.gaussian_process.kernels import DotProduct\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "# Coordinates of the data collection points (red squares on the map)\n",
    "data_points = np.array([\n",
    "    [966, 2992],\n",
    "    [2569, 3767],\n",
    "    [2873, 3447],\n",
    "    [2621, 4286],\n",
    "    [1312, 3830],\n",
    "    [3828, 2667],\n",
    "    [242, 2442],\n",
    "    [1711, 3145],\n",
    "    [2852, 1584],\n",
    "    [1903, 2393]\n",
    "])\n",
    "data_points = data_points[:, ::-1]\n",
    "\n",
    "\n",
    "# Sample signal strengths at these data points (randomly generated for this example)\n",
    "signal_strengths = np.array([-86.35, -96.03, -99.60, -99.47, -92.57, -98.94, -85.31, -100.40, -99.86, -88.08])\n",
    "\n",
    "# Define a Gaussian Process model with a Radial Basis Function (RBF) kernel.\n",
    "#kernel = C(10.0, (1e-3, 1e4)) * RBF(length_scale=1000.0, length_scale_bounds=(1e-2, 2e3))\n",
    "kernel = DotProduct(sigma_0=1.0, sigma_0_bounds=(1e-05, 100000.0))\n",
    "gp = GaussianProcessRegressor(kernel=kernel, n_restarts_optimizer=10, alpha = 1e-7)\n",
    "\n",
    "# Fit the model to the data points.\n",
    "gp.fit(data_points, signal_strengths)\n",
    "\n",
    "# Create a mesh grid to represent the 2D space for PDF calculation.\n",
    "x = np.linspace(0, map_.shape[0], map_.shape[0] , endpoint=False)\n",
    "y = np.linspace(0, map_.shape[1], map_.shape[1], endpoint=False)\n",
    "X, Y = np.meshgrid(x, y)\n",
    "xy = np.vstack([X.ravel(), Y.ravel()]).T\n",
    "\n",
    "# Predict the signal strength at each point in the mesh grid.\n",
    "Z_mean, Z_std = gp.predict(xy, return_std=True)\n",
    "Z_mean = Z_mean.reshape(X.shape)\n",
    "Z_std = Z_std.reshape(X.shape)\n",
    "\n",
    "\n",
    "\n",
    "plt.figure(figsize=(13, 8))\n",
    "plt.contourf(X, Y, Z_mean, 100, cmap='viridis')\n",
    "plt.colorbar(label='dBX')\n",
    "data_points = data_points[:, ::-1]\n",
    "\n",
    "# Plot the data collection points for reference\n",
    "scatter = plt.scatter(data_points[:, 0], data_points[:, 1], c=signal_strengths, s=50, cmap='plasma', label='Data Collection Points')\n",
    "cbar = plt.colorbar(scatter, label='Signal Strength (dBX)', location='left')  # Colorbar for signal strengths on the left\n",
    "plt.legend()\n",
    "\n",
    "plt.xlabel('UTM_E [m]')\n",
    "plt.ylabel('UTM_N [m]')\n",
    "plt.title('2D Predictions of Signal Strength')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Normalize the signal strength predictions to form a probability distribution\n",
    "max_strength = np.max(Z_mean)\n",
    "probabilities = np.exp(Z_mean - max_strength)  # Exponential to make it a positive distribution\n",
    "probabilities /= np.sum(probabilities)  # Normalize to sum up to 1\n",
    "\n",
    "plt.figure(figsize=(13, 8))\n",
    "plt.contourf(X, Y, probabilities, 100, cmap='viridis')\n",
    "plt.colorbar(label='Probability')\n",
    "\n",
    "# Plot the data collection points with a different colormap\n",
    "scatter = plt.scatter(data_points[:, 0], data_points[:, 1], c=signal_strengths, s=50, cmap='plasma', label='Data Collection Points')\n",
    "cbar = plt.colorbar(scatter, label='Signal Strength (dBX)', location='left')  # Colorbar for signal strengths on the left\n",
    "plt.legend()\n",
    "\n",
    "plt.xlabel('UTM_E [m]')\n",
    "plt.ylabel('UTM_N [m]')\n",
    "plt.title('2D Probability Distribution of Transmitter Location')\n",
    "plt.show()\n",
    "\n",
    "print(np.sum(probabilities[:100, :100]))\n",
    "\n",
    "\n",
    "# Calculate the 2D cumulative distribution function (CDF)\n",
    "cumulative_prob = np.cumsum(probabilities, axis=0)\n",
    "cumulative_prob = np.cumsum(cumulative_prob, axis=1)\n",
    "\n",
    "# Plot the 2D CDF\n",
    "plt.figure(figsize=(13, 8))\n",
    "plt.contourf(X, Y, cumulative_prob, 100, cmap='viridis')\n",
    "plt.colorbar(label='Cumulative Probability')\n",
    "\n",
    "# Plot the data collection points for reference\n",
    "scatter = plt.scatter(data_points[:, 0], data_points[:, 1], c=signal_strengths, s=50, cmap='jet', label='Data Collection Points')\n",
    "cbar = plt.colorbar(scatter, label='Signal Strength (dBX)', location='left')  # Colorbar for signal strengths on the left\n",
    "plt.legend()\n",
    "\n",
    "plt.xlabel('UTM_E [m]')\n",
    "plt.ylabel('UTM_N [m]')\n",
    "plt.title('2D Cumulative Distribution Function of Transmitter Location')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "Zmean_dp = []\n",
    "for dp in data_points:\n",
    "    Zmean_dp.append(Z_mean[dp[0], dp[1]])\n",
    "\n",
    "err = signal_strengths-np.array(Zmean_dp)\n",
    "print(err)\n",
    "print(np.mean(err))\n",
    "print(np.std(err))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "610cdd7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum(probabilities[:2000, :2000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f536234",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.optimize import least_squares\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Given data points and their signal strengths\n",
    "data_points = np.array([\n",
    "    [966, 2992],\n",
    "    [2569, 3767],\n",
    "    [2873, 3447],\n",
    "    [2621, 4286],\n",
    "    [1312, 3830],\n",
    "    [3828, 2667],\n",
    "    [242, 2442],\n",
    "    [1711, 3145],\n",
    "    [2852, 1584],\n",
    "    [1903, 2393]\n",
    "])\n",
    "\n",
    "signal_strengths = np.array([-86.35, -96.03, -99.60, -99.47, -92.57, -98.94, -85.31, -100.40, -99.86, -88.08])\n",
    "\n",
    "# Map dimensions and scale\n",
    "map_height, map_width = 5202, 5842  # in pixels\n",
    "pixel_to_meter = 0.5  # Each pixel is 0.5 meter by 0.5 meter\n",
    "\n",
    "# Define the path loss model function\n",
    "def signal_strength_model(d, n, P0):\n",
    "    # Assuming a reference distance d0 of 1 meter\n",
    "    d0 = 1.0\n",
    "    # Calculate the path loss\n",
    "    RSS = P0 - 10 * n * np.log10(1 + d / d0)\n",
    "    return RSS\n",
    "\n",
    "# Define the distance calculation function\n",
    "def calculate_distance(p1, p2):\n",
    "    # Calculate the Euclidean distance between two points and convert to meters\n",
    "    return np.linalg.norm(np.array(p1) - np.array(p2), axis=1) * pixel_to_meter\n",
    "\n",
    "\n",
    "x = np.linspace(0, map_.shape[1], map_.shape[1] , endpoint=False)\n",
    "y = np.linspace(0, map_.shape[0], map_.shape[0], endpoint=False)\n",
    "X, Y = np.meshgrid(x, y)\n",
    "xy = np.vstack([X.ravel(), Y.ravel()]).T\n",
    "\n",
    "dists = np.zeros((map_.shape[1], map_.shape[0], len(data_points)))\n",
    "for idx, dp in enumerate(data_points):\n",
    "    dists[:, :, idx] = calculate_distance(xy, dp).reshape((5842,5202))\n",
    "    \n",
    "#dist_tx_dps = calculate_distance(xy, data_po)\n",
    "# Corrected optimization function to include the transmitter location\n",
    "def optimize_signal_strength(params, transmitter, dists, measured_strengths):\n",
    "    n, P0 = params\n",
    "    predictions = np.array([\n",
    "        signal_strength_model(dists[transmitter[0], transmitter[0], :].ravel(), n, P0)\n",
    "    ])\n",
    "    return predictions.ravel() - measured_strengths\n",
    "\n",
    "def dB_to_lin(pow_dB):\n",
    "    return 10**(np.array(pow_dB)/10)\n",
    "\n",
    "def lin_to_dB(pow_lin):\n",
    "    return 10*np.log10(np.array(pow_lin))\n",
    "\n",
    "result = np.zeros((1, data_points.shape[0]))\n",
    "for i in tqdm(range(2000)):\n",
    "    for j in range(2000):\n",
    "        # Perform optimization for the first data point as the transmitter\n",
    "        transmitter_location = [i, j]\n",
    "        initial_guess = [3.0, -30]\n",
    "        # Running the optimization with the corrected function\n",
    "        res = least_squares(optimize_signal_strength, initial_guess, args=(transmitter_location, dists, signal_strengths))\n",
    "\n",
    "        # Optimized parameters\n",
    "        n_opt, P0_opt = res.x\n",
    "\n",
    "        \n",
    "        estimated_strengths = np.array([\n",
    "        dB_to_lin(signal_strength_model(dists[i,j,:], n_opt, P0_opt))\n",
    "        ])\n",
    "        result += probabilities[i, j]*estimated_strengths\n",
    "        if i%1000 == 0 and j%5000 == 0:\n",
    "            print(lin_to_dB(result))\n",
    "\n",
    "print(lin_to_dB(result))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82f620af",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = lin_to_dB(result*100/93)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6a1d276",
   "metadata": {},
   "outputs": [],
   "source": [
    "err = res.ravel()-signal_strengths\n",
    "print(np.mean(err))\n",
    "print(np.std(err))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c84017a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.optimize import least_squares\n",
    "from tqdm import tqdm\n",
    "from multiprocessing import Pool\n",
    "\n",
    "# Given data points and their signal strengths in dB\n",
    "data_points = np.array([\n",
    "    [966, 2992],\n",
    "    [2569, 3767],\n",
    "    [2873, 3447],\n",
    "    [2621, 4286],\n",
    "    [1312, 3830],\n",
    "    [3828, 2667],\n",
    "    [242, 2442],\n",
    "    [1711, 3145],\n",
    "    [2852, 1584],\n",
    "    [1903, 2393]\n",
    "])\n",
    "signal_strengths = np.array([-86.35, -96.03, -99.60, -99.47, -92.57, -98.94, -85.31, -100.40, -99.86, -88.08])\n",
    "\n",
    "# Map dimensions and scale\n",
    "map_height, map_width = 5202, 5842  # in pixels\n",
    "pixel_to_meter = 0.5  # Each pixel is 0.5 meter by 0.5 meter\n",
    "\n",
    "# Define the signal strength model function\n",
    "def signal_strength_model(d, n, P0):\n",
    "    # Assuming a reference distance d0 of 1 meter\n",
    "    d0 = 1.0\n",
    "    # Calculate the path loss\n",
    "    PL = 10 * n * np.log10(d / d0)\n",
    "    # The received signal strength in dB\n",
    "    RSS = P0 - PL\n",
    "    return RSS\n",
    "\n",
    "# Define the distance calculation function\n",
    "def calculate_distance(p1, p2):\n",
    "    # Calculate the Euclidean distance between two points and convert to meters\n",
    "    return np.linalg.norm(np.array(p1) - np.array(p2)) * pixel_to_meter\n",
    "\n",
    "# Optimization function using signal strengths in dB\n",
    "def optimize_signal_strength(params, transmitter, coordinates, measured_strengths_db):\n",
    "    n, P0 = params\n",
    "    distances = np.array([calculate_distance(transmitter, coord) for coord in coordinates])\n",
    "    predictions_db = signal_strength_model(distances, n, P0)\n",
    "    return predictions_db - measured_strengths_db\n",
    "\n",
    "# Function to optimize for each transmitter location\n",
    "def optimize_for_location(loc_index):\n",
    "    i, j = loc_index // map_width, loc_index % map_width\n",
    "    transmitter_location = [i, j]\n",
    "    initial_guess = [3.0, -30]  # Initial guess for n and P0\n",
    "    res = least_squares(optimize_signal_strength, initial_guess, args=(transmitter_location, data_points, signal_strengths))\n",
    "    n_opt, P0_opt = res.x\n",
    "    return loc_index, n_opt, P0_opt\n",
    "\n",
    "# Run the optimization in parallel for each pixel as potential transmitter location\n",
    "def run_optimization():\n",
    "    # Create a pool of workers equal to the number of available CPU cores\n",
    "    with Pool(processes=None) as pool:\n",
    "        total_pixels = map_height * map_width\n",
    "        # Create an iterator for progress reporting\n",
    "        iterator = tqdm(pool.imap_unordered(optimize_for_location, range(total_pixels)), total=total_pixels)\n",
    "        # Collect the results\n",
    "        results = list(iterator)\n",
    "    return results\n",
    "\n",
    "# Main execution\n",
    "if __name__ == '__main__':\n",
    "    optimization_results = run_optimization()\n",
    "    # Process results here\n",
    "    # For example, you can print the results or save them to a file\n",
    "    for loc_index, n_opt, P0_opt in optimization_results:\n",
    "        print(f\"Location Index: {loc_index}, Path Loss Exponent: {n_opt}, Reference Power: {P0_opt}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c97b23a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04d48edd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def repmat(matrix, row_rep, col_rep):\n",
    "    \"\"\"\n",
    "    Replicate a 2D matrix (list of lists) a specified number of times along the row and column dimensions.\n",
    "\n",
    "    :param matrix: List of lists representing the matrix.\n",
    "    :param row_rep: Number of times to replicate each row.\n",
    "    :param col_rep: Number of times to replicate the entire matrix along the column dimension.\n",
    "    :return: Replicated matrix.\n",
    "    \"\"\"\n",
    "    # Replicate rows\n",
    "    replicated_rows = [row * row_rep for row in matrix]\n",
    "\n",
    "    # Replicate the entire matrix along the column dimension\n",
    "    replicated_matrix = replicated_rows * col_rep\n",
    "\n",
    "    return replicated_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8be50b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.linalg.norm(np.array([[1,2],[3,4]])-np.array([1,3]), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "569cfdcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array([[1,2],[3,4]])-np.array(repmat([[1,3]], 1, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5e11ce6",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array([[1,2],[3,4]])-np.array([1,3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77e1e703",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baea57de",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47a6a8f7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66f091ce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efd1ad93",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a8d43d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savez(\"GPR_output.npz\", X, Y, Z_mean, probabilities)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
