{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sparse Reconstruction Localization Analysis\n",
    "\n",
    "This notebook demonstrates the **joint sparse superposition reconstruction** approach for transmitter localization:\n",
    "1. Load processed power measurements from monitoring locations\n",
    "2. Convert data from dB to linear scale\n",
    "3. Run sparse reconstruction algorithm\n",
    "4. Visualize sparse transmit power field\n",
    "5. Compare with likelihood-based approach\n",
    "\n",
    "---\n",
    "\n",
    "## Overview\n",
    "\n",
    "**Sparse Reconstruction Formulation:**\n",
    "\n",
    "$$\\hat{\\mathbf{t}} = \\arg\\min_{\\mathbf{t}\\ge 0} \\|\\mathbf{W}(\\log_{10}(\\mathbf{A}_{\\text{model}}\\mathbf{t}) - \\log_{10}(\\mathbf{p}))\\|_{2}^{2} + \\lambda \\|\\mathbf{t}\\|_{1}$$\n",
    "\n",
    "**Key Features:**\n",
    "- Single-stage joint optimization (vs. two-stage likelihood)\n",
    "- Explicit sparsity constraint (few active transmitters)\n",
    "- Convex optimization (global optimum guaranteed)\n",
    "- Fast computation (~10-30 seconds vs. 5-10 minutes)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Setup and Configuration\n",
    "\n",
    "Import modules and configure the analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary modules\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import yaml\n",
    "import sys\n",
    "import time\n",
    "from pathlib import Path\n",
    "\n",
    "# Add parent directory to path\n",
    "sys.path.insert(0, str(Path.cwd().parent))\n",
    "\n",
    "# Import utility functions\n",
    "from src.utils import (\n",
    "    load_slc_map, \n",
    "    load_monitoring_locations, \n",
    "    get_sensor_locations_array,\n",
    "    load_transmitter_locations,\n",
    "    lin_to_dB\n",
    ")\n",
    "\n",
    "# Import SPARSE RECONSTRUCTION modules\n",
    "from src.sparse_reconstruction import (\n",
    "    joint_sparse_reconstruction,\n",
    "    compute_signal_strength_at_points,\n",
    "    dbm_to_linear,\n",
    "    linear_to_dbm\n",
    ")\n",
    "\n",
    "# Import visualization\n",
    "from src.visualization.spatial_plots import (\n",
    "    plot_transmit_power_map,\n",
    "    plot_signal_estimates_map\n",
    ")\n",
    "\n",
    "print(\"✓ Modules imported successfully\")\n",
    "print(\"✓ Using SPARSE RECONSTRUCTION algorithm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "from itertools import combinations\n",
    "import numpy as np\n",
    "import yaml\n",
    "import sys\n",
    "import time\n",
    "from pathlib import Path\n",
    "\n",
    "# Add parent directory to path\n",
    "sys.path.insert(0, str(Path.cwd().parent))\n",
    "\n",
    "# Import utility functions\n",
    "from src.utils import (\n",
    "    load_slc_map, \n",
    "    load_monitoring_locations, \n",
    "    get_sensor_locations_array,\n",
    "    load_transmitter_locations\n",
    ")\n",
    "\n",
    "# Import SPARSE RECONSTRUCTION modules\n",
    "from src.sparse_reconstruction import joint_sparse_reconstruction\n",
    "\n",
    "# ============================================================================\n",
    "# CONFIGURATION\n",
    "# ============================================================================\n",
    "\n",
    "ALL_TRANSMITTERS = ['mario', 'moran', 'guesthouse', 'wasatch', 'ustar']\n",
    "SEEDS = [39, 45, 52, 55, 58, 61, 64, 70, 73, 123]\n",
    "\n",
    "# Generate all subsets of size 1, 2, 3, 4, and 5\n",
    "TRANSMITTER_SUBSETS = []\n",
    "for size in [1, 2, 3, 4, 5]:\n",
    "    TRANSMITTER_SUBSETS.extend([list(combo) for combo in combinations(ALL_TRANSMITTERS, size)])\n",
    "\n",
    "print(f\"Total configurations: {len(SEEDS) * len(TRANSMITTER_SUBSETS)}\")\n",
    "\n",
    "# ============================================================================\n",
    "# LOAD MAP AND CONFIG (one-time load, shared across all iterations)\n",
    "# ============================================================================\n",
    "\n",
    "with open('../config/parameters.yaml', 'r') as f:\n",
    "    config = yaml.safe_load(f)\n",
    "\n",
    "print(\"Loading SLC map...\")\n",
    "map_data = load_slc_map(\n",
    "    map_folder_dir=\"../\",\n",
    "    downsample_factor=config['spatial']['downsample_factor']\n",
    ")\n",
    "print(f\"✓ Map loaded: shape {map_data['shape']}\")\n",
    "\n",
    "# Load all transmitter locations (for metrics/validation later)\n",
    "all_tx_locations = load_transmitter_locations(\n",
    "    config_path='../config/transmitter_locations.yaml',\n",
    "    map_data=map_data\n",
    ")\n",
    "\n",
    "# ============================================================================\n",
    "# MAIN LOOP\n",
    "# ============================================================================\n",
    "\n",
    "failed = []\n",
    "total = len(SEEDS) * len(TRANSMITTER_SUBSETS)\n",
    "\n",
    "for i, (random_seed, transmitters) in enumerate([(s, t) for s in SEEDS for t in TRANSMITTER_SUBSETS], 1):\n",
    "    TRANSMITTER_COMMA = \",\".join(transmitters)\n",
    "    TRANSMITTER_UNDERSCORE = \"_\".join(transmitters)\n",
    "    \n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"[{i}/{total}] Processing: {TRANSMITTER_COMMA} | seed={random_seed}\")\n",
    "    print(f\"{'='*70}\")\n",
    "    \n",
    "    try:\n",
    "        # ---------------------------------------------------------------\n",
    "        # Step 1: Process raw data to monitoring locations\n",
    "        # ---------------------------------------------------------------\n",
    "        !python ../scripts/process_raw_data_to_monitoring.py \\\n",
    "            --input-dir \"C:/Users/serha/raw_data/stat_rot/stat/\" \\\n",
    "            --transmitter {TRANSMITTER_COMMA} \\\n",
    "            --num-locations 10 \\\n",
    "            --output-yaml ../config/monitoring_locations_{TRANSMITTER_UNDERSCORE}_seed_{random_seed}.yaml \\\n",
    "            --output-data ../data/processed/{TRANSMITTER_UNDERSCORE}_seed_{random_seed}/ \\\n",
    "            --dedup-threshold 5.0 \\\n",
    "            --random-seed {random_seed}\n",
    "        \n",
    "        # ---------------------------------------------------------------\n",
    "        # Step 2: Load processed data\n",
    "        # ---------------------------------------------------------------\n",
    "        CUSTOM_YAML = f'../config/monitoring_locations_{TRANSMITTER_UNDERSCORE}_seed_{random_seed}.yaml'\n",
    "        DATA_DIR = Path(f'../data/processed/{TRANSMITTER_UNDERSCORE}_seed_{random_seed}/')\n",
    "        \n",
    "        locations_config = load_monitoring_locations(\n",
    "            config_path=CUSTOM_YAML,\n",
    "            map_data=map_data\n",
    "        )\n",
    "        \n",
    "        observed_powers_dB = np.load(DATA_DIR / f\"{TRANSMITTER_UNDERSCORE}_avg_powers.npy\")\n",
    "        sensor_locations = get_sensor_locations_array(locations_config)\n",
    "        \n",
    "        # ---------------------------------------------------------------\n",
    "        # Step 3: Run joint_sparse_reconstruction with TIREM to cache\n",
    "        # ---------------------------------------------------------------\n",
    "        print(f\"Running TIREM-based joint_sparse_reconstruction to cache propagation data...\")\n",
    "        \n",
    "        start_time = time.time()\n",
    "        \n",
    "        tx_map, info = joint_sparse_reconstruction(\n",
    "            sensor_locations=sensor_locations,\n",
    "            observed_powers_dBm=observed_powers_dB,\n",
    "            input_is_linear=False,\n",
    "            solve_in_linear_domain=True,\n",
    "            map_shape=map_data['shape'],\n",
    "            scale=config['spatial']['proxel_size'],\n",
    "            np_exponent=config['localization']['path_loss_exponent'],\n",
    "            lambda_reg=0,\n",
    "            norm_exponent=0,\n",
    "            enable_reweighting=False,\n",
    "            whitening_method='hetero_geo_aware',\n",
    "            sigma_noise=5e-9,\n",
    "            eta=0.01,\n",
    "            feature_rho=[0.2, 1e10, 1e10, 1e10],\n",
    "            solver='glrt',\n",
    "            selection_method='max',\n",
    "            cluster_max_candidates=30,\n",
    "            glrt_max_iter=30,\n",
    "            glrt_threshold=4.0,\n",
    "            dedupe_distance_m=25.0,\n",
    "            return_linear_scale=False,\n",
    "            verbose=True,\n",
    "            model_type='tirem',\n",
    "            model_config_path='../config/tirem_parameters.yaml',\n",
    "            n_jobs=-1\n",
    "        )\n",
    "        \n",
    "        elapsed_time = time.time() - start_time\n",
    "        print(f\"✓ Completed in {elapsed_time:.2f}s | Transmitters found: {len(info.get('solver_info', {}).get('support', []))}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"FAILED: {TRANSMITTER_UNDERSCORE} seed={random_seed} - {e}\")\n",
    "        failed.append((TRANSMITTER_UNDERSCORE, random_seed, str(e)))\n",
    "\n",
    "# ============================================================================\n",
    "# SUMMARY\n",
    "# ============================================================================\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(f\"BATCH COMPLETED!\")\n",
    "print(f\"{'='*70}\")\n",
    "print(f\"Successful: {total - len(failed)}/{total}\")\n",
    "print(f\"Failed: {len(failed)}/{total}\")\n",
    "\n",
    "if failed:\n",
    "    print(\"\\nFailed configurations:\")\n",
    "    for tx, seed, err in failed:\n",
    "        print(f\"  - {tx} (seed={seed}): {err[:50]}...\")\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure which transmitter(s) to analyze\n",
    "TRANSMITTERS = ['mario', 'moran']\n",
    "TRANSMITTER_COMMA = \",\".join(TRANSMITTERS)\n",
    "TRANSMITTER_UNDERSCORE = \"_\".join(TRANSMITTERS)\n",
    "\n",
    "print(f\"Analyzing transmitter(s): {TRANSMITTER_COMMA}\")\n",
    "\n",
    "random_seed=32\n",
    "\n",
    "'''\n",
    "if random_seed is not None:\n",
    "    !python ../scripts/process_raw_data_to_monitoring.py --input-dir \"C:/Users/serha/raw_data/stat_rot/stat/\" --transmitter {TRANSMITTER_COMMA} --num-locations 10 --output-yaml ../config/monitoring_locations_{TRANSMITTER_UNDERSCORE}_seed_{random_seed}.yaml --output-data ../data/processed/{TRANSMITTER_UNDERSCORE}_seed_{random_seed}/ --dedup-threshold 5.0 --random-seed {random_seed}\n",
    "else:\n",
    "    !python ../scripts/process_raw_data_to_monitoring.py --input-dir \"C:/Users/serha/raw_data/stat_rot/stat/\" --transmitter {TRANSMITTER_COMMA} --num-locations 10 --output-yaml ../config/monitoring_locations_{TRANSMITTER_UNDERSCORE}.yaml --output-data ../data/processed/{TRANSMITTER_UNDERSCORE}/ --dedup-threshold 5.0\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set file paths\n",
    "if random_seed is not None:\n",
    "    CUSTOM_YAML = f'../config/monitoring_locations_{TRANSMITTER_UNDERSCORE}_seed_{random_seed}.yaml'\n",
    "    DATA_DIR = Path(f'../data/processed/{TRANSMITTER_UNDERSCORE}_seed_{random_seed}/')\n",
    "else:\n",
    "    CUSTOM_YAML = f'../config/monitoring_locations_{TRANSMITTER_UNDERSCORE}.yaml'\n",
    "    DATA_DIR = Path(f'../data/processed/{TRANSMITTER_UNDERSCORE}/')\n",
    "\n",
    "print(f\"Configuration file: {CUSTOM_YAML}\")\n",
    "print(f\"Data directory: {DATA_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Load Map and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load configuration\n",
    "with open('../config/parameters.yaml', 'r') as f:\n",
    "    config = yaml.safe_load(f)\n",
    "\n",
    "print(\"Configuration loaded:\")\n",
    "print(f\"  Proxel size: {config['spatial']['proxel_size']} m/pixel\")\n",
    "print(f\"  Path loss exponent: {config['localization']['path_loss_exponent']}\")\n",
    "print(f\"  Shadowing σ: {config['localization']['std_deviation']} dB\")\n",
    "print(f\"  Correlation distance δ_c: {config['localization']['correlation_coeff']} m\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load SLC map\n",
    "print(\"Loading SLC map...\")\n",
    "map_data = load_slc_map(\n",
    "    map_folder_dir=\"../\",\n",
    "    downsample_factor=config['spatial']['downsample_factor']\n",
    ")\n",
    "\n",
    "print(f\"✓ Map loaded: shape {map_data['shape']}\")\n",
    "print(f\"  UTM Easting range: [{map_data['UTM_long'].min():.1f}, {map_data['UTM_long'].max():.1f}] m\")\n",
    "print(f\"  UTM Northing range: [{map_data['UTM_lat'].min():.1f}, {map_data['UTM_lat'].max():.1f}] m\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load transmitter locations for visualization\n",
    "print(\"Loading transmitter locations...\")\n",
    "all_tx_locations = load_transmitter_locations(\n",
    "    config_path='../config/transmitter_locations.yaml',\n",
    "    map_data=map_data\n",
    ")\n",
    "\n",
    "# Filter to only show transmitters being analyzed\n",
    "tx_locations = {name: all_tx_locations[name] for name in TRANSMITTERS if name in all_tx_locations}\n",
    "\n",
    "print(f\"✓ Loaded {len(tx_locations)} transmitter location(s) for display\")\n",
    "print(f\"\\nTransmitter Locations:\")\n",
    "for tx_name, tx_data in tx_locations.items():\n",
    "    print(f\"  {tx_name:12s}: ({tx_data['latitude']:.6f}, {tx_data['longitude']:.6f}) → {tx_data['coordinates']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Load Monitoring Locations and Power Measurements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load monitoring locations\n",
    "print(f\"Loading monitoring locations from {CUSTOM_YAML}...\")\n",
    "locations_config = load_monitoring_locations(\n",
    "    config_path=CUSTOM_YAML,\n",
    "    map_data=map_data\n",
    ")\n",
    "\n",
    "print(f\"✓ Loaded {len(locations_config['data_points'])} monitoring locations\")\n",
    "print(f\"  UTM Zone: {locations_config['utm_zone']}{'N' if locations_config['northern_hemisphere'] else 'S'}\")\n",
    "\n",
    "# Display locations\n",
    "print(\"\\nMonitoring Locations:\")\n",
    "for loc in locations_config['data_points']:\n",
    "    print(f\"  {loc['name']:12s}: ({loc['latitude']:.5f}, {loc['longitude']:.5f}) → {loc['coordinates']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load processed power measurements (in dB)\n",
    "print(f\"\\nLoading power measurements from {DATA_DIR}...\")\n",
    "\n",
    "observed_powers_dB = np.load(DATA_DIR / f\"{TRANSMITTER_UNDERSCORE}_avg_powers.npy\")\n",
    "power_stds = np.load(DATA_DIR / f\"{TRANSMITTER_UNDERSCORE}_std_powers.npy\")\n",
    "sample_counts = np.load(DATA_DIR / f\"{TRANSMITTER_UNDERSCORE}_sample_counts.npy\")\n",
    "\n",
    "print(f\"✓ Loaded power measurements:\")\n",
    "print(f\"  Number of sensors: {len(observed_powers_dB)}\")\n",
    "print(f\"  Power range: [{observed_powers_dB.min():.2f}, {observed_powers_dB.max():.2f}] dB\")\n",
    "print(f\"  Total samples: {sample_counts.sum()}\")\n",
    "print(f\"  Observed powers (dB): {observed_powers_dB}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display summary table\n",
    "summary_df = pd.read_csv(DATA_DIR / f\"{TRANSMITTER_UNDERSCORE}_summary.csv\")\n",
    "print(\"\\nPower Measurement Summary:\")\n",
    "print(summary_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Convert Data for Sparse Reconstruction\n",
    "\n",
    "The sparse reconstruction algorithm works in **linear power scale** (mW), not dB.\n",
    "\n",
    "**Conversion:** $P[\\text{mW}] = 10^{P[\\text{dBm}]/10}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert observed powers from dBm to linear scale (mW)\n",
    "observed_powers_linear = dbm_to_linear(observed_powers_dB)\n",
    "\n",
    "print(\"Unit Conversion:\")\n",
    "print(f\"  Input (dBm):  {observed_powers_dB}\")\n",
    "print(f\"  Output (mW):  {observed_powers_linear}\")\n",
    "print(f\"\\n  Linear scale range: [{observed_powers_linear.min():.2e}, {observed_powers_linear.max():.2e}] mW\")\n",
    "\n",
    "# Note: These are very small numbers (e.g., 1e-8 mW = 1e-11 W = 10 pW)\n",
    "# This is normal for RF propagation at ~100m distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get sensor locations in pixel coordinates\n",
    "sensor_locations = get_sensor_locations_array(locations_config)\n",
    "\n",
    "print(f\"Sensor locations (pixel coordinates):\")\n",
    "print(f\"  Shape: {sensor_locations.shape}\")\n",
    "print(f\"  Array: {sensor_locations}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Run Sparse Reconstruction Algorithm\n",
    "\n",
    "**Optimization Problem:**\n",
    "\n",
    "$$\\hat{\\mathbf{t}} = \\arg\\min_{\\mathbf{t}\\ge 0} \\|\\mathbf{W}(\\log_{10}(\\mathbf{A}_{\\text{model}}\\mathbf{t}) - \\log_{10}(\\mathbf{p}))\\|_{2}^{2} + \\lambda \\|\\mathbf{t}\\|_{1}$$\n",
    "\n",
    "**Parameters:**\n",
    "- $\\mathbf{t}$: Transmit power field (N grid points)\n",
    "- $\\mathbf{p}$: Observed powers (M sensors)\n",
    "- $\\mathbf{A}_{\\text{model}}$: Propagation matrix (linear path gains)\n",
    "- $\\mathbf{W}$: Whitening matrix ($\\mathbf{V}^{-1/2}$)\n",
    "- $\\lambda$: Sparsity regularization parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_power = np.mean(observed_powers_linear)\n",
    "#lambda_reg = 50 / mean_power\n",
    "lambda_reg = 0\n",
    "print(f\"Regularization parameter: λ = {lambda_reg:.2e}\")\n",
    "\n",
    "# Run sparse reconstruction\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"RUNNING SPARSE RECONSTRUCTION ALGORITHM\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "tx_map_sparse_reweighted, info_sparse_reweighted = joint_sparse_reconstruction(\n",
    "    sensor_locations=sensor_locations,\n",
    "    observed_powers_dBm=observed_powers_dB,  # Function converts internally\n",
    "    input_is_linear=False,\n",
    "    solve_in_linear_domain=True,\n",
    "    map_shape=map_data['shape'],\n",
    "    scale=config['spatial']['proxel_size'],\n",
    "    np_exponent=config['localization']['path_loss_exponent'],\n",
    "    lambda_reg=lambda_reg,\n",
    "    norm_exponent=0,\n",
    "    enable_reweighting=False, # Set to True to enable reweighting\n",
    "    whitening_method='hetero_geo_aware', # 'hetero_diag' or 'spatial_corr_exp_decay' or 'hetero_geo_aware'\n",
    "    sigma_noise=5e-9, # The standard deviation of the noise\n",
    "    eta=0.01, # The weight of the reweighting term\n",
    "    feature_rho=[0.2, 1e10, 1e10, 1e10],  # Custom norm. values for LOS, shadowing, obstacles, distance\n",
    "    solver='glrt',  # Use scipy L-BFGS-B for non-convex log-domain objective\n",
    "    selection_method='max',  # Use cluster selection method, 'max' or 'cluster'\n",
    "    cluster_max_candidates=30,  # The maximum number of candidates to consider for clustering\n",
    "    glrt_max_iter=30,  # The maximum number of iterations for the GLRT algorithm\n",
    "    glrt_threshold=4.0,   # The threshold for the GLRT test statistic\n",
    "    dedupe_distance_m=25.0,  # The deduplication distance for transmitters placed too close to each other\n",
    "    return_linear_scale=False,  # Return in dBm for visualization\n",
    "    verbose=True,\n",
    "    model_type='raytracing',\n",
    "    model_config_path='../config/sionna_parameters.yaml',\n",
    "    n_jobs=-1 \n",
    ")\n",
    "\n",
    "elapsed_time = time.time() - start_time\n",
    "\n",
    "print(f\"\\n✓ Reconstruction completed in {elapsed_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Analyze Sparse Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find top-K non-zero locations\n",
    "K = 10  # Number of top locations to display\n",
    "\n",
    "# Convert to linear scale for sorting\n",
    "tx_map_linear_reweighted = dbm_to_linear(tx_map_sparse_reweighted)\n",
    "flat_powers_reweighted = tx_map_linear_reweighted.ravel()\n",
    "top_k_indices_reweighted = np.argsort(flat_powers_reweighted)[-K:][::-1]  # Top K descending\n",
    "\n",
    "print(f\"\\nTop {K} Transmitter Locations (by power):\")\n",
    "print(\"-\" * 80)\n",
    "print(f\"{'Rank':<6} {'Row':<8} {'Col':<8} {'Power (dBm)':<15} {'Power (mW)':<15}\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "for rank, idx in enumerate(top_k_indices_reweighted, 1):\n",
    "    row = idx // map_data['shape'][1]\n",
    "    col = idx % map_data['shape'][1]\n",
    "    power_linear = flat_powers_reweighted[idx]\n",
    "    power_dBm = linear_to_dbm(power_linear)\n",
    "    \n",
    "    if power_linear > 1e-15:  # Only show non-negligible values\n",
    "        print(f\"{rank:<6} {row:<8} {col:<8} {power_dBm:<15.2f} {power_linear:<15.2e}\")\n",
    "\n",
    "print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Visualize Sparse Transmit Power Map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize sparse transmit power field\n",
    "tx_name_list = \" & \".join(TRANSMITTERS)\n",
    "\n",
    "fig, ax = plot_transmit_power_map(\n",
    "    transmit_power_map=tx_map_sparse_reweighted,\n",
    "    data_points=sensor_locations,\n",
    "    observed_powers=observed_powers_dB,\n",
    "    UTM_lat=map_data['UTM_lat'],\n",
    "    UTM_long=map_data['UTM_long'],\n",
    "    band_name=f\"{tx_name_list} Transmitter\",\n",
    "    transmitter_locations=tx_locations\n",
    ")\n",
    "\n",
    "# Add sparsity annotation\n",
    "ax.text(\n",
    "    0.02, 0.98, \n",
    "    f\"Sparsity: {info_sparse_reweighted['sparsity']*100:.1f}%\\n\"\n",
    "    f\"Non-zero: {info_sparse_reweighted['n_nonzero']} / {np.prod(map_data['shape'])}\\n\"\n",
    "    f\"λ = {lambda_reg:.2e}\",\n",
    "    transform=ax.transAxes,\n",
    "    fontsize=10,\n",
    "    verticalalignment='top',\n",
    "    bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.8)\n",
    ")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the indices of the 3 found transmitters\n",
    "support_indices = info_sparse_reweighted['solver_info']['support']\n",
    "height, width = map_data['shape']\n",
    "\n",
    "print(f\"Found {len(support_indices)} transmitters at indices: {support_indices}\")\n",
    "\n",
    "for idx in support_indices:\n",
    "    r, c = divmod(idx, width)\n",
    "    power = tx_map_sparse_reweighted[r, c]\n",
    "    print(f\"  - Location (row={r}, col={c}), Power={power:.2f} dBm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7.5: Visualize GLRT Candidates History\n",
    "\n",
    "If the GLRT solver was used, we can visualize the candidates considered at each iteration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize GLRT Iteration History\n",
    "if 'solver_info' in info_sparse_reweighted and 'candidates_history' in info_sparse_reweighted['solver_info']:\n",
    "    solver_info = info_sparse_reweighted['solver_info']\n",
    "    history = solver_info['candidates_history']\n",
    "    print(f\"\\nVisualizing candidates at each iteration... ({len(history)} iterations)\")\n",
    "    \n",
    "    # Helper to plot map with UTM coordinates\n",
    "    def plot_glrt_step(score_map, iter_num, selected_idx, selected_score, score_label=\"GLRT Score\"):\n",
    "        fig = plt.figure(figsize=(13, 8))\n",
    "        ax = fig.gca()\n",
    "        \n",
    "        # Plot sensors\n",
    "        scatter = ax.scatter(sensor_locations[:, 0], sensor_locations[:, 1],\n",
    "                             c=observed_powers_dB, s=150, edgecolor='green',\n",
    "                             linewidth=2, cmap='hot', label='Monitoring Locations', zorder=6)\n",
    "        \n",
    "        tx_locs = globals().get('tx_locations', None)\n",
    "        if tx_locs is not None:\n",
    "            tx_coords = np.array([tx['coordinates'] for tx in tx_locs.values()])\n",
    "            ax.scatter(tx_coords[:, 0], tx_coords[:, 1],\n",
    "                       marker='x', s=200, c='blue', linewidth=3,\n",
    "                       label='True Transmitter Locations', zorder=10)\n",
    "        \n",
    "        # Plot candidates (sparse score map)\n",
    "        nonzero_mask = score_map > 0\n",
    "        if np.sum(nonzero_mask) > 0:\n",
    "            nonzero_indices = np.argwhere(nonzero_mask)\n",
    "            nonzero_row = nonzero_indices[:, 0]\n",
    "            nonzero_col = nonzero_indices[:, 1]\n",
    "            nonzero_values = score_map[nonzero_mask]\n",
    "            \n",
    "            sparse_scatter = ax.scatter(nonzero_col, nonzero_row,\n",
    "                                      c=nonzero_values, s=300, marker='s',\n",
    "                                      cmap='viridis', edgecolor='black', linewidth=1,\n",
    "                                      label='Candidates', zorder=5)\n",
    "            \n",
    "            cbar = plt.colorbar(sparse_scatter, label=score_label)\n",
    "            cbar.ax.tick_params(labelsize=18)\n",
    "            cbar.set_label(label=score_label, size=18)\n",
    "            cbar.ax.set_position([0.77, 0.1, 0.04, 0.8])\n",
    "            \n",
    "        # Highlight selected\n",
    "        sel_row, sel_col = np.unravel_index(selected_idx, map_data['shape'])\n",
    "        ax.scatter([sel_col], [sel_row], c='magenta', marker='*', s=400, label='Selected Candidate', zorder=11)\n",
    "\n",
    "        UTM_lat = map_data['UTM_lat']\n",
    "        UTM_long = map_data['UTM_long']\n",
    "        interval = max(1, len(UTM_lat) // 5)\n",
    "        tick_values = list(range(0, len(UTM_lat), interval))\n",
    "        tick_labels = ['{:.1f}'.format(lat) for lat in UTM_lat[::interval]]\n",
    "        plt.xticks(ticks=tick_values, labels=tick_labels, fontsize=14, rotation=0)\n",
    "\n",
    "        interval = max(1, len(UTM_long) // 5)\n",
    "        tick_values = list(range(0, len(UTM_long), interval))\n",
    "        tick_labels = ['{:.1f}'.format(lat) for lat in UTM_long[::interval]]\n",
    "        plt.yticks(ticks=[0] + tick_values[1:], labels=[\"\"] + tick_labels[1:], fontsize=14, rotation=90)\n",
    "\n",
    "        ax.set_xlim([0, map_data['shape'][1]])\n",
    "        ax.set_ylim([0, map_data['shape'][0]])\n",
    "        \n",
    "        plt.xlabel('UTM$_E$ [m]', fontsize=18, labelpad=10)\n",
    "        plt.ylabel('UTM$_N$ [m]', fontsize=18, labelpad=10)\n",
    "        \n",
    "        # Dynamic Title\n",
    "        plt.title(f\"GLRT Iteration {iter_num} ({score_label}: {selected_score:.4f})\", fontsize=20)\n",
    "        \n",
    "        scatter_cbar = plt.colorbar(scatter, label='Observed Signal [dBm]', location='left')\n",
    "        scatter_cbar.ax.tick_params(labelsize=18)\n",
    "        scatter_cbar.set_label(label='Observed Signal [dBm]', size=18)\n",
    "        scatter_cbar.ax.set_position([0.18, 0.1, 0.04, 0.8])\n",
    "        \n",
    "        plt.legend(loc='upper right')\n",
    "        plt.show()\n",
    "\n",
    "    # Determine what to show\n",
    "    whitening_method = solver_info.get('whitening_method', 'unknown')\n",
    "    \n",
    "    for item in history:\n",
    "        height, width = map_data['shape']\n",
    "        score_map = np.zeros((height, width))\n",
    "        \n",
    "        top_indices = item['top_indices']\n",
    "        top_scores = item['top_scores']\n",
    "        \n",
    "        # Fill scores (raw scores are usually plotted on map for contrast)\n",
    "        rows, cols = np.unravel_index(top_indices, (height, width))\n",
    "        score_map[rows, cols] = top_scores\n",
    "        \n",
    "        # Use normalized score for title if available and relevant\n",
    "        if whitening_method == 'hetero_geo_aware':\n",
    "             display_score = item.get('normalized_score', item['selected_score'])\n",
    "             label = \"Corrected Score\"\n",
    "        else:\n",
    "             display_score = item['selected_score']\n",
    "             label = \"GLRT Score\"\n",
    "        \n",
    "        plot_glrt_step(score_map, item['iteration'], item['selected_index'], display_score, score_label=label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Localization Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import metrics module\n",
    "from src.evaluation.metrics import compute_localization_metrics, extract_locations_from_map\n",
    "\n",
    "# ============================================================================\n",
    "# METRICS CALCULATION\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"LOCALIZATION METRICS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# 1. Get True Locations\n",
    "# tx_locations is a dict {name: data}. data['coordinates'] is [col, row]\n",
    "true_locs_pixels = np.array([tx['coordinates'] for tx in tx_locations.values()])\n",
    "\n",
    "# 2. Get Estimated Locations\n",
    "# From info_sparse_reweighted['solver_info']['support'] if available (GLRT)\n",
    "if 'solver_info' in info_sparse_reweighted and 'support' in info_sparse_reweighted['solver_info']:\n",
    "    support_indices = info_sparse_reweighted['solver_info']['support']\n",
    "    height, width = map_data['shape']\n",
    "    \n",
    "    # Filter out transmitters with power below -190 dBm\n",
    "    valid_indices = []\n",
    "    for idx in support_indices:\n",
    "        r, c = idx // width, idx % width\n",
    "        power_dbm = tx_map_sparse_reweighted[r, c]\n",
    "        if power_dbm > -190:\n",
    "            valid_indices.append(idx)\n",
    "    \n",
    "    print(f\"Filtered: {len(support_indices)} -> {len(valid_indices)} transmitters (removed {len(support_indices) - len(valid_indices)} with power < -190 dBm)\")\n",
    "    \n",
    "    est_rows = [idx // width for idx in valid_indices]\n",
    "    est_cols = [idx % width for idx in valid_indices]\n",
    "    est_locs_pixels = np.column_stack((est_cols, est_rows)) if valid_indices else np.empty((0, 2))\n",
    "else:\n",
    "    # Fallback: extract from map\n",
    "    est_locs_pixels = extract_locations_from_map(tx_map_sparse_reweighted, threshold=1e-10)\n",
    "\n",
    "# 3. Compute Metrics\n",
    "metrics = compute_localization_metrics(\n",
    "    true_locations=true_locs_pixels,\n",
    "    estimated_locations=est_locs_pixels,\n",
    "    scale=config['spatial']['proxel_size'],\n",
    "    tolerance=200.0  # meters\n",
    ")\n",
    "\n",
    "# 4. Display Results\n",
    "print(f\"True Transmitters:      {metrics['n_true']}\")\n",
    "print(f\"Estimated Transmitters: {metrics['n_est']}\")\n",
    "\n",
    "if not np.isnan(metrics['ale']):\n",
    "    print(f\"\\nAverage Localization Error (ALE): {metrics['ale']:.2f} meters\")\n",
    "else:\n",
    "    print(\"\\nAverage Localization Error (ALE): N/A\")\n",
    "\n",
    "print(f\"\\nDetection Metrics (Tolerance = 200.0 m):\")\n",
    "print(f\"  True Positives (TP):  {metrics['tp']}\")\n",
    "print(f\"  False Positives (FP): {metrics['fp']}\")\n",
    "print(f\"  False Negatives (FN): {metrics['fn']}\")\n",
    "print(f\"  Probability of Detection (Pd): {metrics['pd']*100:.1f}%\")\n",
    "print(f\"  Precision:                     {metrics['precision']*100:.1f}%\")\n",
    "print(f\"  False Alarm Rate (1-Precision): {metrics['far']*100:.1f}%\")\n",
    "print(f\"  F1 Score:                      {metrics['f1_score']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 9: Compare Predictions with Observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "# Compute predicted powers at sensor locations using sparse transmit field\n",
    "print(\"Computing signal strength at sensor locations...\")\n",
    "\n",
    "# Need to convert tx_map back to linear for physics-based propagation\n",
    "tx_map_linear = dbm_to_linear(tx_map_sparse)\n",
    "\n",
    "predicted_powers_linear = compute_signal_strength_at_points(\n",
    "    transmit_power_map_linear=tx_map_linear,\n",
    "    target_locations=sensor_locations,\n",
    "    scale=config['spatial']['proxel_size'],\n",
    "    np_exponent=config['localization']['path_loss_exponent'],\n",
    "    return_linear_scale=False,  # Return in dBm\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "predicted_powers_dB = predicted_powers_linear\n",
    "\n",
    "print(f\"\\n✓ Predictions computed\")\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "# Evaluation metrics\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"EVALUATION METRICS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Per-sensor comparison\n",
    "print(f\"\\nPer-Sensor Comparison:\")\n",
    "print(\"-\" * 80)\n",
    "print(f\"{'Location':<15} {'Observed (dBm)':<18} {'Predicted (dBm)':<18} {'Error (dB)':<12}\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "errors = []\n",
    "for i, loc in enumerate(locations_config['data_points']):\n",
    "    obs = observed_powers_dB[i]\n",
    "    pred = predicted_powers_dB[i]\n",
    "    error = obs - pred\n",
    "    errors.append(error)\n",
    "    print(f\"{loc['name']:<15} {obs:<18.2f} {pred:<18.2f} {error:<12.2f}\")\n",
    "\n",
    "errors = np.array(errors)\n",
    "print(\"-\" * 80)\n",
    "\n",
    "# Summary statistics\n",
    "mse = np.mean(errors**2)\n",
    "rmse = np.sqrt(mse)\n",
    "mae = np.mean(np.abs(errors))\n",
    "max_error = np.max(np.abs(errors))\n",
    "\n",
    "print(f\"\\nSummary Statistics:\")\n",
    "print(f\"  Mean Squared Error (MSE):  {mse:.2f} dB²\")\n",
    "print(f\"  Root Mean Squared Error:   {rmse:.2f} dB\")\n",
    "print(f\"  Mean Absolute Error (MAE): {mae:.2f} dB\")\n",
    "print(f\"  Max Absolute Error:        {max_error:.2f} dB\")\n",
    "\n",
    "# Compare with baseline (mean predictor)\n",
    "variance_baseline = np.var(observed_powers_dB)\n",
    "variance_reduction = (1 - mse / variance_baseline) * 100\n",
    "\n",
    "print(f\"\\n  Baseline variance (mean predictor): {variance_baseline:.2f} dB²\")\n",
    "print(f\"  Variance reduction: {variance_reduction:.1f}%\")\n",
    "\n",
    "if variance_reduction > 0:\n",
    "    print(f\"  ✓ Model outperforms baseline predictor\")\n",
    "else:\n",
    "    print(f\"  ✗ Model underperforms baseline (consider adjusting λ)\")\n",
    "\n",
    "print(\"=\"*70)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "# Scatter plot: Observed vs Predicted\n",
    "fig, ax = plt.subplots(figsize=(8, 8))\n",
    "\n",
    "ax.scatter(observed_powers_dB, predicted_powers_dB, s=100, alpha=0.7, edgecolors='k')\n",
    "\n",
    "# Add diagonal line (perfect prediction)\n",
    "min_val = min(observed_powers_dB.min(), predicted_powers_dB.min())\n",
    "max_val = max(observed_powers_dB.max(), predicted_powers_dB.max())\n",
    "ax.plot([min_val, max_val], [min_val, max_val], 'r--', lw=2, label='Perfect Prediction')\n",
    "\n",
    "ax.set_xlabel('Observed Power (dBm)', fontsize=12)\n",
    "ax.set_ylabel('Predicted Power (dBm)', fontsize=12)\n",
    "ax.set_title('Sparse Reconstruction: Observed vs Predicted Power', fontsize=14, fontweight='bold')\n",
    "ax.legend(fontsize=10)\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Add statistics text box\n",
    "stats_text = f\"RMSE = {rmse:.2f} dB\\nMAE = {mae:.2f} dB\\nSparsity = {info_sparse['sparsity']*100:.1f}%\"\n",
    "ax.text(\n",
    "    0.05, 0.95, stats_text,\n",
    "    transform=ax.transAxes,\n",
    "    fontsize=10,\n",
    "    verticalalignment='top',\n",
    "    bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.8)\n",
    ")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (changen_env)",
   "language": "python",
   "name": "changen_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
